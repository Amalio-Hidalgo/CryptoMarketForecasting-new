{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3dace59",
   "metadata": {},
   "source": [
    "# Crypto Volatility Forecasting â€” Consolidated & Editable Notebook\n",
    "This notebook centralizes **all editable functions** and a clean end-to-end workflow:\n",
    "\n",
    "- Data loading (CoinGecko, Deribit DVOL, Dune, FRED) â€” plug in your keys/headers.\n",
    "\n",
    "- TA-Lib indicators (RSI, MACD, SMA/EMA) computed **preâ€“feature-matrix**.\n",
    "\n",
    "- Feature matrix construction (first differences) and 1-day ahead **target**.\n",
    "\n",
    "- Baselines: **Naive**, **GARCH(1,1)**, **HAR-RV**.\n",
    "\n",
    "- Deep Learning: **LSTM** baseline.\n",
    "\n",
    "- (Optional) **Tsfresh + XGBoost** hook if you want to run the original pipeline.\n",
    "\n",
    "\n",
    "Everything is defined inline so you can edit live. Mirror modules exist under `apiwrappers/` and `extended_models/` for reuse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d041f9",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Imports & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39fc086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 12:45:33,535 INFO numba.cuda.cudadrv.driver init\n",
      "2025-09-26 12:45:47,132 WARNING tsfresh.feature_extraction.settings Dependency not available for matrix_profile, this feature will be disabled!\n",
      "2025-09-26 12:45:47,134 WARNING tsfresh.feature_extraction.settings Dependency not available for matrix_profile, this feature will be disabled!\n"
     ]
    }
   ],
   "source": [
    "# General Utilities \n",
    "import  random, os, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, datetime as dt\n",
    "import dotenv, os\n",
    "\n",
    "# Environment & Dask Client\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "os.makedirs(\"OutputData\", exist_ok=True)\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(filename=\".env\"))\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# API Calls: Data Collection & Standardization\n",
    "import api_wrappers as aw\n",
    "\n",
    "# Tsfresh-Xgboost-Optuna Pipeline\n",
    "import tsxg_pipeline as tsxg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed9de2c",
   "metadata": {},
   "source": [
    "## ðŸ”§ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0075d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core configuration  ---\n",
    "TARGET_COIN = 'ethereum'        # coin to predict volatility for\n",
    "TOP_N = 10                      # top N coins by market cap (CoinGecko universe)\n",
    "DAYS_BACK = 365                 # history depth\n",
    "TIMEZONE = 'Europe/Madrid'      # \n",
    "TRAIN_FRACTION = 0.9            # train/test split\n",
    "MAX_TIMESHIFT = 5\n",
    "TSFRESH_PRESET = \"Minimal\"\n",
    "FDR_LEVEL = 0.05\n",
    "\n",
    "# --- Dune API configuration ---\n",
    "DUNE_QUERIES = {\n",
    "    \"economic_security\": 1933076,\n",
    "    \"daily_dex_volume\": 4388,\n",
    "    \"btc_etf_flows\": 5795477,\n",
    "    \"eth_etf_flows\": 5795645,\n",
    "    \"total_defi_users\": 2972,\n",
    "    \"median_gas\": 2981260,\n",
    "}\n",
    "DUNE_API_KEY = os.getenv(\"DUNE_API_KEY\")\n",
    "DUNE_CSV_PATH = \"OutputData/Dune_Metrics.csv\"\n",
    "\n",
    "# --- FRED API configuration ---\n",
    "FRED_API_KEY= os.getenv(\"FRED_API_KEY\")\n",
    "FRED_KNOWN = {\n",
    "    \"VIXCLS\":   \"vix_equity_vol\",            # CBOE VIX (Equity market volatility index)\n",
    "    \"MOVE\":     \"move_bond_vol\",             # ICE BofA MOVE Index (Bond market volatility)\n",
    "    \"OVXCLS\":   \"ovx_oil_vol\",               # CBOE Crude Oil Volatility Index (Oil market volatility)\n",
    "    \"GVZCLS\":   \"gvz_gold_vol\",              # CBOE Gold Volatility Index (Gold market volatility)\n",
    "    \"DTWEXBGS\": \"usd_trade_weighted_index\",  # Trade-Weighted U.S. Dollar Index (Broad Goods)\n",
    "    \"DGS2\":     \"us_2y_treasury_yield\",      # U.S. 2-Year Treasury Yield (constant maturity)\n",
    "    \"DGS10\":    \"us_10y_treasury_yield\",     # U.S. 10-Year Treasury Yield (constant maturity)\n",
    "}\n",
    "FRED_START_DATE = (dt.datetime.now() - dt.timedelta(days=DAYS_BACK)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# --- LSTM configuration ---\n",
    "LSTM_SEQ_LEN = 7\n",
    "LSTM_UNITS = 64\n",
    "LSTM_EPOCHS = 25\n",
    "LSTM_BATCH = 16\n",
    "\n",
    "# --- GARCH config ---\n",
    "GARCH_SCALE = 100.0             # scale returns to % for stability\n",
    "GARCH_REFIT_EVERY = 5           # refit frequency in days (0 = refit daily)\n",
    "\n",
    "# --- Flags ---\n",
    "USE_SYNTHETIC_DATA = False      # switch to True to demo without hitting APIs\n",
    "RUN_TSFRESH_XGB = False         # set True if you want to run original pipeline\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(RANDOM_SEED); random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1733ddda",
   "metadata": {},
   "source": [
    "## ðŸ§° Utilities: Metrics & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16835e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred): \n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred); \n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def mase(y_true, y_pred, y_naive):\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred); y_naive = np.asarray(y_naive)\n",
    "    denom = np.mean(np.abs(y_true - y_naive)) + 1e-12\n",
    "    return float(np.mean(np.abs(y_true - y_pred)) / denom)\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    ss_res = np.sum((y_true - y_pred)**2); ss_tot = np.sum((y_true - np.mean(y_true))**2) + 1e-12\n",
    "    return float(1 - ss_res/ss_tot)\n",
    "\n",
    "def plot_pred_vs_actual(index, y_true, y_pred, title):\n",
    "    plt.figure()\n",
    "    plt.plot(index, y_true, label='Actual')\n",
    "    plt.plot(index, y_pred, label='Predicted')\n",
    "    plt.title(title); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0feb6e",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Data Loading\n",
    "Plug in your existing API wrapper calls here. This cell defines **editable** functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36fee656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dune_client.client import DuneClient\n",
    "from dune_client.query import QueryBase\n",
    "import time\n",
    "# expects these globals to be defined by the notebook:\n",
    "# TIMEZONE, DAYS_BACK, CG_TOP_N, CG_HEADERS,\n",
    "# DUNE_CSV_PATH, FRED_API_KEY (env)\n",
    "\n",
    "# --- CoinGecko ---\n",
    "def cg_universe(n, cg_headers):\n",
    "    url = \"https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd\"\n",
    "    js = requests.get(url, headers=cg_headers).json()\n",
    "    df = pd.DataFrame(js)\n",
    "    uni = df.head(n)['id'].values\n",
    "    return uni\n",
    "\n",
    "def cgpriceactiondaily(coins, days, timezone, cg_headers):\n",
    "    end   = int(dt.datetime.now(dt.timezone.utc).timestamp()) * 1000\n",
    "    start = int((dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=days)).timestamp()) * 1000\n",
    "    count= 0\n",
    "    for c in coins:\n",
    "        try:\n",
    "            url = f\"https://api.coingecko.com/api/v3/coins/{c}/market_chart/range?vs_currency=usd&from={start}&to={end}\"\n",
    "            js = requests.get(url, headers=cg_headers).json()\n",
    "            p = pd.DataFrame(js[\"prices\"],        columns=[\"t\", f\"prices_{c}\"])\n",
    "            m = pd.DataFrame(js[\"market_caps\"],   columns=[\"t\", f\"marketcaps_{c}\"])\n",
    "            v = pd.DataFrame(js[\"total_volumes\"], columns=[\"t\", f\"total_volumes_{c}\"])\n",
    "            df = p.merge(m, on=\"t\").merge(v, on=\"t\")\n",
    "            df[\"t\"] = pd.to_datetime(df[\"t\"], unit=\"ms\", utc=True)\n",
    "            df = df.set_index(\"t\")\n",
    "            df.columns = [x.lower() for x in df.columns]\n",
    "            df.index = df.index.tz_convert(timezone).tz_localize(None)\n",
    "            df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "            df.index.name = \"date\"\n",
    "            if count ==0: out = df\n",
    "            else: out = out.join(df, how='inner')\n",
    "            count= count+1\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {c}: {e}\")\n",
    "            continue\n",
    "        time.sleep(2)  # Add delay to avoid rate limits\n",
    "    return out\n",
    "# --- Deribit DVOL ---\n",
    "def deribit_dvol_daily_multi(currencies, days, timezone, resolution=\"1D\"):\n",
    "    out = None\n",
    "    end   = int(dt.datetime.now(dt.timezone.utc).timestamp()) * 1000\n",
    "    start = int((dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=days)).timestamp()) * 1000\n",
    "    count=0\n",
    "    for cur in currencies:\n",
    "        js = requests.post(\n",
    "            \"https://www.deribit.com/api/v2/\",\n",
    "            json={\"method\": \"public/get_volatility_index_data\",\n",
    "                    \"params\": {\"currency\": cur, \"resolution\": resolution,\n",
    "                                \"end_timestamp\": end, \"start_timestamp\": start}}\n",
    "        ).json()\n",
    "        data = js.get(\"result\", {}).get(\"data\", [])\n",
    "        if not data:\n",
    "            continue\n",
    "        d = pd.DataFrame(data, columns=[\"t\",\"open\",\"high\",\"low\",\"dvol\"])\n",
    "        d[\"t\"] = pd.to_datetime(d[\"t\"], unit=\"ms\", utc=True)\n",
    "        df = d.set_index(\"t\")[[\"dvol\"]].rename(columns={\"dvol\": f\"dvol_{cur.lower()}\"})\n",
    "        df.index = df.index.tz_convert('Europe/Madrid').tz_localize(None)\n",
    "        df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "        df.index.name = \"date\"\n",
    "        if count ==0: out = df\n",
    "        else: out = out.join(df, how='inner')\n",
    "        count= count+1\n",
    "    return out\n",
    "\n",
    "# --- Dune (CSV) ---\n",
    "def dune_metrics_daily(path, timezone):\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(path, index_col=None)\n",
    "    dt_col = None\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            pd.to_datetime(df[c], utc=True, errors=\"raise\")\n",
    "            dt_col = c\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if dt_col is None and \"date\" in df.columns:\n",
    "        dt_col = \"date\"\n",
    "    if dt_col is None:\n",
    "        return pd.DataFrame()\n",
    "    df = df.rename(columns={dt_col: \"date\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], utc=True, errors=\"coerce\")\n",
    "    df = df.set_index(\"date\")\n",
    "    df.index = df.index.tz_convert(timezone).tz_localize(None)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    df.index.name = \"date\"\n",
    "    df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "    return df\n",
    "\n",
    "# --- Dune (API) ---\n",
    "def fetch_dune_queries_df(query_ids, timezone, dune_api_key=None):\n",
    "    dune = DuneClient(api_key=dune_api_key or os.environ.get(\"DUNE_API_KEY\"),\n",
    "                      request_timeout=300, base_url=\"https://api.dune.com\")\n",
    "    out = None\n",
    "    for qid in query_ids:\n",
    "        try:\n",
    "            q = QueryBase(query_id=qid)\n",
    "            df = dune.run_query_dataframe(query=q, ping_frequency=2, batch_size=365)\n",
    "            ok = False\n",
    "            for col in list(df.columns):\n",
    "                try:\n",
    "                    pd.to_datetime(df[col], utc=True, errors=\"raise\")\n",
    "                    df = df.rename(columns={col: \"date\"}).set_index(\"date\")\n",
    "                    ok = True\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if not ok and not isinstance(df.index, pd.DatetimeIndex):\n",
    "                continue\n",
    "            if isinstance(df.index, pd.DatetimeIndex):\n",
    "                df.index = df.index.tz_convert(timezone).tz_localize(None)\n",
    "            df.columns = [c.lower() for c in df.columns]\n",
    "            df.index.name = \"date\"\n",
    "            df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "            out = df if out is None else out.join(df, how=\"inner\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out if out is not None else pd.DataFrame()\n",
    "\n",
    "# --- FRED ---\n",
    "def fetch_fred_series_df(series_ids, start, timezone, fred_api_key=None):\n",
    "    key = fred_api_key or os.getenv(\"FRED_API_KEY\")\n",
    "    if not key:\n",
    "        return pd.DataFrame()\n",
    "    base = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "    out = None\n",
    "    for sid in series_ids:\n",
    "        try:\n",
    "            js = requests.get(base, params={\n",
    "                \"series_id\": sid, \"api_key\": key, \"file_type\": \"json\",\n",
    "                \"observation_start\": start\n",
    "            }).json()\n",
    "            obs = js.get(\"observations\", [])\n",
    "            if not obs:\n",
    "                continue\n",
    "            df = pd.DataFrame(obs)[[\"date\",\"value\"]]\n",
    "            df[\"date\"]  = pd.to_datetime(df[\"date\"], utc=True, errors=\"coerce\")\n",
    "            df[\"value\"] = pd.to_numeric(df[\"value\"].replace(\".\", np.nan), errors=\"coerce\")\n",
    "            df = df.set_index(\"date\").rename(columns={\"value\": sid.lower()})\n",
    "            df.index = df.index.tz_convert(timezone).tz_localize(None)\n",
    "            df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "            df.index.name = \"date\"\n",
    "            out = df if out is None else out.join(df, how=\"inner\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    if out is not None and {\"dgs10\",\"dgs2\"}.issubset(out.columns):\n",
    "        out[\"term_spread_10y_2y\"] = out[\"dgs10\"] - out[\"dgs2\"]\n",
    "    return out if out is not None else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0192ae",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ TA-Lib Indicators â€” Inline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f420bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apiwrappers/indicators_talib.py\n",
    "import talib\n",
    "\n",
    "def compute_ta_indicators(\n",
    "    df: pd.DataFrame,\n",
    "    price_prefix: str = \"prices_\",\n",
    "    rsi_period: int = 14,\n",
    "    macd_fast: int = 12,\n",
    "    macd_slow: int = 26,\n",
    "    macd_signal: int = 9,\n",
    "    sma_windows: tuple[int, ...] = (10, 20, 50),\n",
    "    ema_windows: tuple[int, ...] = (10, 20, 50),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute TA-Lib indicators for each symbol column in `df` with name starting by `price_prefix`.\n",
    "    Returns a DataFrame with columns like: rsi{p}_{coin}, macd_{coin}, macd_signal_{coin}, macd_hist_{coin}, sma{w}_{coin}, ema{w}_{coin}\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    price_cols = [c for c in df.columns if c.startswith(price_prefix)]\n",
    "    if not price_cols:\n",
    "        return out\n",
    "    coins = [c[len(price_prefix):] for c in price_cols]\n",
    "    for coin in coins:\n",
    "        p = pd.to_numeric(df[f\"{price_prefix}{coin}\"], errors=\"coerce\")\n",
    "        out[f\"rsi{rsi_period}_{coin}\"] = talib.RSI(p.values, timeperiod=rsi_period)\n",
    "        macd, macd_sig, macd_hist = talib.MACD(p.values, fastperiod=macd_fast, slowperiod=macd_slow, signalperiod=macd_signal)\n",
    "        out[f\"macd_{coin}\"] = macd\n",
    "        out[f\"macd_signal_{coin}\"] = macd_sig\n",
    "        out[f\"macd_hist_{coin}\"] = macd_hist\n",
    "        for w in sma_windows:\n",
    "            out[f\"sma{w}_{coin}\"] = talib.SMA(p.values, timeperiod=w)\n",
    "        for w in ema_windows:\n",
    "            out[f\"ema{w}_{coin}\"] = talib.EMA(p.values, timeperiod=w)\n",
    "    out.index = df.index\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefd5cf",
   "metadata": {},
   "source": [
    "## ðŸ“Š GARCH(1,1) & HAR-RV â€” Inline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e53edb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     p = model.params.to_dict()\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mconst\u001b[39m\u001b[33m\"\u001b[39m: p.get(\u001b[33m\"\u001b[39m\u001b[33mconst\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.0\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mRV1\u001b[39m\u001b[33m\"\u001b[39m: p[\u001b[33m\"\u001b[39m\u001b[33mRV1\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRV5\u001b[39m\u001b[33m\"\u001b[39m: p[\u001b[33m\"\u001b[39m\u001b[33mRV5\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRV22\u001b[39m\u001b[33m\"\u001b[39m: p[\u001b[33m\"\u001b[39m\u001b[33mRV22\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforecast_har\u001b[39m(rv: pd.Series, params: \u001b[43mDict\u001b[49m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], start_idx: \u001b[38;5;28mint\u001b[39m) -> pd.Series:\n\u001b[32m     46\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"One-step-ahead HAR predictions from rv[start_idx:].\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m     rv = rv.astype(\u001b[38;5;28mfloat\u001b[39m); idx = rv.index; n = \u001b[38;5;28mlen\u001b[39m(rv); preds = []\n",
      "\u001b[31mNameError\u001b[39m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "# extended_models/models_garch_har.py\n",
    "from arch import arch_model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def fit_garch_11(returns: pd.Series, scale: float = 100.0):\n",
    "    \"\"\"Fit GARCH(1,1) to returns. Returns arch result object.\"\"\"\n",
    "    r = returns.dropna().astype(float) * scale\n",
    "    am = arch_model(r, mean=\"Zero\", vol=\"GARCH\", p=1, q=1, dist=\"normal\")\n",
    "    res = am.fit(disp=\"off\")\n",
    "    return res\n",
    "\n",
    "def forecast_garch_rolling(returns: pd.Series, train_size: int, scale: float = 100.0, refit_every: int = 0) -> pd.Series:\n",
    "    \"\"\"Rolling one-step-ahead volatility forecast with GARCH(1,1).\"\"\"\n",
    "    r = returns.dropna().astype(float)\n",
    "    idx = r.index; n = len(r)\n",
    "    assert 0 < train_size < n\n",
    "    preds, pred_idx = [], []\n",
    "    last_refit = -1; res = None\n",
    "    for t in range(train_size, n):\n",
    "        if (res is None) or (refit_every == 0) or ((t - last_refit) >= refit_every):\n",
    "            res = fit_garch_11(r.iloc[:t], scale=scale); last_refit = t\n",
    "        fcast = res.forecast(horizon=1)\n",
    "        var_next = fcast.variance.iloc[-1, 0]\n",
    "        vol_next = float(np.sqrt(var_next)) / scale\n",
    "        preds.append(vol_next); pred_idx.append(idx[t])\n",
    "    return pd.Series(preds, index=pred_idx, name=\"garch11_vol_pred\")\n",
    "\n",
    "def _har_features(rv: pd.Series) -> pd.DataFrame:\n",
    "    rv = rv.astype(float)\n",
    "    RV1 = rv.shift(1)\n",
    "    RV5 = rv.shift(1).rolling(5).mean()\n",
    "    RV22 = rv.shift(1).rolling(22).mean()\n",
    "    return pd.DataFrame({\"RV1\": RV1, \"RV5\": RV5, \"RV22\": RV22})\n",
    "\n",
    "def fit_har_ols(rv_train: pd.Series):\n",
    "    \"\"\"Fit HAR-RV via OLS; returns dict of coefficients.\"\"\"\n",
    "    X = _har_features(rv_train)\n",
    "    y = rv_train\n",
    "    df = pd.concat([X, y.rename(\"y\")], axis=1).dropna()\n",
    "    X_ = sm.add_constant(df[[\"RV1\", \"RV5\", \"RV22\"]]); y_ = df[\"y\"]\n",
    "    model = sm.OLS(y_, X_).fit()\n",
    "    p = model.params.to_dict()\n",
    "    return {\"const\": p.get(\"const\", 0.0), \"RV1\": p[\"RV1\"], \"RV5\": p[\"RV5\"], \"RV22\": p[\"RV22\"]}\n",
    "\n",
    "def forecast_har(rv: pd.Series, params: Dict[str, float], start_idx: int) -> pd.Series:\n",
    "    \"\"\"One-step-ahead HAR predictions from rv[start_idx:].\"\"\"\n",
    "    rv = rv.astype(float); idx = rv.index; n = len(rv); preds = []\n",
    "    for t in range(start_idx, n):\n",
    "        rv1 = rv.iloc[t-1] if t-1 >= 0 else np.nan\n",
    "        rv5 = rv.iloc[max(0, t-5):t].mean()\n",
    "        rv22 = rv.iloc[max(0, t-22):t].mean()\n",
    "        x = np.array([1.0, rv1, rv5, rv22])\n",
    "        b = np.array([params[\"const\"], params[\"RV1\"], params[\"RV5\"], params[\"RV22\"]])\n",
    "        preds.append(float(np.dot(x, b)))\n",
    "    return pd.Series(preds, index=idx[start_idx:], name=\"har_vol_pred\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba22fe8d",
   "metadata": {},
   "source": [
    "## ðŸ¤– LSTM â€” Inline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280cf045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended_models/models_lstm.py\n",
    "from __future__ import annotations\n",
    "from typing import Tuple\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def make_univariate_sequences(series: pd.Series, seq_len: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    v = series.dropna().astype(float).values\n",
    "    X, y = [], []\n",
    "    for i in range(len(v) - seq_len):\n",
    "        X.append(v[i:i+seq_len]); y.append(v[i+seq_len])\n",
    "    X = np.asarray(X, dtype=float).reshape(-1, seq_len, 1)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    return X, y\n",
    "\n",
    "def make_multivariate_sequences(X_df: pd.DataFrame, y: pd.Series, seq_len: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X_df = X_df.astype(float); y = y.astype(float)\n",
    "    common = X_df.index.intersection(y.index)\n",
    "    X_df = X_df.loc[common]; y = y.loc[common]\n",
    "    X_np, y_np = X_df.values, y.values\n",
    "    n, n_feat = X_np.shape\n",
    "    X_seq, y_seq = [], []\n",
    "    for t in range(seq_len, n):\n",
    "        X_seq.append(X_np[t-seq_len:t, :]); y_seq.append(y_np[t])\n",
    "    return np.asarray(X_seq, float), np.asarray(y_seq, float)\n",
    "\n",
    "def build_lstm_model(input_shape: Tuple[int, int], units: int = 64, dropout: float = 0.0) -> Sequential:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=input_shape))\n",
    "    if dropout > 0: model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def train_lstm(model: Sequential, X_train: np.ndarray, y_train: np.ndarray, epochs: int = 50, batch_size: int = 32, validation_split: float = 0.1, patience: int = 5, verbose: int = 1):\n",
    "    cb = [EarlyStopping(monitor=\"val_loss\", patience=patience, restore_best_weights=True)] if validation_split and patience else []\n",
    "    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=cb, verbose=verbose, shuffle=False)\n",
    "    return hist\n",
    "\n",
    "def predict_lstm(model: Sequential, X_seq: np.ndarray) -> np.ndarray:\n",
    "    return model.predict(X_seq, verbose=0).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b437320",
   "metadata": {},
   "source": [
    "## ðŸ§± Assemble Unified DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f02e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SYNTHETIC_DATA:\n",
    "    coins = ['bitcoin','ethereum']\n",
    "    O = cgpriceactiondaily(coins, DAYS_BACK, TIMEZONE, cg_headers=None)\n",
    "else:\n",
    "    coins = cg_universe(TOP_N, cg_headers=None)\n",
    "    O = cgpriceactiondaily(coins, DAYS_BACK, TIMEZONE, cg_headers=None)\n",
    "\n",
    "D = deribit_dvol_daily_multi(currencies=('BTC','ETH'), days= DAYS_BACK, timezone= TIMEZONE)\n",
    "U = dune_metrics_daily()\n",
    "M = fetch_fred_series_df({'vix_equity_vol':'VIXCLS'})  # example macro\n",
    "\n",
    "# join on index\n",
    "df = O.join([D, U, M], how='outer').sort_index().ffill().dropna()\n",
    "print('Base df shape:', df.shape)\n",
    "df.tail(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0f4a4",
   "metadata": {},
   "source": [
    "## ðŸ§ª Compute Indicators & Build Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86d8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiwrappers.indicators_talib import compute_ta_indicators\n",
    "\n",
    "ta_df = compute_ta_indicators(df, price_prefix='prices_')\n",
    "df = df.join(ta_df).dropna()\n",
    "\n",
    "# Target coin realized volatility\n",
    "df[f'log_returns_{TARGET_COIN}'] = np.log(df[f'prices_{TARGET_COIN}']).diff()\n",
    "df[f'realized_vol_{TARGET_COIN}'] = df[f'log_returns_{TARGET_COIN}'].abs()\n",
    "\n",
    "# Build feature matrix as first differences\n",
    "X_full = df.diff().dropna()\n",
    "y_full = df[f'realized_vol_{TARGET_COIN}'].shift(-1).dropna()\n",
    "\n",
    "# Align\n",
    "common_idx = X_full.index.intersection(y_full.index)\n",
    "X_full = X_full.loc[common_idx]\n",
    "y_full = y_full.loc[common_idx]\n",
    "\n",
    "# Split\n",
    "n = len(X_full)\n",
    "train_size = int(TRAIN_FRACTION * n)\n",
    "X_train, X_test = X_full.iloc[:train_size], X_full.iloc[train_size:]\n",
    "y_train, y_test = y_full.iloc[:train_size], y_full.iloc[train_size:]\n",
    "\n",
    "X_full.shape, X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75db13e",
   "metadata": {},
   "source": [
    "## ðŸª„ Naive Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd48af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive: predict tomorrow's vol as today's vol\n",
    "y_naive = y_test.shift(1).fillna(method='bfill')\n",
    "print('Naive MAE:', round(mae(y_test.values, y_naive.values), 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e6def",
   "metadata": {},
   "source": [
    "## ðŸ“Š GARCH(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ae42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extended_models.models_garch_har import forecast_garch_rolling\n",
    "\n",
    "returns = df[f'log_returns_{TARGET_COIN}'].dropna()\n",
    "# Restrict to window used by X/y for fair alignment\n",
    "returns = returns.loc[y_full.index.min():y_full.index.max()]\n",
    "train_size_r = int(TRAIN_FRACTION * len(returns))\n",
    "\n",
    "garch_preds = forecast_garch_rolling(returns, train_size=train_size_r, scale=GARCH_SCALE, refit_every=GARCH_REFIT_EVERY)\n",
    "garch_preds = garch_preds.reindex(y_test.index).dropna()\n",
    "\n",
    "garch_mae = mae(y_test.loc[garch_preds.index].values, garch_preds.values)\n",
    "garch_mase = mase(y_test.loc[garch_preds.index].values, garch_preds.values, y_naive.loc[garch_preds.index].values)\n",
    "print('GARCH MAE:', round(garch_mae,6), 'MASE:', round(garch_mase,6))\n",
    "plot_pred_vs_actual(garch_preds.index, y_test.loc[garch_preds.index].values, garch_preds.values, 'GARCH(1,1)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca4cec",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ HAR-RV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extended_models.models_garch_har import fit_har_ols, forecast_har\n",
    "\n",
    "rv = df[f'realized_vol_{TARGET_COIN}'].dropna()\n",
    "rv = rv.loc[y_full.index.min():y_full.index.max()]\n",
    "\n",
    "har_params = fit_har_ols(rv.iloc[:train_size])\n",
    "har_preds = forecast_har(rv, har_params, start_idx=train_size).reindex(y_test.index).dropna()\n",
    "\n",
    "har_mae = mae(y_test.loc[har_preds.index].values, har_preds.values)\n",
    "har_mase = mase(y_test.loc[har_preds.index].values, har_preds.values, y_naive.loc[har_preds.index].values)\n",
    "print('HAR MAE:', round(har_mae,6), 'MASE:', round(har_mase,6))\n",
    "plot_pred_vs_actual(har_preds.index, y_test.loc[har_preds.index].values, har_preds.values, 'HAR-RV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7eacb",
   "metadata": {},
   "source": [
    "## ðŸ¤– LSTM (Univariate on realized vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extended_models.models_lstm import make_univariate_sequences, build_lstm_model, train_lstm, predict_lstm\n",
    "\n",
    "rv_all = df[f'realized_vol_{TARGET_COIN}'].loc[y_full.index]\n",
    "rv_train, rv_test = rv_all.iloc[:train_size], rv_all.iloc[train_size:]\n",
    "\n",
    "X_seq_tr, y_seq_tr = make_univariate_sequences(rv_train, seq_len=LSTM_SEQ_LEN)\n",
    "X_seq_te, y_seq_te = make_univariate_sequences(pd.concat([rv_train.iloc[-LSTM_SEQ_LEN:], rv_test]), seq_len=LSTM_SEQ_LEN)\n",
    "\n",
    "model = build_lstm_model(input_shape=(LSTM_SEQ_LEN, 1), units=LSTM_UNITS, dropout=0.0)\n",
    "_ = train_lstm(model, X_seq_tr, y_seq_tr, epochs=LSTM_EPOCHS, batch_size=LSTM_BATCH, validation_split=0.1, patience=5, verbose=0)\n",
    "\n",
    "lstm_preds = predict_lstm(model, X_seq_te)\n",
    "y_test_aligned = y_test.iloc[len(y_test) - len(lstm_preds):]\n",
    "\n",
    "lstm_mae = mae(y_test_aligned.values, lstm_preds)\n",
    "lstm_mase = mase(y_test_aligned.values, lstm_preds, y_naive.iloc[len(y_naive)-len(lstm_preds):].values)\n",
    "print('LSTM MAE:', round(lstm_mae,6), 'MASE:', round(lstm_mase,6))\n",
    "plot_pred_vs_actual(y_test_aligned.index, y_test_aligned.values, lstm_preds, 'LSTM (Univariate)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabe539",
   "metadata": {},
   "source": [
    "## ðŸŒ² (Optional) Tsfresh + XGBoost Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98395231",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TSFRESH_XGB:\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "        from tsfresh.feature_extraction import extract_features\n",
    "        from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "\n",
    "        # Simple example: use realized vol series only for tsfresh demo\n",
    "        series = df[f'realized_vol_{TARGET_COIN}'].dropna().loc[y_full.index]\n",
    "        df_ts, y_ts = make_forecasting_frame(series, kind='rv', max_timeshift=7, rolling_direction=1)\n",
    "        feats = extract_features(df_ts, column_id='id', column_sort='time', n_jobs=0)\n",
    "        y_ts = y_ts.loc[feats.index]\n",
    "        n_tr = int(TRAIN_FRACTION * len(y_ts))\n",
    "        dtr = xgb.DMatrix(feats.iloc[:n_tr], label=y_ts.iloc[:n_tr])\n",
    "        dte = xgb.DMatrix(feats.iloc[n_tr:], label=y_ts.iloc[n_tr:])\n",
    "        params = {'objective':'reg:squarederror', 'max_depth':4, 'eta':0.1}\n",
    "        bst = xgb.train(params, dtr, num_boost_round=200)\n",
    "        pred = bst.predict(dte)\n",
    "        print('XGB (tsfresh) MAE:', round(mae(y_ts.iloc[n_tr:].values, pred),6))\n",
    "    except Exception as e:\n",
    "        print('Skipped tsfresh+xgb demo:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0bb62",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rows = []\n",
    "rows.append({'Model':'Naive','MAE': mae(y_test.values, y_naive.values)})\n",
    "if 'garch_mae' in globals(): rows.append({'Model':'GARCH(1,1)','MAE': garch_mae, 'MASE': garch_mase})\n",
    "if 'har_mae' in globals(): rows.append({'Model':'HAR-RV','MAE': har_mae, 'MASE': har_mase})\n",
    "if 'lstm_mae' in globals(): rows.append({'Model':'LSTM (uni)','MAE': lstm_mae, 'MASE': lstm_mase})\n",
    "comp_df = pd.DataFrame(rows).set_index('Model')\n",
    "comp_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VF.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
