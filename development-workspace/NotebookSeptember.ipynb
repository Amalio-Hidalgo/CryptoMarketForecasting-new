{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8602e90",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6400fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Utilities \n",
    "import  random, os, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, datetime as dt\n",
    "import dotenv, os, requests, time\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Environment & Dask Client\n",
    "os.makedirs(\"OutputData\", exist_ok=True)\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(filename=\".env\"))\n",
    "\n",
    "# Dune Client\n",
    "from dune_client.client import DuneClient\n",
    "from dune_client.query import QueryBase\n",
    "\n",
    "# My tsfresh rolled dataframe construction, feature extraction, selection, and XGBoost forecasting pipeline\n",
    "import tsxg_pipeline as tsxg\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "from tsfresh.convenience.bindings import dask_feature_extraction_on_chunk\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, impute\n",
    "from tsfresh import extract_features, select_features, extract_relevant_features\n",
    "\n",
    "# Dask Distributed Computing\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask.distributed import progress\n",
    "\n",
    "# Sklearn components\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "\n",
    "# Optuna XGB Dask Pipeline Module\n",
    "import xgboost as xgb\n",
    "from xgboost import dask as dxgb\n",
    "import optuna\n",
    "from optuna.integration.dask import DaskStorage\n",
    "\n",
    "# Silence Redundant Warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032c418",
   "metadata": {},
   "source": [
    "Key Constants and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd68b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "TARGET_COIN = \"ethereum\"\n",
    "BASE_FIAT   = \"usd\"\n",
    "TOP_N       = 10\n",
    "LOOKBACK_DAYS = 365\n",
    "START_DATE = (dt.datetime.now() - dt.timedelta(days=LOOKBACK_DAYS)).strftime(\"%Y-%m-%d\")\n",
    "TODAY= dt.date.today().strftime('%Y-%m-%d')\n",
    "TIMEZONE = \"Europe/Madrid\"\n",
    "plt.rcParams['figure.figsize'] = (20,8)\n",
    "FREQUENCY = \"1D\"  \n",
    "TIME= 31  # days for rolling window\n",
    "SLEEP_TIME= 10  # seconds to wait between API calls to avoid rate limiting\n",
    "# Silence Redundant Warnings\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Dune API configuration ---\n",
    "DUNE_QUERIES = {\n",
    "    \"economic_security\": 1933076,   # Blockchain security metric aggregating hash rate, staking amounts, ...\n",
    "    \"daily_dex_volume\": 4388,       # Daily trading volume across decentralized exchanges\n",
    "    \"btc_etf_flows\": 5795477,       # Capital inflows/outflows for Bitcoin ETF products\n",
    "    \"eth_etf_flows\": 5795645,       # Capital inflows/outflows for Ethereum ETF products\n",
    "    \"total_defi_users\": 2972,       # Unique addresses interacting with DeFi protocols\n",
    "    \"median_gas\": 2981260,          # Median gas price (transaction fees) on Ethereum\n",
    "}\n",
    "DUNE_API_KEY = os.getenv(\"DUNE_API_KEY\")\n",
    "DUNE_CSV_PATH = \"OutputData/Dune_Metrics.csv\"\n",
    "\n",
    "# --- FRED API configuration ---\n",
    "FRED_API_KEY= os.getenv(\"FRED_API_KEY\")\n",
    "FRED_KNOWN = {\n",
    "    \"VIXCLS\":   \"vix_equity_vol\",            # CBOE VIX (Equity market volatility index)\n",
    "    \"MOVE\":     \"move_bond_vol\",             # ICE BofA MOVE Index (Bond market volatility)\n",
    "    \"OVXCLS\":   \"ovx_oil_vol\",               # CBOE Crude Oil Volatility Index (Oil market volatility)\n",
    "    \"GVZCLS\":   \"gvz_gold_vol\",              # CBOE Gold Volatility Index (Gold market volatility)\n",
    "    \"DTWEXBGS\": \"usd_trade_weighted_index\",  # Trade-Weighted U.S. Dollar Index (Broad Goods)\n",
    "    \"DGS2\":     \"us_2y_treasury_yield\",      # U.S. 2-Year Treasury Yield (constant maturity)\n",
    "    \"DGS10\":    \"us_10y_treasury_yield\",     # U.S. 10-Year Treasury Yield (constant maturity)\n",
    "}\n",
    "# --- Optuna XGBoost Pipeline Configuration ---\n",
    "SPLITS = 5  # Time series folds\n",
    "DEFAULT_XGB_METRIC = 'mae'  # For XGBoost/Optuna\n",
    "DEFAULT_TREE_METHOD = 'hist'\n",
    "DEFAULT_EARLY_STOPPING = 25\n",
    "DEFAULT_N_TRIALS = 100\n",
    "DEFAULT_N_ROUNDS = 200\n",
    "\n",
    "# --- TSXG Pipeline configuration ---\n",
    "RANDOM_SEED = 42\n",
    "EXTRACTION_SETTINGS = EfficientFCParameters()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35374ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU optimized cluster for my CPU \n",
    "try: \n",
    "    client.close()\n",
    "    cluster.close()\n",
    "    cluster = LocalCluster(\n",
    "            n_workers=4,\n",
    "            threads_per_worker=5,\n",
    "            processes = True,\n",
    "            dashboard_address=':8787',\n",
    "            resources = {'GPU':2}\n",
    "        )\n",
    "    client = Client(cluster)\n",
    "    client\n",
    "except:\n",
    "    cluster = LocalCluster(\n",
    "                n_workers=4,\n",
    "                threads_per_worker=5,\n",
    "                processes = True,\n",
    "                dashboard_address=':8787',\n",
    "                resources = {'GPU':2}\n",
    "            )\n",
    "    client = Client(cluster)\n",
    "    client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc1f80",
   "metadata": {},
   "source": [
    "Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Modules\n",
    "# Expects these globals to be defined by the notebook:\n",
    "# TIMEZONE, DAYS_BACK, CG_TOP_N, CG_HEADERS,\n",
    "# DUNE_CSV_PATH, FRED_API_KEY (env), FRED_KNOWN, DUNE_QUERIES, DUNE_API_KEY (env), \n",
    "# TARGET_COIN, BASE_FIAT, FREQUENCY, LOOKBACK_DAYS, START_DATE, TODAY, TOP_N, \n",
    "\n",
    "# --- CoinGecko Investment Universe (V1)  ---\n",
    "def CoinGecko_GetUniverse(n, cg_api_key=os.getenv(\"COINGECKO_API_KEY\"), sleep_time=6):\n",
    "    \"\"\"\n",
    "        Top n cryptocurrency IDs from CoinGecko API sorted by market cap.\n",
    "            Parameters:\n",
    "            - n: Number of top coins to retrieve\n",
    "            - cg_api_key: CoinGecko API key\n",
    "        Returns: n\n",
    "            - numpy array of identifiers or dictionary containing both formats\n",
    "        \"\"\"\n",
    "    if cg_api_key is None: cg_api_key = os.getenv(\"COINGECKO_API_KEY\")\n",
    "    cg_headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x_cg_demo_api_key\": cg_api_key\n",
    "        }\n",
    "    url = \"https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd\"\n",
    "    js = requests.get(url, headers=cg_headers).json()\n",
    "    df = pd.DataFrame(js)\n",
    "    time.sleep(sleep_time)\n",
    "    try:\n",
    "        return  df.head(n)['id'].values \n",
    "    except: \n",
    "        return print(\"Error Getting Coin Id's: \", df.loc['error_message'].values)\n",
    "\n",
    "# --- CoinGecko Investment Universe (V2) Returns tickers e.g. ETH, ids e.g. ethereum, or both ---\n",
    "def CoinGecko_GetUniverseV2(n=TOP_N, output_format=\"ids\", \n",
    "                            cg_api_key=os.getenv(\"COINGECKO_API_KEY\"), sleep_time=6):\n",
    "    \"\"\"\n",
    "    Top n cryptocurrency tickers and/or ids from CoinGecko API by market cap.\n",
    "        Parameters:\n",
    "        - n: Number of top coins to retrieve\n",
    "        - output_format: Format of identifiers to return\n",
    "        * \"ids\": CoinGecko IDs (e.g., \"bitcoin\", \"ethereum\")\n",
    "        * \"symbols\": Ticker symbols (e.g., \"BTC\", \"ETH\") for use with Binance API\n",
    "        * \"both\": Returns a dict containing both formats\n",
    "        - cg_api_key: CoinGecko API key\n",
    "        Returns: \n",
    "        - numpy array of identifiers or dictionary containing both formats\n",
    "    \"\"\"\n",
    "    if cg_api_key is None: print(\"No API Key Available\")\n",
    "    cg_headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x_cg_demo_api_key\": cg_api_key\n",
    "    }\n",
    "    url = \"https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc\"\n",
    "    js = requests.get(url, headers=cg_headers).json()\n",
    "    df = pd.DataFrame(js)\n",
    "    time.sleep(sleep_time)\n",
    "    try:\n",
    "        if output_format == \"ids\":\n",
    "            result = df.head(n)['id'].values\n",
    "            print(f\"Retrieved {len(result)} coin IDs by market cap from CoinGecko\")\n",
    "            return result\n",
    "        elif output_format == \"symbols\":\n",
    "            result = df.head(n)['symbol'].str.upper().values\n",
    "            print(f\"Retrieved {len(result)} coin symbols by market cap from CoinGecko\")\n",
    "            return result\n",
    "        elif output_format == \"both\":\n",
    "            ids = df.head(n)['id'].values\n",
    "            symbols = df.head(n)['symbol'].str.upper().values\n",
    "            print(f\"Retrieved {len(ids)} coins by market cap from CoinGecko\")\n",
    "            return {\"ids\": ids, \"ticker\": symbols}\n",
    "        else:\n",
    "            raise ValueError(\"output_format must be 'ids', 'symbols', or 'both'\")\n",
    "    except: \n",
    "        return print(\"Error Getting Coin Id's: \", df.loc['error_message'].values)\n",
    "\n",
    "# --- CoinGecko Price Data ---\n",
    "def CoinGecko_GetPriceAction(coins, start= START_DATE, \n",
    "                             tz=TIMEZONE, cg_api_key=os.getenv(\"COINGECKO_API_KEY\"), freq=FREQUENCY, sleep_time=6):\n",
    "    \"\"\"\"\n",
    "    Only works up to past 365 days, lose intraday data if > 90 days due to API public demo limits.\n",
    "    For longer history, use Binance_GetPriceData below.\n",
    "    \"\"\"\n",
    "    end_timestamp   = int(dt.datetime.now().timestamp()) * 1000\n",
    "    start_timestamp = int(pd.to_datetime(start).timestamp()) * 1000\n",
    "    cg_headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x_cg_demo_api_key\": cg_api_key\n",
    "    }\n",
    "    outbig=None\n",
    "    for c in coins:\n",
    "        try:\n",
    "            url = f\"https://api.coingecko.com/api/v3/coins/{c}/market_chart/range?vs_currency=usd&from={start_timestamp}&to={end_timestamp}\"\n",
    "            js = requests.get(url, headers=cg_headers).json()\n",
    "            outsmall = None\n",
    "            for column in js:\n",
    "                timestamps = pd.to_datetime([x[0]for x in js[column]], unit='ms').tz_localize(TIMEZONE)\n",
    "                values= [x[1] for x in js[column]]\n",
    "                if outsmall is None: outsmall= pd.DataFrame(data= values, columns= [(column+'_'+c)], index= timestamps)\n",
    "                else: outsmall[(column+'_'+c)] = values\n",
    "            outsmall[['prices_'+c, 'market_caps_'+c, 'total_volumes_'+c]] = outsmall[['prices_'+c, 'market_caps_'+c, 'total_volumes_'+c]].apply(pd.to_numeric, errors='coerce')\n",
    "            outsmall.index.name = 'date'\n",
    "            pricesandmc= outsmall[['prices_'+c, 'market_caps_'+c]].resample(freq).last().dropna()\n",
    "            volumes= outsmall[['total_volumes_'+c]].resample(freq).sum().dropna()\n",
    "            outsmall= pricesandmc.join(volumes, how='inner')\n",
    "            time.sleep(sleep_time)\n",
    "            if outbig is None: outbig= outsmall\n",
    "            else: outbig= outbig.join(outsmall, how='inner')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data for {c}: {e}\")\n",
    "            continue\n",
    "            time.sleep(sleep_time)\n",
    "    return outbig\n",
    "\n",
    "# --- CoinGecko OHLC Data ---\n",
    "def CoinGecko_GetOHLC(coins, days=LOOKBACK_DAYS, vs_currency=\"usd\"):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with daily open, high, low, close for each coin in coins.\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    for coin in coins:\n",
    "        try:\n",
    "            url = f\"https://api.coingecko.com/api/v3/coins/{coin}/ohlc?vs_currency={vs_currency}&days={days}\"\n",
    "            # CoinGecko returns [timestamp, open, high, low, close] in ms, daily\n",
    "            js = requests.get(url).json()\n",
    "            if not js or not isinstance(js, list):\n",
    "                print(f\"No OHLC data for {coin}\")\n",
    "                continue\n",
    "            df = pd.DataFrame(js, columns=[\"ts\", f\"open_{coin}\", f\"high_{coin}\", f\"low_{coin}\", f\"close_{coin}\"])\n",
    "            df[\"date\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\").dt.date\n",
    "            df = df.drop(columns=[\"ts\"]).set_index(\"date\")\n",
    "            if out is None:\n",
    "                out = df\n",
    "            else:\n",
    "                out = out.join(df, how=\"outer\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching OHLC for {coin}: {e}\")\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "# --- CoinGecko Extended Historical Data (with pagination) ---\n",
    "def CoinGecko_GetHistoricalData_Paginated(coin_ids, vs_currency=\"usd\", max_days=365, \n",
    "                                          step_days=90, timezone=TIMEZONE, \n",
    "                                          cg_api_key=os.getenv(\"COINGECKO_API_KEY\")):\n",
    "    \"\"\"\n",
    "    Gets extended historical price data from CoinGecko using pagination.\n",
    "    Parameters:\n",
    "        coin_id: CoinGecko coin ID (e.g., 'bitcoin')\n",
    "        vs_currency: Base currency (e.g., 'usd')\n",
    "        max_days: Maximum days to fetch\n",
    "        step_days: Days per request (smaller = more requests but more granular data)\n",
    "        timezone: Timezone for the returned DataFrame index\n",
    "        cg_api_key: CoinGecko API key\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with prices, market caps and volumes with datetime index\n",
    "    \"\"\"\n",
    "    output=None\n",
    "    for coin_id in coin_ids:\n",
    "        full_prices = []\n",
    "        full_market_caps = []\n",
    "        full_volumes = []\n",
    "        cg_headers = {\"accept\": \"application/json\", \"x_cg_demo_api_key\": cg_api_key}\n",
    "        # Start from today and work backwards\n",
    "        end_date = dt.datetime.now()\n",
    "        current_end = int(end_date.timestamp())\n",
    "        target_start_date = end_date - dt.timedelta(days=max_days)\n",
    "        print(f\"Fetching data for {coin_id} from {end_date.date()} back to {target_start_date.date()}\")\n",
    "        api_requests = 0\n",
    "        while True:\n",
    "            # Calculate window\n",
    "            current_start = int((end_date - dt.timedelta(days=step_days)).timestamp())\n",
    "            # Build request\n",
    "            url = f\"https://api.coingecko.com/api/v3/coins/{coin_id}/market_chart/range\"\n",
    "            params = {\n",
    "                \"vs_currency\": vs_currency,\n",
    "                \"from\": current_start,\n",
    "                \"to\": current_end\n",
    "            }\n",
    "            # Make request\n",
    "            response = requests.get(url, headers=cg_headers, params=params)\n",
    "            data = response.json()\n",
    "            api_requests += 1\n",
    "            if 'prices' not in data:\n",
    "                print(f\"No more data available or error after {api_requests} requests\")\n",
    "                if 'error' in data:\n",
    "                    print(f\"Error: {data['error']}\")\n",
    "                break\n",
    "            # Process data - extract timestamps and values\n",
    "            prices = data.get('prices', [])\n",
    "            market_caps = data.get('market_caps', [])\n",
    "            volumes = data.get('total_volumes', [])\n",
    "            if not prices:\n",
    "                break\n",
    "            # Add to collections (older data gets added at the beginning)\n",
    "            full_prices = prices + full_prices\n",
    "            full_market_caps = market_caps + full_market_caps\n",
    "            full_volumes = volumes + full_volumes\n",
    "            print(f\"Request #{api_requests}: Got {len(prices)} price points\")\n",
    "            # Move window back in time\n",
    "            end_date = dt.datetime.fromtimestamp(current_start)\n",
    "            current_end = current_start - 1\n",
    "            # Check if we've gone far enough\n",
    "            if end_date <= target_start_date:\n",
    "                print(f\"Reached target date\")\n",
    "                break\n",
    "            # Respect CoinGecko's rate limits\n",
    "            time.sleep(3)\n",
    "        # Create DataFrame from collected data\n",
    "        if not full_prices:\n",
    "            print(\"No data collected\")\n",
    "            return pd.DataFrame()\n",
    "        # Create individual DataFrames for each data type\n",
    "        df_prices = pd.DataFrame(full_prices, columns=['timestamp', f'prices_{coin_id}'])\n",
    "        df_prices['timestamp'] = pd.to_datetime(df_prices['timestamp'], unit='ms')\n",
    "        df_mcaps = pd.DataFrame(full_market_caps, columns=['timestamp', f'market_caps_{coin_id}'])\n",
    "        df_mcaps['timestamp'] = pd.to_datetime(df_mcaps['timestamp'], unit='ms')\n",
    "        df_volumes = pd.DataFrame(full_volumes, columns=['timestamp', f'total_volumes_{coin_id}'])\n",
    "        df_volumes['timestamp'] = pd.to_datetime(df_volumes['timestamp'], unit='ms')\n",
    "        # Merge the dataframes\n",
    "        df = df_prices.merge(df_mcaps, on='timestamp', how='outer')\n",
    "        df = df.merge(df_volumes, on='timestamp', how='outer')\n",
    "        # Set index and timezone\n",
    "        df = df.set_index('timestamp')\n",
    "        if timezone:\n",
    "            df.index = df.index.tz_localize(timezone)\n",
    "        df.index.name = 'date'\n",
    "        print(f\"Total data points: {len(df)}\")\n",
    "        print(f\"Data ranges from {df.index.min().date()} to {df.index.max().date()}\")\n",
    "        if output is None:\n",
    "            output = df\n",
    "        else:\n",
    "            output = output.join(df, how='outer')\n",
    "    return output\n",
    "# --- Deribit DVOL ---\n",
    "def Deribit_GetDVOL(currencies, days, timezone, resolution=\"1D\"):\n",
    "    out = None\n",
    "    end   = int(dt.datetime.now().timestamp()) * 1000\n",
    "    start = int((dt.datetime.now() - dt.timedelta(days=days)).timestamp()) * 1000\n",
    "    count=0\n",
    "    for cur in currencies:\n",
    "        js = requests.post(\n",
    "            \"https://www.deribit.com/api/v2/\",\n",
    "            json={\"method\": \"public/get_volatility_index_data\",\n",
    "                    \"params\": {\"currency\": cur, \"resolution\": resolution,\n",
    "                                \"end_timestamp\": end, \"start_timestamp\": start}}\n",
    "        ).json()\n",
    "        data = js.get(\"result\", {}).get(\"data\", [])\n",
    "        if not data:\n",
    "            continue\n",
    "        d = pd.DataFrame(data, columns=[\"t\",\"open\",\"high\",\"low\",\"dvol\"])\n",
    "        d[\"t\"] = pd.to_datetime(d[\"t\"], unit=\"ms\")\n",
    "        df = d.set_index(\"t\")[[\"dvol\"]].rename(columns={\"dvol\": f\"dvol_{cur.lower()}\"})\n",
    "        df.index = df.index.tz_localize('Europe/Madrid')\n",
    "        df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "        df.index.name = \"date\"\n",
    "        if count ==0: out = df\n",
    "        else: out = out.join(df, how='inner')\n",
    "        count= count+1\n",
    "    return out\n",
    "\n",
    "# --- Dune (CSV) ---    \n",
    "def Dune_FromCSV(path, timezone):\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(path, index_col=None)\n",
    "    dt_col = None\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            pd.to_datetime(df[c], errors=\"raise\")\n",
    "            dt_col = c\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if dt_col is None and \"date\" in df.columns:\n",
    "        dt_col = \"date\"\n",
    "    if dt_col is None:\n",
    "        return pd.DataFrame()\n",
    "    df = df.rename(columns={dt_col: \"date\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.set_index(\"date\")\n",
    "    df.index = df.index.tz_localize(timezone)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    df.index.name = \"date\"\n",
    "    df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "    return df\n",
    "\n",
    "# --- Dune ---\n",
    "def Dune_GetQueries(query_ids, timezone, dune_api_key=None):\n",
    "    dune = DuneClient(api_key=dune_api_key or os.environ.get(\"DUNE_API_KEY\"),\n",
    "                       base_url=\"https://api.dune.com\")\n",
    "    out = None\n",
    "    for qid in query_ids:\n",
    "        try:\n",
    "            q = QueryBase(query_id=qid)\n",
    "            df = dune.run_query_dataframe(query=q, ping_frequency=2, batch_size=365)\n",
    "            ok = False\n",
    "            for col in list(df.columns):\n",
    "                try:\n",
    "                    pd.to_datetime(df[col], errors=\"raise\")\n",
    "                    df = df.rename(columns={col: \"date\"}).set_index(\"date\")\n",
    "                    ok = True\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            if not ok and not isinstance(df.index, pd.DatetimeIndex):\n",
    "                continue\n",
    "            if isinstance(df.index, pd.DatetimeIndex):\n",
    "                df.index = df.index.tz_localize(timezone)\n",
    "            df.columns = [c.lower() for c in df.columns]\n",
    "            df.index.name = \"date\"\n",
    "            df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "            out = df if out is None else out.join(df, how=\"inner\")\n",
    "        except:\n",
    "            continue\n",
    "    return out if out is not None else print('Error Fetching Dune Queries')\n",
    "\n",
    "# --- FRED ---\n",
    "def Fred_GetSeries(series_ids= FRED_KNOWN, start=START_DATE, timezone=TIMEZONE, fred_api_key=FRED_API_KEY):\n",
    "    key = fred_api_key or os.getenv(\"FRED_API_KEY\")\n",
    "    if not key:\n",
    "        return print(\"No API Key Available\")\n",
    "    base = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "    df= None\n",
    "    for sid in series_ids:\n",
    "        try:\n",
    "            js = requests.get(base, params={\n",
    "                    \"series_id\": sid, \"api_key\": fred_api_key, \"file_type\": \"json\",\n",
    "                    \"observation_start\": start\n",
    "                }).json()\n",
    "            obs= pd.DataFrame(js['observations'])\n",
    "            index = pd.DatetimeIndex(obs['date'], freq='infer', tz=timezone)\n",
    "            obs = obs.set_index(index)['value'].rename(FRED_KNOWN[sid])\n",
    "            obs= pd.to_numeric(obs, errors='coerce')\n",
    "            if df is not None: df= pd.merge(left= df, right=obs, left_index=True, right_index=True)\n",
    "            else: df = obs\n",
    "        except:\n",
    "            print(\"error fetching:\", series_ids[sid])\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "    if df is not None:  return df.asfreq('D', method='ffill')\n",
    "    else: return print('Error Compiling Data')\n",
    "\n",
    "# --- Binance Price Action ---\n",
    "def Binance_GetPriceAction(ids=None, tickers=None,  interval=\"1d\", max_days=365, timezone=TIMEZONE, top_n=TOP_N):\n",
    "    \"\"\"\n",
    "    Gets extended OHLCV data from Binance using pagination to overcome the 1000 candle limit.\n",
    "    Parameters:\n",
    "        symbols: List of trading symbols (e.g., ['BTC', 'ETH'])\n",
    "        interval: Candlestick interval (1m, 3m, 5m, 15m, 30m, 1h, 2h, 4h, 6h, 8h, 12h, 1d, 3d, 1w, 1M)\n",
    "        max_days: Maximum number of days of history to fetch\n",
    "        timezone: Timezone for the returned DataFrame index\n",
    "    Returns:\n",
    "        DataFrame with OHLCV data and datetime index\n",
    "    \"\"\"\n",
    "    outbig = None\n",
    "    if ids is None or tickers is None:\n",
    "        ids, tickers = CoinGecko_GetUniverseV2(n=top_n, output_format=\"both\", \n",
    "                            cg_api_key=os.getenv(\"COINGECKO_API_KEY\")).values()\n",
    "    for id, ticker in zip(ids, tickers):\n",
    "        ticker = ticker.upper()\n",
    "        print(f\"Fetching {interval} candles for {id} going back {max_days} days...\")\n",
    "        # Pagination variables\n",
    "        full_data = []\n",
    "        end_time = int(dt.datetime.now().timestamp() * 1000)  \n",
    "        start_date_target = dt.datetime.now() - dt.timedelta(days=max_days)  \n",
    "        api_requests = 0\n",
    "        while True:\n",
    "            url = \"https://api.binance.com/api/v3/klines\"\n",
    "            params = {\n",
    "                \"symbol\": ticker + \"USDT\",\n",
    "                \"interval\": interval,   \n",
    "                \"endTime\": end_time,\n",
    "                \"limit\": 1000\n",
    "            }\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            api_requests += 1\n",
    "            if not data or len(data) == 0 or (isinstance(data, dict) and 'code' in data):\n",
    "                print(f\"No more data available for {id} after {api_requests} requests\")\n",
    "                break\n",
    "            print(f\"Request #{api_requests}: Got {len(data)} candles for {id}\")\n",
    "            full_data = data + full_data\n",
    "            oldest_timestamp = int(data[0][0])\n",
    "            oldest_date = dt.datetime.fromtimestamp(oldest_timestamp/1000)\n",
    "            if oldest_date <= start_date_target:\n",
    "                print(f\"Reached target date ({start_date_target.date()}) for {id}\")\n",
    "                break\n",
    "            end_time = oldest_timestamp - 1\n",
    "            time.sleep(1)\n",
    "        if not full_data:\n",
    "            print(f\"No data collected for {id}\")\n",
    "            continue\n",
    "        df = pd.DataFrame(full_data, columns=[\n",
    "            'timestamp', 'open', 'high', 'low', 'close', 'volume',\n",
    "            'close_time', 'quote_asset_volume', 'number_of_trades',\n",
    "            'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "        ])\n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "            df[col + '_' + id.lower()] = df[col]  \n",
    "        df['date'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce', utc=True)\n",
    "        df = df.set_index('date').tz_convert(timezone)\n",
    "        symbol_cols = [f\"{col}_{id}\" for col in ['open', 'high', 'low', 'close', 'volume']]\n",
    "        df = df[symbol_cols]\n",
    "        print(f\"Total candles collected for {id}: {len(df)}\")\n",
    "        print(f\"Data ranges from {df.index.min().date()} to {df.index.max().date()}\")\n",
    "        if outbig is None:\n",
    "            outbig = df\n",
    "        else:\n",
    "            outbig = outbig.join(df, how='outer')\n",
    "    if outbig is None:\n",
    "        print(\"No data collected for any symbols.\")\n",
    "        return pd.DataFrame()\n",
    "    outbig = outbig.sort_index()\n",
    "    outbig.index.name = 'date'\n",
    "    print(f\"Combined data has {len(outbig)} rows\")\n",
    "    return outbig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ta-Lib Technical Analysis Indicators\n",
    "import talib\n",
    "def Compute_TAIndicators(df, price_prefix=\"prices_\", rsi_period=14,\n",
    "                          macd_fast=12, macd_slow=26, macd_signal=9,\n",
    "                          sma_windows=(10,20,50), ema_windows=(10,20,50)):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    price_cols = [c for c in df.columns if c.startswith(price_prefix)]\n",
    "    coins = [c[len(price_prefix):] for c in price_cols]\n",
    "    for coin in coins:\n",
    "        try:\n",
    "            p = df[f\"{price_prefix}{coin}\"]\n",
    "            out[f\"rsi{rsi_period}{coin}\"] = talib.RSI(p.values, timeperiod=rsi_period)\n",
    "            macd, macd_sig, macd_hist = talib.MACD(p.values, fastperiod=macd_fast, slowperiod=macd_slow, signalperiod=macd_signal)\n",
    "            out[f\"macd_{coin}\"] = macd; out[f\"macd_signal_{coin}\"] = macd_sig; out[f\"macd_hist_{coin}\"] = macd_hist\n",
    "            for w in sma_windows: out[f\"sma{w}_{coin}\"] = talib.SMA(p.values, timeperiod=w)\n",
    "            for w in ema_windows: out[f\"ema{w}_{coin}\"] = talib.EMA(p.values, timeperiod=w)\n",
    "            out[f\"bb_upper_{coin}\"], out[f\"bb_middle_{coin}\"], out[f\"bb_lower_{coin}\"] = talib.BBANDS(p.values)\n",
    "            out[f\"atr_{coin}\"] = talib.ATR(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
    "            out[f\"adx_{coin}\"] = talib.ADX(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
    "            out[f\"stoch_k_{coin}\"], out[f\"stoch_d_{coin}\"] = talib.STOCH(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
    "            out[f\"cci_{coin}\"] = talib.CCI(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
    "            out[f\"willr_{coin}\"] = talib.WILLR(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
    "            out[f\"mom_{coin}\"] = talib.MOM(p.values)\n",
    "            out[f\"roc_{coin}\"] = talib.ROC(p.values)\n",
    "            out[f\"obv_{coin}\"] = talib.OBV(p.values, df[f\"volume_{coin}\"])\n",
    "            out[f\"mfi_{acoin}\"] = talib.MFI(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values, df[f\"volume_{coin}\"])\n",
    "        except: continue    \n",
    "    out.index = df.index\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSFresh/Dask Modules\n",
    "def roll_dask(df):\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"Processing partition with columns: {df.columns.tolist()}\")\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    rolled = roll_time_series(\n",
    "        df,\n",
    "        column_id='variable',\n",
    "        column_sort='date',\n",
    "        max_timeshift=TIME,\n",
    "        min_timeshift=1,\n",
    "        rolling_direction=1,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    return rolled\n",
    "\n",
    "def extract_dask(df):\n",
    "    df = df.copy().dropna()\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    print(f\"Extracting features for partition with columns: {df.columns.tolist()}\")\n",
    "    features = extract_features(\n",
    "        df,\n",
    "        column_id='id',\n",
    "        column_sort='date',\n",
    "        column_kind='variable',\n",
    "        column_value='value',\n",
    "        default_fc_parameters=EXTRACTION_SETTINGS,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    return features\n",
    "\n",
    "def select_dask(df, y):\n",
    "    df= df.reset_index(level=0, drop=True).join(y, how='inner').dropna()\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    features= select_features(\n",
    "        df.drop('target', axis=1),\n",
    "        df['target'],\n",
    "        ml_task='regression',\n",
    "        fdr_level=0.05,\n",
    "        hypotheses_independent=False,  \n",
    "        n_jobs=1\n",
    "    )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84976162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna XGBoost Dask Pipeline Module\n",
    "def Optuna_XGB_Dask(client, dtrain, \n",
    "                    n_trials=DEFAULT_N_TRIALS, \n",
    "                    n_rounds=DEFAULT_N_ROUNDS, \n",
    "                    eval_metric=DEFAULT_XGB_METRIC,\n",
    "                    tree_method=DEFAULT_TREE_METHOD, \n",
    "                    early_stopping_rounds=DEFAULT_EARLY_STOPPING):\n",
    "    \"\"\"XGBoost optimization with Optuna using DaskArgs:\n",
    "        dtrain: Dask DMatrix (already created with client)\n",
    "        n_trials: Number of optimization trials\n",
    "        eval_metric: Evaluation metric ('mae', 'rmse', etc.)\n",
    "        tree_method: XGBoost tree construction algorithm\n",
    "        early_stopping_rounds: Number of rounds for early stopping\n",
    "    Returns:\n",
    "        optuna.Study: Optimization study results\n",
    "    \"\"\"    \n",
    "    def objective(trial):\n",
    "        param_grid = {\n",
    "        \"verbosity\": 1,\n",
    "        \"num_boost_rounds\": trial.suggest_int(\"num_boost_rounds\", 100, 1000),\n",
    "        \"tree_method\": DEFAULT_TREE_METHOD,\n",
    "        \"eval_metric\": DEFAULT_XGB_METRIC,\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 0.01, 10.0, log=True),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.01, 10.0, log=True),    \n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12), \n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.1, 10, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.3, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "    }\n",
    "            \n",
    "        output = dxgb.train(\n",
    "            client,\n",
    "            param_grid,\n",
    "            dtrain,\n",
    "            # num_boost_round=param_grid[\"num_boost_rounds\"],\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            evals=[(dtrain, \"train\")]\n",
    "        )\n",
    "        return output[\"history\"][\"train\"][eval_metric][-1]\n",
    "\n",
    "    # Create study with parallel optimization\n",
    "    storage = DaskStorage()\n",
    "    study = optuna.create_study(direction=\"minimize\", storage= storage)\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=n_trials,\n",
    "        n_jobs=20,  # Use all available cores\n",
    "        gc_after_trial=True,  # Clean memory after each trial\n",
    "        show_progress_bar=True\n",
    "        )\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec6e8b",
   "metadata": {},
   "source": [
    "Data Collection from Various APIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc902de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ids, tickers] = CoinGecko_GetUniverseV2(TOP_N, output_format=\"both\").values()\n",
    "price_action1 = Binance_GetPriceAction(ids=ids,tickers=tickers, interval=\"1d\", max_days=LOOKBACK_DAYS, timezone=TIMEZONE)\n",
    "price_action2 = CoinGecko_GetPriceAction(ids, start=START_DATE, tz=TIMEZONE, freq='D')\n",
    "dvol = Deribit_GetDVOL(['BTC','ETH'], days=LOOKBACK_DAYS, timezone=TIMEZONE)\n",
    "# onchainanalytics = dune_metrics_daily(DUNE_QUERIES, DUNE_API_KEY) \n",
    "onchainanalytics = Dune_FromCSV(path= DUNE_CSV_PATH, timezone=TIMEZONE)  \n",
    "macrodata= Fred_GetSeries(series_ids= FRED_KNOWN, fred_api_key=FRED_API_KEY, start=START_DATE, timezone=TIMEZONE)\n",
    "price_action1, price_action2, dvol, onchainanalytics, macrodata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384cad09",
   "metadata": {},
   "source": [
    "Combine all Data Sources into one Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432fecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unified = None\n",
    "for df in [price_action1, dvol, onchainanalytics, macrodata, price_action2]:\n",
    "    try: df.index = pd.DatetimeIndex(df.index).tz_localize(TIMEZONE).date\n",
    "    except: df.index = pd.DatetimeIndex(df.index).tz_convert(TIMEZONE).date\n",
    "    if Unified is None: Unified = df\n",
    "    else: Unified = Unified.join(df, how='outer')\n",
    "Unified.tail(365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b73cb9",
   "metadata": {},
   "source": [
    "Feature Container Construction and Target Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= Unified.iloc[-365 : ].dropna(axis=1, thresh=int(0.1*len(Unified))).ffill(limit=3)\n",
    "X[f'log_returns_{TARGET_COIN}']= np.log(X[f'prices_{TARGET_COIN}']) - np.log(X[f'prices_{TARGET_COIN}'].shift(1))\n",
    "X[f'realized_vol_{TARGET_COIN}'] = abs(X[f'log_returns_{TARGET_COIN}'])\n",
    "X= X.diff().dropna()\n",
    "y = X[f'realized_vol_{TARGET_COIN}'].shift(-1).dropna().rename(\"target\")\n",
    "X.rename_axis(index='date', inplace=True)\n",
    "y.rename_axis(index='date', inplace=True)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc09e6",
   "metadata": {},
   "source": [
    "Feature Engineering: techincal analysis indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66aa9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "taindicators= Compute_TAIndicators(X, price_prefix=\"prices_\")\n",
    "X = X.join(taindicators, how='left').dropna()\n",
    "X= X.loc[X.join(y, how='inner').dropna().index]\n",
    "y= y.loc[X.index]   \n",
    "taindicators, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7574c1f",
   "metadata": {},
   "source": [
    "Dask Feature Container Construction & Prep for Tsfresh, Xgboost, Optuna Pipeline: horizontal pandas.dataframe -> melted/stacked dask.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FC = X.reset_index().melt(id_vars=['date']).sort_values(by='variable')\n",
    "npartitions= FC.variable.nunique()\n",
    "FC_dask = dd.from_pandas(FC, npartitions= npartitions)\n",
    "# Check one timeseries per partition\n",
    "print(FC_dask.map_partitions(lambda df: df['variable'].nunique()).compute().unique())\n",
    "FC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c48dd93",
   "metadata": {},
   "source": [
    "TSfresh & Dask Feature Engineering & Selection Pipeline: timeseries rolling -> feature extraction -> feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a011a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSFresh/Dask Execution\n",
    "# Test rolling on one partition for metadata\n",
    "df_test= FC_dask.partitions[0].compute()\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])\n",
    "rolled_test = roll_time_series(\n",
    "    df_test,\n",
    "    column_id='variable',\n",
    "    column_sort='date',\n",
    "    max_timeshift=TIME,\n",
    "    min_timeshift=1,\n",
    ")\n",
    "# Rolling - No persist (fast operation)\n",
    "rolled_dask = FC_dask.map_partitions(roll_dask, meta=rolled_test).persist()\n",
    "# Feature extraction - Persist (expensive step)\n",
    "features_dask = rolled_dask.map_partitions(extract_dask, enforce_metadata=False).persist()\n",
    "# Feature selection - Compute directly (result)\n",
    "selected_dask = features_dask.map_partitions(select_dask, y=y, enforce_metadata=False).persist()\n",
    "# Materialize and join results\n",
    "out = None\n",
    "selected_futures = client.compute(selected_dask.to_delayed())\n",
    "for i, future in enumerate(selected_futures):\n",
    "    df = future.result()  # Get result for this partition\n",
    "    if len(df) > 0:\n",
    "        if out is None:\n",
    "            out = df\n",
    "        else:\n",
    "            out = out.join(df, how='outer')\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2829856",
   "metadata": {},
   "source": [
    "Construct Final OptunaXGBDaskPipeline-ready Dmatrix with Test-Train Split and Unified Feature Container (tsfresh features + relevant base timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_selected = select_features(X, y, fdr_level=0.05, ml_task='regression', hypotheses_independent=False)\n",
    "final_features = out.join(base_selected, how='left')\n",
    "final_features_dask = dd.from_pandas(final_features.join(y, how='left'), npartitions=SPLITS, sort=True)\n",
    "X = final_features_dask.drop('target', axis=1)\n",
    "y = final_features_dask['target']\n",
    "X_train= X.partitions[0:-1]\n",
    "X_test= X.partitions[-1]\n",
    "y_train= y.partitions[0:-1]\n",
    "y_test= y.partitions[-1]\n",
    "X_train, X_test, y_train, y_test = client.persist([X_train, X_test, y_train, y_test])\n",
    "dtrain= dxgb.DaskDMatrix(client, X_train, y_train)\n",
    "final_features, dtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f8da9",
   "metadata": {},
   "source": [
    "Run Optuna Study -> Build Model With Optimal Hyperparameters -> Run Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "study= Optuna_XGB_Dask(client, dtrain, n_trials=100, n_rounds=100, eval_metric= 'mae', tree_method='hist', early_stopping_rounds=20)\n",
    "final_model= dxgb.train(client, study.best_params, dtrain, num_boost_round=100, evals=[(dtrain, \"train\")])\n",
    "model_features = final_model['booster'].feature_names\n",
    "dtest= dxgb.DaskDMatrix(client, X_test[model_features])\n",
    "predictions = dxgb.predict(client, final_model, dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9698c9",
   "metadata": {},
   "source": [
    "Model Results: evaluation, & visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fafffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pd = y_test.compute()\n",
    "predictions_pd = pd.Series(predictions.compute(), index=y_test_pd.index)\n",
    "\n",
    "# Calculate metrics\n",
    "r2 = r2_score(y_true=y_test_pd, y_pred=predictions_pd)\n",
    "mae = np.mean(np.abs(y_test_pd - predictions_pd))\n",
    "std = y_test_pd.std()\n",
    "thresh_var = mae/std \n",
    "\n",
    "# Calculate MASE (Mean Absolute Scaled Error)\n",
    "naive_forecast = y_test_pd.shift(1)\n",
    "mae_naive = np.mean(np.abs(y_test_pd[1:] - naive_forecast[1:]))\n",
    "mase = mae / mae_naive if mae_naive != 0 else np.nan\n",
    "\n",
    "# Print metrics\n",
    "print(f'Standard Deviation: {std:.6f}')\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best MAE: {study.best_value:.6f}\")\n",
    "print(f\"R2 Score: {r2:.6f}\")\n",
    "print(f'MAE/StdDev: {thresh_var:.6f}')\n",
    "print(f'MASE: {mase:.6f}')\n",
    "\n",
    "# Visualize results\n",
    "viz = pd.DataFrame({'Actual': y_test_pd, 'Predicted': predictions_pd})\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot time series\n",
    "plt.subplot(2, 1, 1)\n",
    "viz.plot(ax=plt.gca())\n",
    "plt.title(f'Predicted vs Actual Realized Volatility for {TARGET_COIN}')\n",
    "plt.ylabel('Realized Volatility')\n",
    "\n",
    "# Plot scatter with perfect prediction line\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(viz['Actual'], viz['Predicted'], alpha=0.5)\n",
    "max_val = max(viz['Actual'].max(), viz['Predicted'].max())\n",
    "min_val = min(viz['Actual'].min(), viz['Predicted'].min())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Actual (Perfect prediction = red line)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VF.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
