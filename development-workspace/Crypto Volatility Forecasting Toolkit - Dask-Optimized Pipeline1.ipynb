{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency Volatility Forecasting with Dask\n",
    "\n",
    "This notebook demonstrates the complete pipeline for cryptocurrency volatility forecasting using:\n",
    "\n",
    "- **Distributed Computing**: Dask for memory-efficient processing\n",
    "- **Feature Engineering**: TSFresh with time series rolling\n",
    "- **Machine Learning**: XGBoost with DMatrix optimization\n",
    "- **Professional Evaluation**: Comprehensive metrics and visualization\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Data Loading and Preparation](#data-loading)\n",
    "3. [Pipeline Execution](#pipeline)\n",
    "4. [Results Analysis](#results)\n",
    "5. [Model Interpretation](#interpretation)\n",
    "6. [Production Deployment](#deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom pipeline\n",
    "from crypto_volatility_toolkit import DaskCryptoVolatilityPipeline, create_dask_client\n",
    "from tsfresh.feature_extraction import EfficientFCParameters, MinimalFCParameters\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'TARGET_COIN': 'ethereum',\n",
    "    'LOOKBACK_DAYS': 365,\n",
    "    'TOP_N_COINS': 10,\n",
    "    'TIMEZONE': 'Europe/Madrid',\n",
    "    \n",
    "    # Pipeline parameters\n",
    "    'MAX_TIMESHIFT': 7,\n",
    "    'TEST_SIZE': 0.2,\n",
    "    'FDR_LEVEL': 0.05,\n",
    "    \n",
    "    # Dask configuration\n",
    "    'N_WORKERS': 4,\n",
    "    'THREADS_PER_WORKER': 2,\n",
    "    'MEMORY_LIMIT': '4GB',\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    'XGB_PARAMS': {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation {#data-loading}\n",
    "\n",
    "Load cryptocurrency data from your existing pipeline or create synthetic data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load your existing data\n",
    "# Uncomment and modify these lines to use your actual data\n",
    "\n",
    "# import dotenv\n",
    "# dotenv.load_dotenv()\n",
    "\n",
    "# # Use your existing API functions\n",
    "# from api_wrappers import (\n",
    "#     cg_universe, cgpriceactiondaily, deribit_dvol_daily_multi,\n",
    "#     dune_metrics_daily, fetch_fred_series_df\n",
    "# )\n",
    "\n",
    "# # Load real data\n",
    "# coins = cg_universe(CONFIG['TOP_N_COINS'], cg_headers=None)\n",
    "# price_data = cgpriceactiondaily(coins, CONFIG['LOOKBACK_DAYS'], CONFIG['TIMEZONE'], None)\n",
    "# dvol_data = deribit_dvol_daily_multi(['BTC', 'ETH'], CONFIG['LOOKBACK_DAYS'], CONFIG['TIMEZONE'])\n",
    "# macro_data = fetch_fred_series_df(['VIXCLS'], '2020-01-01', CONFIG['TIMEZONE'])\n",
    "\n",
    "# # Combine datasets\n",
    "# df_combined = price_data.join([dvol_data, macro_data], how='outer').fillna(method='ffill').dropna()\n",
    "\n",
    "# Option 2: Create synthetic data for demonstration\n",
    "print(\"üìä Creating synthetic cryptocurrency data for demonstration...\")\n",
    "\n",
    "# Generate realistic synthetic data\n",
    "np.random.seed(42)\n",
    "n_days = CONFIG['LOOKBACK_DAYS']\n",
    "dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n",
    "\n",
    "# Simulate price data with realistic volatility clustering\n",
    "def simulate_garch_returns(n, alpha=0.1, beta=0.8, omega=0.01):\n",
    "    \"\"\"Simulate GARCH(1,1) returns for realistic volatility clustering\"\"\"\n",
    "    returns = np.zeros(n)\n",
    "    sigma2 = np.zeros(n)\n",
    "    sigma2[0] = omega / (1 - alpha - beta)\n",
    "    \n",
    "    for t in range(1, n):\n",
    "        sigma2[t] = omega + alpha * returns[t-1]**2 + beta * sigma2[t-1]\n",
    "        returns[t] = np.sqrt(sigma2[t]) * np.random.randn()\n",
    "    \n",
    "    return returns\n",
    "\n",
    "# Create synthetic data\n",
    "btc_returns = simulate_garch_returns(n_days) * 0.03\n",
    "eth_returns = simulate_garch_returns(n_days) * 0.04\n",
    "sol_returns = simulate_garch_returns(n_days) * 0.06\n",
    "\n",
    "# Convert to prices\n",
    "btc_prices = 50000 * np.exp(np.cumsum(btc_returns))\n",
    "eth_prices = 3000 * np.exp(np.cumsum(eth_returns))\n",
    "sol_prices = 100 * np.exp(np.cumsum(sol_returns))\n",
    "\n",
    "# Create DataFrame\n",
    "df_combined = pd.DataFrame({\n",
    "    'prices_bitcoin': btc_prices,\n",
    "    'prices_ethereum': eth_prices,\n",
    "    'prices_solana': sol_prices,\n",
    "    'dvol_btc': np.random.gamma(2, 0.5, n_days) + 20,\n",
    "    'dvol_eth': np.random.gamma(2, 0.6, n_days) + 25,\n",
    "    'vixcls': np.random.gamma(3, 0.3, n_days) + 15,\n",
    "    'dgs10': np.random.normal(3.5, 0.5, n_days),\n",
    "    'usd_trade_weighted_index': np.random.normal(100, 2, n_days)\n",
    "}, index=dates)\n",
    "\n",
    "print(f\"‚úÖ Data created with shape: {df_combined.shape}\")\n",
    "print(f\"üìÖ Date range: {df_combined.index.min().date()} to {df_combined.index.max().date()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable (realized volatility)\n",
    "target_coin = CONFIG['TARGET_COIN']\n",
    "print(f\"üéØ Creating target variable for {target_coin}\")\n",
    "\n",
    "# Calculate log returns\n",
    "df_combined[f'log_returns_{target_coin}'] = np.log(df_combined[f'prices_{target_coin}']).diff()\n",
    "\n",
    "# Calculate realized volatility (absolute returns)\n",
    "df_combined[f'realized_vol_{target_coin}'] = df_combined[f'log_returns_{target_coin}'].abs()\n",
    "\n",
    "# Create features matrix (first differences for stationarity)\n",
    "X_raw = df_combined.select_dtypes(include=[np.number]).diff().dropna()\n",
    "\n",
    "# Create target (next-day realized volatility)\n",
    "y = df_combined[f'realized_vol_{target_coin}'].shift(-1).dropna()\n",
    "\n",
    "# Align X and y\n",
    "common_idx = X_raw.index.intersection(y.index)\n",
    "X = X_raw.loc[common_idx]\n",
    "y = y.loc[common_idx]\n",
    "\n",
    "print(f\"üìä Features matrix shape: {X.shape}\")\n",
    "print(f\"üéØ Target series length: {len(y)}\")\n",
    "print(f\"üìà Target statistics:\")\n",
    "print(f\"   Mean: {y.mean():.6f}\")\n",
    "print(f\"   Std:  {y.std():.6f}\")\n",
    "print(f\"   Min:  {y.min():.6f}\")\n",
    "print(f\"   Max:  {y.max():.6f}\")\n",
    "\n",
    "# Plot target variable\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Time series plot\n",
    "axes[0].plot(y.index, y.values, alpha=0.7)\n",
    "axes[0].set_title(f'{target_coin.title()} Realized Volatility')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Realized Volatility')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution plot\n",
    "axes[1].hist(y.values, bins=50, alpha=0.7, density=True)\n",
    "axes[1].set_title('Realized Volatility Distribution')\n",
    "axes[1].set_xlabel('Realized Volatility')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline Execution {#pipeline}\n",
    "\n",
    "Now we'll run the complete Dask-based volatility forecasting pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dask client\n",
    "print(\"üöÄ Creating Dask client...\")\n",
    "client = create_dask_client(\n",
    "    n_workers=CONFIG['N_WORKERS'],\n",
    "    threads_per_worker=CONFIG['THREADS_PER_WORKER'],\n",
    "    memory_limit=CONFIG['MEMORY_LIMIT']\n",
    ")\n",
    "\n",
    "print(f\"üåê Dask dashboard: {client.dashboard_link}\")\n",
    "print(f\"üë• Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "\n",
    "# Display client info\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "print(\"üèóÔ∏è Initializing Dask Crypto Volatility Pipeline...\")\n",
    "pipeline = DaskCryptoVolatilityPipeline(client=client, random_state=42)\n",
    "\n",
    "print(\"‚úÖ Pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline\n",
    "print(\"üöÄ Running complete volatility forecasting pipeline...\")\n",
    "print(\"This may take several minutes depending on your system...\")\n",
    "\n",
    "results = pipeline.run_complete_pipeline(\n",
    "    X_wide=X,\n",
    "    y=y,\n",
    "    test_size=CONFIG['TEST_SIZE'],\n",
    "    max_timeshift=CONFIG['MAX_TIMESHIFT'],\n",
    "    fc_parameters=EfficientFCParameters(),  # Use EfficientFCParameters for balance of speed/features\n",
    "    fdr_level=CONFIG['FDR_LEVEL'],\n",
    "    xgb_params=CONFIG['XGB_PARAMS'],\n",
    "    plot_results=True\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ Pipeline execution completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis {#results}\n",
    "\n",
    "Let's analyze the results in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "model = results['model']\n",
    "metrics = results['metrics']\n",
    "predictions = results['predictions']\n",
    "feature_names = results['feature_names']\n",
    "\n",
    "print(\"üìä MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Format and display metrics\n",
    "metric_descriptions = {\n",
    "    'MAE': 'Mean Absolute Error',\n",
    "    'MSE': 'Mean Squared Error', \n",
    "    'RMSE': 'Root Mean Squared Error',\n",
    "    'R2': 'R-squared (Coefficient of Determination)',\n",
    "    'MAPE': 'Mean Absolute Percentage Error (%)',\n",
    "    'MASE': 'Mean Absolute Scaled Error',\n",
    "    'Directional_Accuracy': 'Directional Accuracy (%)'\n",
    "}\n",
    "\n",
    "for metric, value in metrics.items():\n",
    "    desc = metric_descriptions.get(metric, metric)\n",
    "    if metric in ['MAPE', 'Directional_Accuracy']:\n",
    "        print(f\"{desc:35s}: {value:8.2f}%\")\n",
    "    else:\n",
    "        print(f\"{desc:35s}: {value:8.6f}\")\n",
    "\n",
    "print(f\"\\nüìà Number of selected features: {len(feature_names)}\")\n",
    "print(f\"üß† Model type: XGBoost Regressor\")\n",
    "print(f\"üéØ Best iteration: {model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results visualization\n",
    "y_test = predictions['y_test']\n",
    "y_pred = predictions['y_pred']\n",
    "y_naive = predictions['y_naive']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Cryptocurrency Volatility Forecasting Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Time series comparison\n",
    "axes[0, 0].plot(y_test.index, y_test.values, label='Actual', alpha=0.8, linewidth=1.5)\n",
    "axes[0, 0].plot(y_test.index, y_pred, label='XGBoost', alpha=0.8, linewidth=1.5)\n",
    "axes[0, 0].plot(y_test.index, y_naive.values, label='Naive', alpha=0.6, linewidth=1, linestyle='--')\n",
    "axes[0, 0].set_title('Volatility Forecasts Over Time')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Realized Volatility')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Actual vs Predicted scatter\n",
    "axes[0, 1].scatter(y_test.values, y_pred, alpha=0.6, s=20, color='blue', label='XGBoost')\n",
    "axes[0, 1].scatter(y_test.values, y_naive.values, alpha=0.4, s=20, color='orange', label='Naive')\n",
    "min_val, max_val = y_test.min(), y_test.max()\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect')\n",
    "axes[0, 1].set_title('Actual vs Predicted')\n",
    "axes[0, 1].set_xlabel('Actual Volatility')\n",
    "axes[0, 1].set_ylabel('Predicted Volatility')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals analysis\n",
    "residuals = y_test.values - y_pred\n",
    "axes[0, 2].scatter(y_pred, residuals, alpha=0.6, s=20)\n",
    "axes[0, 2].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "axes[0, 2].set_title('Residuals vs Predicted')\n",
    "axes[0, 2].set_xlabel('Predicted Volatility')\n",
    "axes[0, 2].set_ylabel('Residuals')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuals distribution\n",
    "axes[1, 0].hist(residuals, bins=30, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
    "axes[1, 0].axvline(residuals.mean(), color='red', linestyle='--', alpha=0.8, label=f'Mean: {residuals.mean():.6f}')\n",
    "axes[1, 0].set_title('Residuals Distribution')\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Model comparison (MAE)\n",
    "mae_xgb = metrics['MAE']\n",
    "mae_naive = np.mean(np.abs(y_test.values - y_naive.values))\n",
    "models = ['XGBoost', 'Naive']\n",
    "mae_scores = [mae_xgb, mae_naive]\n",
    "colors = ['blue', 'orange']\n",
    "\n",
    "bars = axes[1, 1].bar(models, mae_scores, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_title('Model Comparison (MAE)')\n",
    "axes[1, 1].set_ylabel('Mean Absolute Error')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, mae_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{score:.6f}', ha='center', va='bottom')\n",
    "\n",
    "# 6. Cumulative absolute errors\n",
    "cum_abs_error_xgb = np.cumsum(np.abs(y_test.values - y_pred))\n",
    "cum_abs_error_naive = np.cumsum(np.abs(y_test.values - y_naive.values))\n",
    "\n",
    "axes[1, 2].plot(y_test.index, cum_abs_error_xgb, label='XGBoost', linewidth=2)\n",
    "axes[1, 2].plot(y_test.index, cum_abs_error_naive, label='Naive', linewidth=2)\n",
    "axes[1, 2].set_title('Cumulative Absolute Error')\n",
    "axes[1, 2].set_xlabel('Date')\n",
    "axes[1, 2].set_ylabel('Cumulative |Error|')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print improvement metrics\n",
    "improvement_mae = ((mae_naive - mae_xgb) / mae_naive) * 100\n",
    "mase = metrics['MASE']\n",
    "\n",
    "print(f\"\\nüìà MODEL IMPROVEMENT ANALYSIS\")\n",
    "print(f\"=\" * 40)\n",
    "print(f\"MAE Improvement over Naive: {improvement_mae:+.2f}%\")\n",
    "print(f\"MASE Score: {mase:.4f} ({'Better' if mase < 1 else 'Worse'} than naive)\")\n",
    "if 'Directional_Accuracy' in metrics:\n",
    "    print(f\"Directional Accuracy: {metrics['Directional_Accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Interpretation {#interpretation}\n",
    "\n",
    "Let's examine the most important features and understand what drives volatility predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"üîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get feature importance\n",
    "importance_gain = model.get_score(importance_type='gain')\n",
    "importance_weight = model.get_score(importance_type='weight')\n",
    "importance_cover = model.get_score(importance_type='cover')\n",
    "\n",
    "# Create comprehensive feature importance DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': list(importance_gain.keys()),\n",
    "    'gain': [importance_gain.get(f, 0) for f in importance_gain.keys()],\n",
    "    'weight': [importance_weight.get(f, 0) for f in importance_gain.keys()],\n",
    "    'cover': [importance_cover.get(f, 0) for f in importance_gain.keys()]\n",
    "})\n",
    "\n",
    "# Sort by gain and display top features\n",
    "feature_importance_df = feature_importance_df.sort_values('gain', ascending=False)\n",
    "top_features =# filepath: examples/crypto_volatility_forecasting.ipynb
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency Volatility Forecasting with Dask\n",
    "\n",
    "This notebook demonstrates the complete pipeline for cryptocurrency volatility forecasting using:\n",
    "\n",
    "- **Distributed Computing**: Dask for memory-efficient processing\n",
    "- **Feature Engineering**: TSFresh with time series rolling\n",
    "- **Machine Learning**: XGBoost with DMatrix optimization\n",
    "- **Professional Evaluation**: Comprehensive metrics and visualization\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Data Loading and Preparation](#data-loading)\n",
    "3. [Pipeline Execution](#pipeline)\n",
    "4. [Results Analysis](#results)\n",
    "5. [Model Interpretation](#interpretation)\n",
    "6. [Production Deployment](#deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration {#setup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom pipeline\n",
    "from crypto_volatility_toolkit import DaskCryptoVolatilityPipeline, create_dask_client\n",
    "from tsfresh.feature_extraction import EfficientFCParameters, MinimalFCParameters\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'TARGET_COIN': 'ethereum',\n",
    "    'LOOKBACK_DAYS': 365,\n",
    "    'TOP_N_COINS': 10,\n",
    "    'TIMEZONE': 'Europe/Madrid',\n",
    "    \n",
    "    # Pipeline parameters\n",
    "    'MAX_TIMESHIFT': 7,\n",
    "    'TEST_SIZE': 0.2,\n",
    "    'FDR_LEVEL': 0.05,\n",
    "    \n",
    "    # Dask configuration\n",
    "    'N_WORKERS': 4,\n",
    "    'THREADS_PER_WORKER': 2,\n",
    "    'MEMORY_LIMIT': '4GB',\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    'XGB_PARAMS': {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation {#data-loading}\n",
    "\n",
    "Load cryptocurrency data from your existing pipeline or create synthetic data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load your existing data\n",
    "# Uncomment and modify these lines to use your actual data\n",
    "\n",
    "# import dotenv\n",
    "# dotenv.load_dotenv()\n",
    "\n",
    "# # Use your existing API functions\n",
    "# from api_wrappers import (\n",
    "#     cg_universe, cgpriceactiondaily, deribit_dvol_daily_multi,\n",
    "#     dune_metrics_daily, fetch_fred_series_df\n",
    "# )\n",
    "\n",
    "# # Load real data\n",
    "# coins = cg_universe(CONFIG['TOP_N_COINS'], cg_headers=None)\n",
    "# price_data = cgpriceactiondaily(coins, CONFIG['LOOKBACK_DAYS'], CONFIG['TIMEZONE'], None)\n",
    "# dvol_data = deribit_dvol_daily_multi(['BTC', 'ETH'], CONFIG['LOOKBACK_DAYS'], CONFIG['TIMEZONE'])\n",
    "# macro_data = fetch_fred_series_df(['VIXCLS'], '2020-01-01', CONFIG['TIMEZONE'])\n",
    "\n",
    "# # Combine datasets\n",
    "# df_combined = price_data.join([dvol_data, macro_data], how='outer').fillna(method='ffill').dropna()\n",
    "\n",
    "# Option 2: Create synthetic data for demonstration\n",
    "print(\"üìä Creating synthetic cryptocurrency data for demonstration...\")\n",
    "\n",
    "# Generate realistic synthetic data\n",
    "np.random.seed(42)\n",
    "n_days = CONFIG['LOOKBACK_DAYS']\n",
    "dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n",
    "\n",
    "# Simulate price data with realistic volatility clustering\n",
    "def simulate_garch_returns(n, alpha=0.1, beta=0.8, omega=0.01):\n",
    "    \"\"\"Simulate GARCH(1,1) returns for realistic volatility clustering\"\"\"\n",
    "    returns = np.zeros(n)\n",
    "    sigma2 = np.zeros(n)\n",
    "    sigma2[0] = omega / (1 - alpha - beta)\n",
    "    \n",
    "    for t in range(1, n):\n",
    "        sigma2[t] = omega + alpha * returns[t-1]**2 + beta * sigma2[t-1]\n",
    "        returns[t] = np.sqrt(sigma2[t]) * np.random.randn()\n",
    "    \n",
    "    return returns\n",
    "\n",
    "# Create synthetic data\n",
    "btc_returns = simulate_garch_returns(n_days) * 0.03\n",
    "eth_returns = simulate_garch_returns(n_days) * 0.04\n",
    "sol_returns = simulate_garch_returns(n_days) * 0.06\n",
    "\n",
    "# Convert to prices\n",
    "btc_prices = 50000 * np.exp(np.cumsum(btc_returns))\n",
    "eth_prices = 3000 * np.exp(np.cumsum(eth_returns))\n",
    "sol_prices = 100 * np.exp(np.cumsum(sol_returns))\n",
    "\n",
    "# Create DataFrame\n",
    "df_combined = pd.DataFrame({\n",
    "    'prices_bitcoin': btc_prices,\n",
    "    'prices_ethereum': eth_prices,\n",
    "    'prices_solana': sol_prices,\n",
    "    'dvol_btc': np.random.gamma(2, 0.5, n_days) + 20,\n",
    "    'dvol_eth': np.random.gamma(2, 0.6, n_days) + 25,\n",
    "    'vixcls': np.random.gamma(3, 0.3, n_days) + 15,\n",
    "    'dgs10': np.random.normal(3.5, 0.5, n_days),\n",
    "    'usd_trade_weighted_index': np.random.normal(100, 2, n_days)\n",
    "}, index=dates)\n",
    "\n",
    "print(f\"‚úÖ Data created with shape: {df_combined.shape}\")\n",
    "print(f\"üìÖ Date range: {df_combined.index.min().date()} to {df_combined.index.max().date()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable (realized volatility)\n",
    "target_coin = CONFIG['TARGET_COIN']\n",
    "print(f\"üéØ Creating target variable for {target_coin}\")\n",
    "\n",
    "# Calculate log returns\n",
    "df_combined[f'log_returns_{target_coin}'] = np.log(df_combined[f'prices_{target_coin}']).diff()\n",
    "\n",
    "# Calculate realized volatility (absolute returns)\n",
    "df_combined[f'realized_vol_{target_coin}'] = df_combined[f'log_returns_{target_coin}'].abs()\n",
    "\n",
    "# Create features matrix (first differences for stationarity)\n",
    "X_raw = df_combined.select_dtypes(include=[np.number]).diff().dropna()\n",
    "\n",
    "# Create target (next-day realized volatility)\n",
    "y = df_combined[f'realized_vol_{target_coin}'].shift(-1).dropna()\n",
    "\n",
    "# Align X and y\n",
    "common_idx = X_raw.index.intersection(y.index)\n",
    "X = X_raw.loc[common_idx]\n",
    "y = y.loc[common_idx]\n",
    "\n",
    "print(f\"üìä Features matrix shape: {X.shape}\")\n",
    "print(f\"üéØ Target series length: {len(y)}\")\n",
    "print(f\"üìà Target statistics:\")\n",
    "print(f\"   Mean: {y.mean():.6f}\")\n",
    "print(f\"   Std:  {y.std():.6f}\")\n",
    "print(f\"   Min:  {y.min():.6f}\")\n",
    "print(f\"   Max:  {y.max():.6f}\")\n",
    "\n",
    "# Plot target variable\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Time series plot\n",
    "axes[0].plot(y.index, y.values, alpha=0.7)\n",
    "axes[0].set_title(f'{target_coin.title()} Realized Volatility')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Realized Volatility')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution plot\n",
    "axes[1].hist(y.values, bins=50, alpha=0.7, density=True)\n",
    "axes[1].set_title('Realized Volatility Distribution')\n",
    "axes[1].set_xlabel('Realized Volatility')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline Execution {#pipeline}\n",
    "\n",
    "Now we'll run the complete Dask-based volatility forecasting pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dask client\n",
    "print(\"üöÄ Creating Dask client...\")\n",
    "client = create_dask_client(\n",
    "    n_workers=CONFIG['N_WORKERS'],\n",
    "    threads_per_worker=CONFIG['THREADS_PER_WORKER'],\n",
    "    memory_limit=CONFIG['MEMORY_LIMIT']\n",
    ")\n",
    "\n",
    "print(f\"üåê Dask dashboard: {client.dashboard_link}\")\n",
    "print(f\"üë• Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "\n",
    "# Display client info\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "print(\"üèóÔ∏è Initializing Dask Crypto Volatility Pipeline...\")\n",
    "pipeline = DaskCryptoVolatilityPipeline(client=client, random_state=42)\n",
    "\n",
    "print(\"‚úÖ Pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline\n",
    "print(\"üöÄ Running complete volatility forecasting pipeline...\")\n",
    "print(\"This may take several minutes depending on your system...\")\n",
    "\n",
    "results = pipeline.run_complete_pipeline(\n",
    "    X_wide=X,\n",
    "    y=y,\n",
    "    test_size=CONFIG['TEST_SIZE'],\n",
    "    max_timeshift=CONFIG['MAX_TIMESHIFT'],\n",
    "    fc_parameters=EfficientFCParameters(),  # Use EfficientFCParameters for balance of speed/features\n",
    "    fdr_level=CONFIG['FDR_LEVEL'],\n",
    "    xgb_params=CONFIG['XGB_PARAMS'],\n",
    "    plot_results=True\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ Pipeline execution completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Analysis {#results}\n",
    "\n",
    "Let's analyze the results in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "model = results['model']\n",
    "metrics = results['metrics']\n",
    "predictions = results['predictions']\n",
    "feature_names = results['feature_names']\n",
    "\n",
    "print(\"üìä MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Format and display metrics\n",
    "metric_descriptions = {\n",
    "    'MAE': 'Mean Absolute Error',\n",
    "    'MSE': 'Mean Squared Error', \n",
    "    'RMSE': 'Root Mean Squared Error',\n",
    "    'R2': 'R-squared (Coefficient of Determination)',\n",
    "    'MAPE': 'Mean Absolute Percentage Error (%)',\n",
    "    'MASE': 'Mean Absolute Scaled Error',\n",
    "    'Directional_Accuracy': 'Directional Accuracy (%)'\n",
    "}\n",
    "\n",
    "for metric, value in metrics.items():\n",
    "    desc = metric_descriptions.get(metric, metric)\n",
    "    if metric in ['MAPE', 'Directional_Accuracy']:\n",
    "        print(f\"{desc:35s}: {value:8.2f}%\")\n",
    "    else:\n",
    "        print(f\"{desc:35s}: {value:8.6f}\")\n",
    "\n",
    "print(f\"\\nüìà Number of selected features: {len(feature_names)}\")\n",
    "print(f\"üß† Model type: XGBoost Regressor\")\n",
    "print(f\"üéØ Best iteration: {model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results visualization\n",
    "y_test = predictions['y_test']\n",
    "y_pred = predictions['y_pred']\n",
    "y_naive = predictions['y_naive']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Cryptocurrency Volatility Forecasting Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Time series comparison\n",
    "axes[0, 0].plot(y_test.index, y_test.values, label='Actual', alpha=0.8, linewidth=1.5)\n",
    "axes[0, 0].plot(y_test.index, y_pred, label='XGBoost', alpha=0.8, linewidth=1.5)\n",
    "axes[0, 0].plot(y_test.index, y_naive.values, label='Naive', alpha=0.6, linewidth=1, linestyle='--')\n",
    "axes[0, 0].set_title('Volatility Forecasts Over Time')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Realized Volatility')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Actual vs Predicted scatter\n",
    "axes[0, 1].scatter(y_test.values, y_pred, alpha=0.6, s=20, color='blue', label='XGBoost')\n",
    "axes[0, 1].scatter(y_test.values, y_naive.values, alpha=0.4, s=20, color='orange', label='Naive')\n",
    "min_val, max_val = y_test.min(), y_test.max()\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect')\n",
    "axes[0, 1].set_title('Actual vs Predicted')\n",
    "axes[0, 1].set_xlabel('Actual Volatility')\n",
    "axes[0, 1].set_ylabel('Predicted Volatility')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals analysis\n",
    "residuals = y_test.values - y_pred\n",
    "axes[0, 2].scatter(y_pred, residuals, alpha=0.6, s=20)\n",
    "axes[0, 2].axhline(y=0, color='r', linestyle='--', alpha=0.8)\n",
    "axes[0, 2].set_title('Residuals vs Predicted')\n",
    "axes[0, 2].set_xlabel('Predicted Volatility')\n",
    "axes[0, 2].set_ylabel('Residuals')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuals distribution\n",
    "axes[1, 0].hist(residuals, bins=30, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
    "axes[1, 0].axvline(residuals.mean(), color='red', linestyle='--', alpha=0.8, label=f'Mean: {residuals.mean():.6f}')\n",
    "axes[1, 0].set_title('Residuals Distribution')\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Model comparison (MAE)\n",
    "mae_xgb = metrics['MAE']\n",
    "mae_naive = np.mean(np.abs(y_test.values - y_naive.values))\n",
    "models = ['XGBoost', 'Naive']\n",
    "mae_scores = [mae_xgb, mae_naive]\n",
    "colors = ['blue', 'orange']\n",
    "\n",
    "bars = axes[1, 1].bar(models, mae_scores, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_title('Model Comparison (MAE)')\n",
    "axes[1, 1].set_ylabel('Mean Absolute Error')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, mae_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{score:.6f}', ha='center', va='bottom')\n",
    "\n",
    "# 6. Cumulative absolute errors\n",
    "cum_abs_error_xgb = np.cumsum(np.abs(y_test.values - y_pred))\n",
    "cum_abs_error_naive = np.cumsum(np.abs(y_test.values - y_naive.values))\n",
    "\n",
    "axes[1, 2].plot(y_test.index, cum_abs_error_xgb, label='XGBoost', linewidth=2)\n",
    "axes[1, 2].plot(y_test.index, cum_abs_error_naive, label='Naive', linewidth=2)\n",
    "axes[1, 2].set_title('Cumulative Absolute Error')\n",
    "axes[1, 2].set_xlabel('Date')\n",
    "axes[1, 2].set_ylabel('Cumulative |Error|')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print improvement metrics\n",
    "improvement_mae = ((mae_naive - mae_xgb) / mae_naive) * 100\n",
    "mase = metrics['MASE']\n",
    "\n",
    "print(f\"\\nüìà MODEL IMPROVEMENT ANALYSIS\")\n",
    "print(f\"=\" * 40)\n",
    "print(f\"MAE Improvement over Naive: {improvement_mae:+.2f}%\")\n",
    "print(f\"MASE Score: {mase:.4f} ({'Better' if mase < 1 else 'Worse'} than naive)\")\n",
    "if 'Directional_Accuracy' in metrics:\n",
    "    print(f\"Directional Accuracy: {metrics['Directional_Accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Interpretation {#interpretation}\n",
    "\n",
    "Let's examine the most important features and understand what drives volatility predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"üîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get feature importance\n",
    "importance_gain = model.get_score(importance_type='gain')\n",
    "importance_weight = model.get_score(importance_type='weight')\n",
    "importance_cover = model.get_score(importance_type='cover')\n",
    "\n",
    "# Create comprehensive feature importance DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': list(importance_gain.keys()),\n",
    "    'gain': [importance_gain.get(f, 0) for f in importance_gain.keys()],\n",
    "    'weight': [importance_weight.get(f, 0) for f in importance_gain.keys()],\n",
    "    'cover': [importance_cover.get(f, 0) for f in importance_gain.keys()]\n",
    "})\n",
    "\n",
    "# Sort by gain and display top features\n",
    "feature_importance_df = feature_importance_df.sort_values('gain', ascending=False)\n",
    "top_features =