{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8602e90",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6400fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Utilities \n",
    "import  random, os, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, datetime as dt\n",
    "import dotenv, os, requests, time\n",
    "\n",
    "# Environment & Dask Client\n",
    "os.makedirs(\"OutputData\", exist_ok=True)\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(filename=\".env\"))\n",
    "\n",
    "# Dune Client\n",
    "from dune_client.client import DuneClient\n",
    "from dune_client.query import QueryBase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032c418",
   "metadata": {},
   "source": [
    "Key Constants and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd68b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Constants\n",
    "TARGET_COIN = \"ethereum\"\n",
    "BASE_FIAT   = \"usd\"\n",
    "TOP_N       = 20\n",
    "LOOKBACK_DAYS = 365\n",
    "START_DATE = (dt.datetime.now() - dt.timedelta(days=LOOKBACK_DAYS)).strftime(\"%Y-%m-%d\")\n",
    "TODAY= dt.date.today().strftime('%Y-%m-%d')\n",
    "TIMEZONE = \"Europe/Madrid\"\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "FREQUENCY = \"1D\"  \n",
    "TIME= 7  # days for rolling window\n",
    "# --- Dune API configuration ---\n",
    "DUNE_QUERIES = {\n",
    "    \"economic_security\": 1933076,\n",
    "    \"daily_dex_volume\": 4388,\n",
    "    \"btc_etf_flows\": 5795477,\n",
    "    \"eth_etf_flows\": 5795645,\n",
    "    \"total_defi_users\": 2972,\n",
    "    \"median_gas\": 2981260,\n",
    "}\n",
    "DUNE_API_KEY = os.getenv(\"DUNE_API_KEY\")\n",
    "DUNE_CSV_PATH = \"OutputData/Dune_Metrics.csv\"\n",
    "\n",
    "# --- FRED API configuration ---\n",
    "FRED_API_KEY= os.getenv(\"FRED_API_KEY\")\n",
    "FRED_KNOWN = {\n",
    "    \"VIXCLS\":   \"vix_equity_vol\",            # CBOE VIX (Equity market volatility index)\n",
    "    \"MOVE\":     \"move_bond_vol\",             # ICE BofA MOVE Index (Bond market volatility)\n",
    "    \"OVXCLS\":   \"ovx_oil_vol\",               # CBOE Crude Oil Volatility Index (Oil market volatility)\n",
    "    \"GVZCLS\":   \"gvz_gold_vol\",              # CBOE Gold Volatility Index (Gold market volatility)\n",
    "    \"DTWEXBGS\": \"usd_trade_weighted_index\",  # Trade-Weighted U.S. Dollar Index (Broad Goods)\n",
    "    \"DGS2\":     \"us_2y_treasury_yield\",      # U.S. 2-Year Treasury Yield (constant maturity)\n",
    "    \"DGS10\":    \"us_10y_treasury_yield\",     # U.S. 10-Year Treasury Yield (constant maturity)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d590ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Modules\n",
    "# Expects these globals to be defined by the notebook:\n",
    "# TIMEZONE, DAYS_BACK, CG_TOP_N, CG_HEADERS,\n",
    "# DUNE_CSV_PATH, FRED_API_KEY (env), FRED_KNOWN, DUNE_QUERIES, DUNE_API_KEY (env), \n",
    "# TARGET_COIN, BASE_FIAT, FREQUENCY, LOOKBACK_DAYS, START_DATE, TODAY, TOP_N, \n",
    "\n",
    "# --- CoinGecko Investment Universe (V1)  ---\n",
    "def CoinGecko_GetUniverse(n, cg_api_key=os.getenv(\"COINGECKO_API_KEY\")):\n",
    "    \"\"\"\n",
    "        Top n cryptocurrency IDs from CoinGecko API sorted by market cap.\n",
    "            Parameters:\n",
    "            - n: Number of top coins to retrieve\n",
    "            - cg_api_key: CoinGecko API key\n",
    "        Returns: n\n",
    "            - numpy array of identifiers or dictionary containing both formats\n",
    "        \"\"\"\n",
    "    if cg_api_key is None: cg_api_key = os.getenv(\"COINGECKO_API_KEY\")\n",
    "    cg_headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x_cg_demo_api_key\": cg_api_key\n",
    "        }\n",
    "    url = \"https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd\"\n",
    "    js = requests.get(url, headers=cg_headers).json()\n",
    "    df = pd.DataFrame(js)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        return  df.head(n)['id'].values \n",
    "    except: \n",
    "        return print(\"Error Getting Coin Id's: \", df.loc['error_message'].values)\n",
    "\n",
    "# --- CoinGecko Investment Universe (V2) Returns tickers e.g. ETH, ids e.g. ethereum, or both ---\n",
    "def CoinGecko_GetUniverseV2(n=TOP_N, output_format=\"ids\", \n",
    "                            cg_api_key=os.getenv(\"COINGECKO_API_KEY\")):\n",
    "    \"\"\"\n",
    "    Top n cryptocurrency tickers and/or ids from CoinGecko API by market cap.\n",
    "        Parameters:\n",
    "        - n: Number of top coins to retrieve\n",
    "        - output_format: Format of identifiers to return\n",
    "        * \"ids\": CoinGecko IDs (e.g., \"bitcoin\", \"ethereum\")\n",
    "        * \"symbols\": Ticker symbols (e.g., \"BTC\", \"ETH\") for use with Binance API\n",
    "        * \"both\": Returns a dict containing both formats\n",
    "        - cg_api_key: CoinGecko API key\n",
    "        Returns: \n",
    "        - numpy array of identifiers or dictionary containing both formats\n",
    "    \"\"\"\n",
    "    if cg_api_key is None: print(\"No API Key Available\")\n",
    "    cg_headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"x_cg_demo_api_key\": cg_api_key\n",
    "    }\n",
    "    url = \"https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc\"\n",
    "    js = requests.get(url, headers=cg_headers).json()\n",
    "    df = pd.DataFrame(js)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        if output_format == \"ids\":\n",
    "            result = df.head(n)['id'].values\n",
    "            print(f\"Retrieved {len(result)} coin IDs by market cap from CoinGecko\")\n",
    "            return result\n",
    "        elif output_format == \"symbols\":\n",
    "            result = df.head(n)['symbol'].str.upper().values\n",
    "            print(f\"Retrieved {len(result)} coin symbols by market cap from CoinGecko\")\n",
    "            return result\n",
    "        elif output_format == \"both\":\n",
    "            ids = df.head(n)['id'].values\n",
    "            symbols = df.head(n)['symbol'].str.upper().values\n",
    "            print(f\"Retrieved {len(ids)} coins by market cap from CoinGecko\")\n",
    "            return {\"ids\": ids, \"ticker\": symbols}\n",
    "        else:\n",
    "            raise ValueError(\"output_format must be 'ids', 'symbols', or 'both'\")\n",
    "    except: \n",
    "        return print(\"Error Getting Coin Id's: \", df.loc['error_message'].values)\n",
    "\n",
    "# --- CoinGecko Price Data ---\n",
    "def CoinGecko_GetPriceAction(coins, start= START_DATE, \n",
    "                             tz=TIMEZONE, cg_api_key=os.getenv(\"COINGECKO_API_KEY\"), freq=FREQUENCY):\n",
    "    \"\"\"\"\n",
    "    Only works up to past 365 days, lose intraday data if > 90 days due to API public demo limits.\n",
    "    For longer history, use Binance_GetPriceData below.\n",
    "    \"\"\"\n",
    "    end_timestamp   = int(dt.datetime.now().timestamp()) * 1000\n",
    "    start_timestamp = int(pd.to_datetime(start).timestamp()) * 1000\n",
    "    cg_headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"x_cg_demo_api_key\": cg_api_key\n",
    "    }\n",
    "    outbig=None\n",
    "    for c in coins:\n",
    "        try:\n",
    "            url = f\"https://api.coingecko.com/api/v3/coins/{c}/market_chart/range?vs_currency=usd&from={start_timestamp}&to={end_timestamp}\"\n",
    "            js = requests.get(url, headers=cg_headers).json()\n",
    "            outsmall = None\n",
    "            for column in js:\n",
    "                timestamps = pd.to_datetime([x[0]for x in js[column]], unit='ms').tz_localize(TIMEZONE)\n",
    "                values= [x[1] for x in js[column]]\n",
    "                if outsmall is None: outsmall= pd.DataFrame(data= values, columns= [(column+'_'+c)], index= timestamps)\n",
    "                else: outsmall[(column+'_'+c)] = values\n",
    "            outsmall[['prices_'+c, 'market_caps_'+c, 'total_volumes_'+c]] = outsmall[['prices_'+c, 'market_caps_'+c, 'total_volumes_'+c]].apply(pd.to_numeric, errors='coerce')\n",
    "            outsmall.index.name = 'date'\n",
    "            pricesandmc= outsmall[['prices_'+c, 'market_caps_'+c]].resample(freq).last().dropna()\n",
    "            volumes= outsmall[['total_volumes_'+c]].resample(freq).sum().dropna()\n",
    "            outsmall= pricesandmc.join(volumes, how='inner')\n",
    "            time.sleep(6)\n",
    "            if outbig is None: outbig= outsmall\n",
    "            else: outbig= outbig.join(outsmall, how='inner')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing data for {c}: {e}\")\n",
    "            continue\n",
    "            time.sleep(6)\n",
    "    return outbig\n",
    "\n",
    "# --- CoinGecko OHLC Data ---\n",
    "def CoinGecko_GetOHLC(coins, days=LOOKBACK_DAYS, vs_currency=\"usd\"):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with daily open, high, low, close for each coin in coins.\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    for coin in coins:\n",
    "        try:\n",
    "            url = f\"https://api.coingecko.com/api/v3/coins/{coin}/ohlc?vs_currency={vs_currency}&days={days}\"\n",
    "            # CoinGecko returns [timestamp, open, high, low, close] in ms, daily\n",
    "            js = requests.get(url).json()\n",
    "            if not js or not isinstance(js, list):\n",
    "                print(f\"No OHLC data for {coin}\")\n",
    "                continue\n",
    "            df = pd.DataFrame(js, columns=[\"ts\", f\"open_{coin}\", f\"high_{coin}\", f\"low_{coin}\", f\"close_{coin}\"])\n",
    "            df[\"date\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\").dt.date\n",
    "            df = df.drop(columns=[\"ts\"]).set_index(\"date\")\n",
    "            if out is None:\n",
    "                out = df\n",
    "            else:\n",
    "                out = out.join(df, how=\"outer\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching OHLC for {coin}: {e}\")\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "# --- CoinGecko Extended Historical Data (with pagination) ---\n",
    "def CoinGecko_GetHistoricalData_Paginated(coin_ids, vs_currency=\"usd\", max_days=365, \n",
    "                                          step_days=90, timezone=TIMEZONE, \n",
    "                                          cg_api_key=os.getenv(\"COINGECKO_API_KEY\")):\n",
    "    \"\"\"\n",
    "    Gets extended historical price data from CoinGecko using pagination.\n",
    "    Parameters:\n",
    "        coin_id: CoinGecko coin ID (e.g., 'bitcoin')\n",
    "        vs_currency: Base currency (e.g., 'usd')\n",
    "        max_days: Maximum days to fetch\n",
    "        step_days: Days per request (smaller = more requests but more granular data)\n",
    "        timezone: Timezone for the returned DataFrame index\n",
    "        cg_api_key: CoinGecko API key\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with prices, market caps and volumes with datetime index\n",
    "    \"\"\"\n",
    "    output=None\n",
    "    for coin_id in coin_ids:\n",
    "        full_prices = []\n",
    "        full_market_caps = []\n",
    "        full_volumes = []\n",
    "        cg_headers = {\"accept\": \"application/json\", \"x_cg_demo_api_key\": cg_api_key}\n",
    "        # Start from today and work backwards\n",
    "        end_date = dt.datetime.now()\n",
    "        current_end = int(end_date.timestamp())\n",
    "        target_start_date = end_date - dt.timedelta(days=max_days)\n",
    "        print(f\"Fetching data for {coin_id} from {end_date.date()} back to {target_start_date.date()}\")\n",
    "        api_requests = 0\n",
    "        while True:\n",
    "            # Calculate window\n",
    "            current_start = int((end_date - dt.timedelta(days=step_days)).timestamp())\n",
    "            # Build request\n",
    "            url = f\"https://api.coingecko.com/api/v3/coins/{coin_id}/market_chart/range\"\n",
    "            params = {\n",
    "                \"vs_currency\": vs_currency,\n",
    "                \"from\": current_start,\n",
    "                \"to\": current_end\n",
    "            }\n",
    "            # Make request\n",
    "            response = requests.get(url, headers=cg_headers, params=params)\n",
    "            data = response.json()\n",
    "            api_requests += 1\n",
    "            if 'prices' not in data:\n",
    "                print(f\"No more data available or error after {api_requests} requests\")\n",
    "                if 'error' in data:\n",
    "                    print(f\"Error: {data['error']}\")\n",
    "                break\n",
    "            # Process data - extract timestamps and values\n",
    "            prices = data.get('prices', [])\n",
    "            market_caps = data.get('market_caps', [])\n",
    "            volumes = data.get('total_volumes', [])\n",
    "            if not prices:\n",
    "                break\n",
    "            # Add to collections (older data gets added at the beginning)\n",
    "            full_prices = prices + full_prices\n",
    "            full_market_caps = market_caps + full_market_caps\n",
    "            full_volumes = volumes + full_volumes\n",
    "            print(f\"Request #{api_requests}: Got {len(prices)} price points\")\n",
    "            # Move window back in time\n",
    "            end_date = dt.datetime.fromtimestamp(current_start)\n",
    "            current_end = current_start - 1\n",
    "            # Check if we've gone far enough\n",
    "            if end_date <= target_start_date:\n",
    "                print(f\"Reached target date\")\n",
    "                break\n",
    "            # Respect CoinGecko's rate limits\n",
    "            time.sleep(3)\n",
    "        # Create DataFrame from collected data\n",
    "        if not full_prices:\n",
    "            print(\"No data collected\")\n",
    "            return pd.DataFrame()\n",
    "        # Create individual DataFrames for each data type\n",
    "        df_prices = pd.DataFrame(full_prices, columns=['timestamp', f'prices_{coin_id}'])\n",
    "        df_prices['timestamp'] = pd.to_datetime(df_prices['timestamp'], unit='ms')\n",
    "        df_mcaps = pd.DataFrame(full_market_caps, columns=['timestamp', f'market_caps_{coin_id}'])\n",
    "        df_mcaps['timestamp'] = pd.to_datetime(df_mcaps['timestamp'], unit='ms')\n",
    "        df_volumes = pd.DataFrame(full_volumes, columns=['timestamp', f'total_volumes_{coin_id}'])\n",
    "        df_volumes['timestamp'] = pd.to_datetime(df_volumes['timestamp'], unit='ms')\n",
    "        # Merge the dataframes\n",
    "        df = df_prices.merge(df_mcaps, on='timestamp', how='outer')\n",
    "        df = df.merge(df_volumes, on='timestamp', how='outer')\n",
    "        # Set index and timezone\n",
    "        df = df.set_index('timestamp')\n",
    "        if timezone:\n",
    "            df.index = df.index.tz_localize(timezone)\n",
    "        df.index.name = 'date'\n",
    "        print(f\"Total data points: {len(df)}\")\n",
    "        print(f\"Data ranges from {df.index.min().date()} to {df.index.max().date()}\")\n",
    "        if output is None:\n",
    "            output = df\n",
    "        else:\n",
    "            output = output.join(df, how='outer')\n",
    "    return output\n",
    "# --- Deribit DVOL ---\n",
    "def Deribit_GetDVOL(currencies, days, timezone, resolution=\"1D\"):\n",
    "    out = None\n",
    "    end   = int(dt.datetime.now().timestamp()) * 1000\n",
    "    start = int((dt.datetime.now() - dt.timedelta(days=days)).timestamp()) * 1000\n",
    "    count=0\n",
    "    for cur in currencies:\n",
    "        js = requests.post(\n",
    "            \"https://www.deribit.com/api/v2/\",\n",
    "            json={\"method\": \"public/get_volatility_index_data\",\n",
    "                    \"params\": {\"currency\": cur, \"resolution\": resolution,\n",
    "                                \"end_timestamp\": end, \"start_timestamp\": start}}\n",
    "        ).json()\n",
    "        data = js.get(\"result\", {}).get(\"data\", [])\n",
    "        if not data:\n",
    "            continue\n",
    "        d = pd.DataFrame(data, columns=[\"t\",\"open\",\"high\",\"low\",\"dvol\"])\n",
    "        d[\"t\"] = pd.to_datetime(d[\"t\"], unit=\"ms\")\n",
    "        df = d.set_index(\"t\")[[\"dvol\"]].rename(columns={\"dvol\": f\"dvol_{cur.lower()}\"})\n",
    "        df.index = df.index.tz_localize('Europe/Madrid')\n",
    "        df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "        df.index.name = \"date\"\n",
    "        if count ==0: out = df\n",
    "        else: out = out.join(df, how='inner')\n",
    "        count= count+1\n",
    "    return out\n",
    "\n",
    "# --- Dune (CSV) ---    \n",
    "def Dune_FromCSV(path, timezone):\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(path, index_col=None)\n",
    "    dt_col = None\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            pd.to_datetime(df[c], errors=\"raise\")\n",
    "            dt_col = c\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if dt_col is None and \"date\" in df.columns:\n",
    "        dt_col = \"date\"\n",
    "    if dt_col is None:\n",
    "        return pd.DataFrame()\n",
    "    df = df.rename(columns={dt_col: \"date\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.set_index(\"date\")\n",
    "    df.index = df.index.tz_localize(timezone)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    df.index.name = \"date\"\n",
    "    df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "    return df\n",
    "\n",
    "# --- Dune ---\n",
    "def Dune_GetQueries(query_ids, timezone, dune_api_key=None):\n",
    "    dune = DuneClient(api_key=dune_api_key or os.environ.get(\"DUNE_API_KEY\"),\n",
    "                       base_url=\"https://api.dune.com\")\n",
    "    out = None\n",
    "    for qid in query_ids:\n",
    "        try:\n",
    "            q = QueryBase(query_id=qid)\n",
    "            df = dune.run_query_dataframe(query=q, ping_frequency=2, batch_size=365)\n",
    "            ok = False\n",
    "            for col in list(df.columns):\n",
    "                try:\n",
    "                    pd.to_datetime(df[col], errors=\"raise\")\n",
    "                    df = df.rename(columns={col: \"date\"}).set_index(\"date\")\n",
    "                    ok = True\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            if not ok and not isinstance(df.index, pd.DatetimeIndex):\n",
    "                continue\n",
    "            if isinstance(df.index, pd.DatetimeIndex):\n",
    "                df.index = df.index.tz_localize(timezone)\n",
    "            df.columns = [c.lower() for c in df.columns]\n",
    "            df.index.name = \"date\"\n",
    "            df = df.resample(\"1D\").last().dropna(how=\"any\")\n",
    "            out = df if out is None else out.join(df, how=\"inner\")\n",
    "        except:\n",
    "            continue\n",
    "    return out if out is not None else print('Error Fetching Dune Queries')\n",
    "\n",
    "# --- FRED ---\n",
    "def Fred_GetSeries(series_ids= FRED_KNOWN, start=START_DATE, timezone=TIMEZONE, fred_api_key=FRED_API_KEY):\n",
    "    key = fred_api_key or os.getenv(\"FRED_API_KEY\")\n",
    "    if not key:\n",
    "        return print(\"No API Key Available\")\n",
    "    base = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "    df= None\n",
    "    for sid in series_ids:\n",
    "        try:\n",
    "            js = requests.get(base, params={\n",
    "                    \"series_id\": sid, \"api_key\": fred_api_key, \"file_type\": \"json\",\n",
    "                    \"observation_start\": start\n",
    "                }).json()\n",
    "            obs= pd.DataFrame(js['observations'])\n",
    "            index = pd.DatetimeIndex(obs['date'], freq='infer', tz=timezone)\n",
    "            obs = obs.set_index(index)['value'].rename(FRED_KNOWN[sid])\n",
    "            obs= pd.to_numeric(obs, errors='coerce')\n",
    "            if df is not None: df= pd.merge(left= df, right=obs, left_index=True, right_index=True)\n",
    "            else: df = obs\n",
    "        except:\n",
    "            print(\"error fetching:\", series_ids[sid])\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "    if df is not None:  return df.asfreq('D', method='ffill')\n",
    "    else: return print('Error Compiling Data')\n",
    "\n",
    "# --- Binance Price Action ---\n",
    "def Binance_GetPriceAction(ids=None, tickers=None,  interval=\"1d\", max_days=365, timezone=TIMEZONE, top_n=TOP_N):\n",
    "    \"\"\"\n",
    "    Gets extended OHLCV data from Binance using pagination to overcome the 1000 candle limit.\n",
    "    Parameters:\n",
    "        symbols: List of trading symbols (e.g., ['BTC', 'ETH'])\n",
    "        interval: Candlestick interval (1m, 3m, 5m, 15m, 30m, 1h, 2h, 4h, 6h, 8h, 12h, 1d, 3d, 1w, 1M)\n",
    "        max_days: Maximum number of days of history to fetch\n",
    "        timezone: Timezone for the returned DataFrame index\n",
    "    Returns:\n",
    "        DataFrame with OHLCV data and datetime index\n",
    "    \"\"\"\n",
    "    outbig = None\n",
    "    if ids is None or tickers is None:\n",
    "        ids, tickers = CoinGecko_GetUniverseV2(n=top_n, output_format=\"both\", \n",
    "                            cg_api_key=os.getenv(\"COINGECKO_API_KEY\")).values()\n",
    "    for id, ticker in zip(ids, tickers):\n",
    "        ticker = ticker.upper()\n",
    "        print(f\"Fetching {interval} candles for {id} going back {max_days} days...\")\n",
    "        # Pagination variables\n",
    "        full_data = []\n",
    "        end_time = int(dt.datetime.now().timestamp() * 1000)  # Current time in milliseconds\n",
    "        start_date_target = dt.datetime.now() - dt.timedelta(days=max_days)  # Target oldest date\n",
    "        api_requests = 0\n",
    "        # Loop until we have enough data or run out of history\n",
    "        while True:\n",
    "            url = \"https://api.binance.com/api/v3/klines\"\n",
    "            params = {\n",
    "                \"symbol\": ticker + \"USDT\",\n",
    "                \"interval\": interval,   \n",
    "                \"endTime\": end_time,\n",
    "                \"limit\": 1000\n",
    "            }\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            api_requests += 1\n",
    "            # Check if we got valid data\n",
    "            if not data or len(data) == 0 or (isinstance(data, dict) and 'code' in data):\n",
    "                print(f\"No more data available for {id} after {api_requests} requests\")\n",
    "                break\n",
    "            print(f\"Request #{api_requests}: Got {len(data)} candles for {id}\")\n",
    "            # Add to our collection (at the beginning since we're going backward in time)\n",
    "            full_data = data + full_data\n",
    "            # Get the oldest timestamp from this batch\n",
    "            oldest_timestamp = int(data[0][0])\n",
    "            oldest_date = dt.datetime.fromtimestamp(oldest_timestamp/1000)\n",
    "            # Check if we've gone far enough back in time\n",
    "            if oldest_date <= start_date_target:\n",
    "                print(f\"Reached target date ({start_date_target.date()}) for {id}\")\n",
    "                break\n",
    "            # Set end time to the start of the earliest candle minus 1ms for next iteration\n",
    "            end_time = oldest_timestamp - 1\n",
    "            # Avoid hitting rate limits\n",
    "            time.sleep(1)\n",
    "        # Process the collected data for this symbol\n",
    "        if not full_data:\n",
    "            print(f\"No data collected for {id}\")\n",
    "            continue\n",
    "        df = pd.DataFrame(full_data, columns=[\n",
    "            'timestamp', 'open', 'high', 'low', 'close', 'volume',\n",
    "            'close_time', 'quote_asset_volume', 'number_of_trades',\n",
    "            'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'\n",
    "        ])\n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "            df[col + '_' + id.lower()] = df[col]  # Create symbol-specific columns\n",
    "        df['date'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce', utc=True)\n",
    "        df = df.set_index('date').tz_convert(timezone)\n",
    "        symbol_cols = [f\"{col}_{id}\" for col in ['open', 'high', 'low', 'close', 'volume']]\n",
    "        df = df[symbol_cols]\n",
    "        print(f\"Total candles collected for {id}: {len(df)}\")\n",
    "        print(f\"Data ranges from {df.index.min().date()} to {df.index.max().date()}\")\n",
    "        if outbig is None:\n",
    "            outbig = df\n",
    "        else:\n",
    "            outbig = outbig.join(df, how='outer')\n",
    "    if outbig is None:\n",
    "        print(\"No data collected for any symbols.\")\n",
    "        return pd.DataFrame()\n",
    "    outbig = outbig.sort_index()\n",
    "    outbig.index.name = 'date'\n",
    "    print(f\"Combined data has {len(outbig)} rows\")\n",
    "    return outbig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac0585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering \n",
    "# Ta-Lib Technical Analysis Indicators\n",
    "import talib\n",
    "def Compute_TAIndicators(df, price_prefix=\"prices_\", rsi_period=14,\n",
    "                          macd_fast=12, macd_slow=26, macd_signal=9,\n",
    "                          sma_windows=(10,20,50), ema_windows=(10,20,50)):\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    price_cols = [c for c in df.columns if c.startswith(price_prefix)]\n",
    "    coins = [c[len(price_prefix):] for c in price_cols]\n",
    "    for coin in coins:\n",
    "        try:\n",
    "            p = df[f\"{price_prefix}{coin}\"]\n",
    "            out[f\"rsi{rsi_period}{coin}\"] = talib.RSI(p.values, timeperiod=rsi_period)\n",
    "            macd, macd_sig, macd_hist = talib.MACD(p.values, fastperiod=macd_fast, slowperiod=macd_slow, signalperiod=macd_signal)\n",
    "            out[f\"macd_{coin}\"] = macd; out[f\"macd_signal_{coin}\"] = macd_sig; out[f\"macd_hist_{coin}\"] = macd_hist\n",
    "            for w in sma_windows: out[f\"sma{w}_{coin}\"] = talib.SMA(p.values, timeperiod=w)\n",
    "            for w in ema_windows: out[f\"ema{w}_{coin}\"] = talib.EMA(p.values, timeperiod=w)\n",
    "            out[f\"bb_upper_{coin}\"], out[f\"bb_middle_{coin}\"], out[f\"bb_lower_{coin}\"] = talib.BBANDS(p.values)\n",
    "            out[f\"atr_{coin}\"] = talib.ATR(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
    "            out[f\"adx_{coin}\"] = talib.ADX(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
    "            out[f\"stoch_k_{coin}\"], out[f\"stoch_d_{coin}\"] = talib.STOCH(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
    "            out[f\"cci_{coin}\"] = talib.CCI(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
    "            out[f\"willr_{coin}\"] = talib.WILLR(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
    "            out[f\"mom_{coin}\"] = talib.MOM(p.values)\n",
    "            out[f\"roc_{coin}\"] = talib.ROC(p.values)\n",
    "            out[f\"obv_{coin}\"] = talib.OBV(p.values, df[f\"volume_{coin}\"])\n",
    "            out[f\"mfi_{coin}\"] = talib.MFI(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values, df[f\"volume_{coin}\"])\n",
    "        except: continue    \n",
    "    out.index = df.index\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec6e8b",
   "metadata": {},
   "source": [
    "Data Collection from Various APIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc902de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 20 coins by market cap from CoinGecko\n",
      "Fetching 1d candles for bitcoin going back 365 days...\n",
      "Request #1: Got 1000 candles for bitcoin\n",
      "Reached target date (2024-09-29) for bitcoin\n",
      "Total candles collected for bitcoin: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Fetching 1d candles for ethereum going back 365 days...\n",
      "Request #1: Got 1000 candles for ethereum\n",
      "Reached target date (2024-09-29) for ethereum\n",
      "Total candles collected for ethereum: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Fetching 1d candles for tether going back 365 days...\n",
      "No more data available for tether after 1 requests\n",
      "No data collected for tether\n",
      "Fetching 1d candles for ripple going back 365 days...\n",
      "Request #1: Got 1000 candles for ripple\n",
      "Reached target date (2024-09-29) for ripple\n",
      "Total candles collected for ripple: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Fetching 1d candles for binancecoin going back 365 days...\n",
      "Request #1: Got 1000 candles for binancecoin\n",
      "Reached target date (2024-09-29) for binancecoin\n",
      "Total candles collected for binancecoin: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Fetching 1d candles for solana going back 365 days...\n",
      "Request #1: Got 1000 candles for solana\n",
      "Reached target date (2024-09-29) for solana\n",
      "Total candles collected for solana: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Fetching 1d candles for usd-coin going back 365 days...\n",
      "Request #1: Got 1000 candles for usd-coin\n",
      "Reached target date (2024-09-29) for usd-coin\n",
      "Total candles collected for usd-coin: 1000\n",
      "Data ranges from 2022-07-23 to 2025-09-29\n",
      "Fetching 1d candles for staked-ether going back 365 days...\n",
      "No more data available for staked-ether after 1 requests\n",
      "No data collected for staked-ether\n",
      "Fetching 1d candles for dogecoin going back 365 days...\n",
      "Request #1: Got 1000 candles for dogecoin\n",
      "Reached target date (2024-09-29) for dogecoin\n",
      "Total candles collected for dogecoin: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Fetching 1d candles for tron going back 365 days...\n",
      "Request #1: Got 1000 candles for tron\n",
      "Reached target date (2024-09-29) for tron\n",
      "Total candles collected for tron: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Fetching 1d candles for cardano going back 365 days...\n",
      "Request #1: Got 1000 candles for cardano\n",
      "Reached target date (2024-09-29) for cardano\n",
      "Total candles collected for cardano: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Fetching 1d candles for wrapped-steth going back 365 days...\n",
      "No more data available for wrapped-steth after 1 requests\n",
      "No data collected for wrapped-steth\n",
      "Fetching 1d candles for wrapped-beacon-eth going back 365 days...\n",
      "Request #1: Got 804 candles for wrapped-beacon-eth\n",
      "Reached target date (2024-09-29) for wrapped-beacon-eth\n",
      "Total candles collected for wrapped-beacon-eth: 804\n",
      "Data ranges from 2023-07-19 to 2025-09-29\n",
      "Fetching 1d candles for chainlink going back 365 days...\n",
      "Request #1: Got 1000 candles for chainlink\n",
      "Reached target date (2024-09-29) for chainlink\n",
      "Total candles collected for chainlink: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Fetching 1d candles for wrapped-bitcoin going back 365 days...\n",
      "Request #1: Got 886 candles for wrapped-bitcoin\n",
      "Reached target date (2024-09-29) for wrapped-bitcoin\n",
      "Total candles collected for wrapped-bitcoin: 886\n",
      "Data ranges from 2023-04-28 to 2025-09-29\n",
      "Fetching 1d candles for ethena-usde going back 365 days...\n",
      "Request #1: Got 21 candles for ethena-usde\n",
      "No more data available for ethena-usde after 2 requests\n",
      "Total candles collected for ethena-usde: 21\n",
      "Data ranges from 2025-09-09 to 2025-09-29\n",
      "Fetching 1d candles for figure-heloc going back 365 days...\n",
      "No more data available for figure-heloc after 1 requests\n",
      "No data collected for figure-heloc\n",
      "Fetching 1d candles for avalanche-2 going back 365 days...\n",
      "Request #1: Got 1000 candles for avalanche-2\n",
      "Reached target date (2024-09-29) for avalanche-2\n",
      "Total candles collected for avalanche-2: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Fetching 1d candles for hyperliquid going back 365 days...\n",
      "No more data available for hyperliquid after 1 requests\n",
      "No data collected for hyperliquid\n",
      "Fetching 1d candles for stellar going back 365 days...\n",
      "Request #1: Got 1000 candles for stellar\n",
      "Reached target date (2024-09-29) for stellar\n",
      "Total candles collected for stellar: 1000\n",
      "Data ranges from 2023-01-04 to 2025-09-29\n",
      "Combined data has 1066 rows\n",
      "Error processing data for solana: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for usd-coin: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for staked-ether: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for dogecoin: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for tron: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for cardano: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for wrapped-steth: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for wrapped-beacon-eth: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for chainlink: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for wrapped-bitcoin: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for ethena-usde: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for figure-heloc: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for avalanche-2: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for hyperliquid: non convertible value e with the unit 'ms', at position 0\n",
      "Error processing data for stellar: non convertible value e with the unit 'ms', at position 0\n",
      "error fetching: move_bond_vol\n"
     ]
    }
   ],
   "source": [
    "[ids, tickers] = CoinGecko_GetUniverseV2(TOP_N, output_format=\"both\").values()\n",
    "price_action1 = Binance_GetPriceAction(ids=ids,tickers=tickers, interval=\"1d\", max_days=LOOKBACK_DAYS, timezone=TIMEZONE)\n",
    "price_action2 = CoinGecko_GetPriceAction(ids, start=START_DATE, tz=TIMEZONE, freq='D')\n",
    "dvol = Deribit_GetDVOL(['BTC','ETH'], days=LOOKBACK_DAYS, timezone=TIMEZONE)\n",
    "# onchainanalytics = dune_metrics_daily(DUNE_QUERIES, DUNE_API_KEY) \n",
    "onchainanalytics = Dune_FromCSV(path= DUNE_CSV_PATH, timezone=TIMEZONE)  \n",
    "macrodata= Fred_GetSeries(series_ids= FRED_KNOWN, fred_api_key=FRED_API_KEY, start=START_DATE, timezone=TIMEZONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd2bb5",
   "metadata": {},
   "source": [
    "Feature Engineering, Target Variable Creation, Technical Analysis Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b6219ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vix_equity_vol</th>\n",
       "      <th>ovx_oil_vol</th>\n",
       "      <th>gvz_gold_vol</th>\n",
       "      <th>usd_trade_weighted_index</th>\n",
       "      <th>us_2y_treasury_yield</th>\n",
       "      <th>us_10y_treasury_yield</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-09-30 00:00:00+02:00</th>\n",
       "      <td>16.73</td>\n",
       "      <td>39.86</td>\n",
       "      <td>18.08</td>\n",
       "      <td>121.5298</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01 00:00:00+02:00</th>\n",
       "      <td>19.26</td>\n",
       "      <td>42.87</td>\n",
       "      <td>18.57</td>\n",
       "      <td>121.9152</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-02 00:00:00+02:00</th>\n",
       "      <td>18.90</td>\n",
       "      <td>46.52</td>\n",
       "      <td>18.07</td>\n",
       "      <td>121.8154</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-03 00:00:00+02:00</th>\n",
       "      <td>20.49</td>\n",
       "      <td>54.50</td>\n",
       "      <td>18.28</td>\n",
       "      <td>122.3469</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-04 00:00:00+02:00</th>\n",
       "      <td>19.21</td>\n",
       "      <td>46.37</td>\n",
       "      <td>17.48</td>\n",
       "      <td>122.4533</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-22 00:00:00+02:00</th>\n",
       "      <td>16.10</td>\n",
       "      <td>30.88</td>\n",
       "      <td>18.43</td>\n",
       "      <td>120.2461</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-23 00:00:00+02:00</th>\n",
       "      <td>16.64</td>\n",
       "      <td>32.58</td>\n",
       "      <td>19.06</td>\n",
       "      <td>120.0568</td>\n",
       "      <td>3.53</td>\n",
       "      <td>4.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-24 00:00:00+02:00</th>\n",
       "      <td>16.18</td>\n",
       "      <td>34.08</td>\n",
       "      <td>18.30</td>\n",
       "      <td>120.6024</td>\n",
       "      <td>3.57</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-25 00:00:00+02:00</th>\n",
       "      <td>16.74</td>\n",
       "      <td>36.85</td>\n",
       "      <td>19.07</td>\n",
       "      <td>120.9635</td>\n",
       "      <td>3.64</td>\n",
       "      <td>4.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-26 00:00:00+02:00</th>\n",
       "      <td>15.29</td>\n",
       "      <td>35.85</td>\n",
       "      <td>18.59</td>\n",
       "      <td>120.8615</td>\n",
       "      <td>3.63</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           vix_equity_vol  ovx_oil_vol  gvz_gold_vol  \\\n",
       "date                                                                   \n",
       "2024-09-30 00:00:00+02:00           16.73        39.86         18.08   \n",
       "2024-10-01 00:00:00+02:00           19.26        42.87         18.57   \n",
       "2024-10-02 00:00:00+02:00           18.90        46.52         18.07   \n",
       "2024-10-03 00:00:00+02:00           20.49        54.50         18.28   \n",
       "2024-10-04 00:00:00+02:00           19.21        46.37         17.48   \n",
       "...                                   ...          ...           ...   \n",
       "2025-09-22 00:00:00+02:00           16.10        30.88         18.43   \n",
       "2025-09-23 00:00:00+02:00           16.64        32.58         19.06   \n",
       "2025-09-24 00:00:00+02:00           16.18        34.08         18.30   \n",
       "2025-09-25 00:00:00+02:00           16.74        36.85         19.07   \n",
       "2025-09-26 00:00:00+02:00           15.29        35.85         18.59   \n",
       "\n",
       "                           usd_trade_weighted_index  us_2y_treasury_yield  \\\n",
       "date                                                                        \n",
       "2024-09-30 00:00:00+02:00                  121.5298                  3.66   \n",
       "2024-10-01 00:00:00+02:00                  121.9152                  3.61   \n",
       "2024-10-02 00:00:00+02:00                  121.8154                  3.63   \n",
       "2024-10-03 00:00:00+02:00                  122.3469                  3.70   \n",
       "2024-10-04 00:00:00+02:00                  122.4533                  3.93   \n",
       "...                                             ...                   ...   \n",
       "2025-09-22 00:00:00+02:00                  120.2461                  3.61   \n",
       "2025-09-23 00:00:00+02:00                  120.0568                  3.53   \n",
       "2025-09-24 00:00:00+02:00                  120.6024                  3.57   \n",
       "2025-09-25 00:00:00+02:00                  120.9635                  3.64   \n",
       "2025-09-26 00:00:00+02:00                  120.8615                  3.63   \n",
       "\n",
       "                           us_10y_treasury_yield  \n",
       "date                                              \n",
       "2024-09-30 00:00:00+02:00                   3.81  \n",
       "2024-10-01 00:00:00+02:00                   3.74  \n",
       "2024-10-02 00:00:00+02:00                   3.79  \n",
       "2024-10-03 00:00:00+02:00                   3.85  \n",
       "2024-10-04 00:00:00+02:00                   3.98  \n",
       "...                                          ...  \n",
       "2025-09-22 00:00:00+02:00                   4.15  \n",
       "2025-09-23 00:00:00+02:00                   4.12  \n",
       "2025-09-24 00:00:00+02:00                   4.16  \n",
       "2025-09-25 00:00:00+02:00                   4.18  \n",
       "2025-09-26 00:00:00+02:00                   4.20  \n",
       "\n",
       "[362 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macrodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d66aa9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_21416\\1747441123.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"willr_{coin}\"] = talib.WILLR(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values)\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_21416\\1747441123.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"mom_{coin}\"] = talib.MOM(p.values)\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_21416\\1747441123.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"roc_{coin}\"] = talib.ROC(p.values)\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_21416\\1747441123.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"obv_{coin}\"] = talib.OBV(p.values, df[f\"volume_{coin}\"])\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_21416\\1747441123.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[f\"mfi_{coin}\"] = talib.MFI(df[f\"high_{coin}\"], df[f\"low_{coin}\"], p.values, df[f\"volume_{coin}\"])\n"
     ]
    }
   ],
   "source": [
    "Unified = None\n",
    "for df in [price_action1, dvol, onchainanalytics, macrodata, price_action2]:\n",
    "    try: df.index = pd.DatetimeIndex(df.index).tz_localize(TIMEZONE).date\n",
    "    except: df.index = pd.DatetimeIndex(df.index).tz_convert(TIMEZONE).date\n",
    "    if Unified is None: Unified = df\n",
    "    else: Unified = Unified.join(df, how='outer')\n",
    "X= Unified.iloc[-365 : ].dropna(axis=1, thresh=int(0.1*len(Unified))).ffill(limit=3)\n",
    "X[f'log_returns_{TARGET_COIN}']= np.log(X[f'prices_{TARGET_COIN}']) - np.log(X[f'prices_{TARGET_COIN}'].shift(1))\n",
    "X[f'realized_vol_{TARGET_COIN}'] = abs(X[f'log_returns_{TARGET_COIN}'])\n",
    "X= X.diff().dropna()\n",
    "y = X[f'realized_vol_{TARGET_COIN}'].shift(-1).dropna().rename(\"target\")\n",
    "taindicators= Compute_TAIndicators(X, price_prefix=\"prices_\")\n",
    "X = X.join(taindicators, how='left').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6e83e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 74/74 [00:29<00:00,  2.52it/s]\n",
      "Feature Extraction:  40%|████      | 36/90 [19:56<29:55, 33.24s/it]   \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRemoteTraceback\u001b[39m                           Traceback (most recent call last)",
      "\u001b[31mRemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ~~~~^^^^^^^^^^^^^^^\n  File \"c:\\VolatilityForecast\\VolatilityForecastNew\\VF.venv\\Lib\\site-packages\\tsfresh\\utilities\\distribution.py\", line 43, in _function_with_partly_reduce\n    results = list(itertools.chain.from_iterable(results))\nMemoryError\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m y.rename_axis(index=\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)   \n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtsxg_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtsxg\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results= \u001b[43mtsxg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtsxg_multiprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxtimeshift\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mevaluation_metrics\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VolatilityForecast\\VolatilityForecastNew\\tsxg_pipeline.py:57\u001b[39m, in \u001b[36mtsxg_multiprocessing\u001b[39m\u001b[34m(X, y, id, sort, maxtimeshift, njobs, fcparameters, fdrlvl, split_ratio, plot, xgb_params)\u001b[39m\n\u001b[32m     48\u001b[39m rolled = roll_time_series(\n\u001b[32m     49\u001b[39m     stacked,\n\u001b[32m     50\u001b[39m     column_id=\u001b[38;5;28mid\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     n_jobs=njobs\n\u001b[32m     54\u001b[39m ).dropna()\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# extract features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m features_raw = \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrolled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfcparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnjobs\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# merge by kind key\u001b[39;00m\n\u001b[32m     68\u001b[39m count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VolatilityForecast\\VolatilityForecastNew\\VF.venv\\Lib\\site-packages\\tsfresh\\feature_extraction\\extraction.py:164\u001b[39m, in \u001b[36mextract_features\u001b[39m\u001b[34m(timeseries_container, default_fc_parameters, kind_to_fc_parameters, column_id, column_sort, column_kind, column_value, chunksize, n_jobs, show_warnings, disable_progressbar, impute_function, profile, profiling_filename, profiling_sorting, distributor, pivot)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    162\u001b[39m     warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m result = \u001b[43m_do_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeseries_container\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_progressbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_warnings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_warnings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkind_to_fc_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkind_to_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistributor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistributor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpivot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpivot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# Impute the result if requested\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m impute_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VolatilityForecast\\VolatilityForecastNew\\VF.venv\\Lib\\site-packages\\tsfresh\\feature_extraction\\extraction.py:294\u001b[39m, in \u001b[36m_do_extraction\u001b[39m\u001b[34m(df, column_id, column_value, column_kind, column_sort, default_fc_parameters, kind_to_fc_parameters, n_jobs, chunk_size, disable_progressbar, show_warnings, distributor, pivot)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mthe passed distributor is not an DistributorBaseClass object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    288\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    289\u001b[39m     default_fc_parameters=default_fc_parameters,\n\u001b[32m    290\u001b[39m     kind_to_fc_parameters=kind_to_fc_parameters,\n\u001b[32m    291\u001b[39m     show_warnings=show_warnings,\n\u001b[32m    292\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m result = \u001b[43mdistributor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_do_extraction_on_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pivot:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VolatilityForecast\\VolatilityForecastNew\\VF.venv\\Lib\\site-packages\\tsfresh\\utilities\\distribution.py:241\u001b[39m, in \u001b[36mIterableDistributorBaseClass.map_reduce\u001b[39m\u001b[34m(self, map_function, data, function_kwargs, chunk_size, data_length)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m     result = (\n\u001b[32m    236\u001b[39m         \u001b[38;5;28mself\u001b[39m.distribute(\n\u001b[32m    237\u001b[39m             _function_with_partly_reduce, chunk_generator, map_kwargs\n\u001b[32m    238\u001b[39m         ),\n\u001b[32m    239\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m result = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VolatilityForecast\\VolatilityForecastNew\\VF.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\pool.py:873\u001b[39m, in \u001b[36mIMapIterator.next\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m    872\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    123\u001b[39m job, i, func, args, kwds = task\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, func(*args, **kwds))\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VolatilityForecast\\VolatilityForecastNew\\VF.venv\\Lib\\site-packages\\tsfresh\\utilities\\distribution.py:43\u001b[39m, in \u001b[36m_function_with_partly_reduce\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     41\u001b[39m kwargs = kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m     42\u001b[39m results = (map_function(chunk, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_list)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m results = \u001b[38;5;28mlist\u001b[39m(itertools.chain.from_iterable(results))\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "X= X.loc[X.join(y, how='inner').dropna().index]\n",
    "y= y.loc[X.index]   \n",
    "X.rename_axis(index='date', inplace=True)\n",
    "y.rename_axis(index='date', inplace=True)   \n",
    "import tsxg_pipeline as tsxg\n",
    "results= tsxg.tsxg_multiprocessing(X, y, maxtimeshift=5)\n",
    "results['evaluation_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e6f2de6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 74/74 [00:36<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "Featurecontainer= X.reset_index().melt(id_vars=['index']).rename(columns={'index':'date'})\n",
    "rolled= roll_time_series(Featurecontainer, column_id='variable', column_sort='date', max_timeshift= 7, min_timeshift=0, rolling_direction=1, n_jobs=18)\n",
    "y= rolled[rolled['variable']=='target'].sort_values('id').groupby('date').last()['value']\n",
    "features = extract_relevant_features(rolled, y, column_id='date', column_sort='id', column_kind='variable', column_value='value', n_jobs=18, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VF.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
