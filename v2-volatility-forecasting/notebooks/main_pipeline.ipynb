{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8290be",
   "metadata": {},
   "source": [
    "## 🚀 Setup Instructions\n",
    "\n",
    "**Important: Run this notebook from the `notebooks/` directory after:**\n",
    "\n",
    "1. **Installing the package:** `pip install -e .` (from project root)\n",
    "2. **Activating virtual environment:** `venv\\Scripts\\activate`\n",
    "3. **Configuring API keys:** Copy `.env.example` to `.env` and add your keys\n",
    "4. **Starting Jupyter:** `cd notebooks && jupyter lab`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54f93a",
   "metadata": {},
   "source": [
    "# Cryptocurrency Volatility Forecasting - Main Pipeline\n",
    "\n",
    "This notebook implements the complete cryptocurrency volatility forecasting pipeline, consolidating the proven methodology from the original research notebook into a production-ready workflow.\n",
    "\n",
    "## Pipeline Architecture:\n",
    "1. **Data Collection**: Multi-source cryptocurrency and macroeconomic data aggregation\n",
    "2. **Feature Engineering**: TSFresh automated feature extraction with technical analysis indicators\n",
    "3. **Distributed Processing**: Dask-optimized computation for large-scale time series operations\n",
    "4. **Model Training**: XGBoost implementation with Optuna hyperparameter optimization\n",
    "5. **Performance Evaluation**: Comprehensive model assessment using multiple metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c677b",
   "metadata": {},
   "source": [
    "## Environment Setup and Module Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f05477",
   "metadata": {},
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "285c4686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Target Coin: ethereum\n",
      "  Data Frequency: 1D\n",
      "  Lookback Period: 365 days\n",
      "  Universe Size: Top 10 cryptocurrencies\n",
      "  ML Trials: 50\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters - modify these for your analysis\n",
    "TARGET_COIN = \"ethereum\"      # Main coin to forecast (bitcoin, ethereum, etc.)\n",
    "TOP_N = 10                   # Number of top cryptocurrencies to include\n",
    "LOOKBACK_DAYS = 365         # Historical data period in days\n",
    "FREQUENCY = \"1D\"            # Data frequency: \"1D\" for daily, \"1H\" for hourly\n",
    "TIMEZONE = \"Europe/Madrid\"  # Timezone for data collection\n",
    "\n",
    "# Machine learning parameters\n",
    "N_TRIALS = 50               # Number of Optuna optimization trials\n",
    "N_ROUNDS = 200              # XGBoost training rounds\n",
    "EVAL_METRIC = 'mae'         # Evaluation metric for optimization\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Target Coin: {TARGET_COIN}\")\n",
    "print(f\"  Data Frequency: {FREQUENCY}\")\n",
    "print(f\"  Lookback Period: {LOOKBACK_DAYS} days\")\n",
    "print(f\"  Universe Size: Top {TOP_N} cryptocurrencies\")\n",
    "print(f\"  ML Trials: {N_TRIALS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2489cc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 15:11:18,982 INFO numba.cuda.cudadrv.driver init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna not available.\n",
      "Module imports completed successfully\n",
      "Working directory: c:\\CryptoMarketForecasting-new\\v2-volatility-forecasting\n",
      "API keys loaded from environment variables\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import random, os, pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Change to repository root and add src to path\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "repo_root = os.path.dirname(notebook_dir)\n",
    "os.chdir(repo_root)\n",
    "sys.path.append('src')\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configure environment\n",
    "os.makedirs(\"OutputData\", exist_ok=True)\n",
    "plt.rcParams['figure.figsize'] = (20, 8)\n",
    "\n",
    "# Import toolkit modules\n",
    "from data.collectors import CryptoDataCollector\n",
    "from features.engineering import CryptoFeatureEngineer\n",
    "from models.pipeline import CryptoVolatilityMLPipeline\n",
    "from utils.dask_helpers import create_optimized_dask_client, cleanup_dask_client\n",
    "\n",
    "print(\"Module imports completed successfully\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"API keys loaded from environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2384ea",
   "metadata": {},
   "source": [
    "## Distributed Computing Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ffdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dask distributed computing cluster\n",
    "client = create_optimized_dask_client(\n",
    "    n_workers=4,\n",
    "    threads_per_worker=2,\n",
    "    memory_limit='4GB',\n",
    "    dashboard_port=8787,\n",
    "    processes=True\n",
    ")\n",
    "\n",
    "print(f\"Dask cluster ready at: http://localhost:8787/status\")\n",
    "# Display cluster configuration\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0356110d",
   "metadata": {},
   "source": [
    "## 3. Data Collection from Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90974047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data collector with our configuration\n",
    "collector = CryptoDataCollector(\n",
    "    timezone=TIMEZONE,\n",
    "    top_n=TOP_N,\n",
    "    lookback_days=LOOKBACK_DAYS,\n",
    "    frequency=FREQUENCY\n",
    ")\n",
    "\n",
    "print(f\"Data Collector Configuration:\")\n",
    "print(f\"  Frequency: {collector.FREQUENCY}\")\n",
    "print(f\"  Lookback Days: {collector.LOOKBACK_DAYS}\")\n",
    "\n",
    "print(f\"\\nAPI Frequency Resolutions:\")\n",
    "print(f\"  Pandas: {collector.get_pandas_freq()}\")\n",
    "print(f\"  Binance: {collector.get_binance_interval()}\")\n",
    "print(f\"  Deribit: {collector.get_deribit_resolution()}\")\n",
    "print(f\"  FRED: {collector.get_fred_frequency()}\")\n",
    "print(f\"  Dune: {collector.get_dune_resolution()}\")\n",
    "\n",
    "# Check batch sizes - should reflect the actual timeframe\n",
    "batch_size = collector.get_batch_size_for_frequency()\n",
    "print(f\"\\nBatch Sizes (based on {LOOKBACK_DAYS} days at {FREQUENCY} frequency):\")\n",
    "print(f\"  Dune Batch Size: {batch_size}\")\n",
    "print(f\"  Calculation: {LOOKBACK_DAYS} days × {24 if FREQUENCY in ['1H', '1h', 'hourly'] else 1} = {batch_size}\")\n",
    "\n",
    "# Collect all data sources\n",
    "print(f\"\\nStarting comprehensive data collection at {FREQUENCY} frequency...\")\n",
    "data_sources = collector.collect_all_data()\n",
    "\n",
    "# Display collected data info\n",
    "print(\"\\nData Collection Summary:\")\n",
    "for source, df in data_sources.items():\n",
    "    if not df.empty:\n",
    "        print(f\"  {source}: {df.shape} | {df.index.min().date()} to {df.index.max().date()}\")\n",
    "        print(f\"    Columns: {list(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "    else:\n",
    "        print(f\"  {source}: Empty DataFrame\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67258324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data sources\n",
    "unified_data = collector.combine_data_sources(data_sources)\n",
    "\n",
    "print(f\"Unified dataset shape: {unified_data.shape}\")\n",
    "print(f\"Date range: {unified_data.index.min()} to {unified_data.index.max()}\")\n",
    "print(f\"Columns: {list(unified_data.columns[:10])}...\")  # Show first 10 columns\n",
    "\n",
    "# Display recent data\n",
    "unified_data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6103b",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Target Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19171118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = CryptoFeatureEngineer(\n",
    "    time_window=14,  # Rolling window for TSFresh features\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Prepare features and target variable\n",
    "print(f\"Preparing features and target for {TARGET_COIN}...\")\n",
    "X, y = engineer.prepare_target_variable(\n",
    "    unified_data, \n",
    "    target_coin=TARGET_COIN\n",
    ")\n",
    "\n",
    "print(f\"Base features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target ({TARGET_COIN} realized volatility) statistics:\")\n",
    "print(f\"   Mean: {y.mean():.6f}\")\n",
    "print(f\"   Std: {y.std():.6f}\")\n",
    "print(f\"   Min: {y.min():.6f}\")\n",
    "print(f\"   Max: {y.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add technical analysis indicators\n",
    "print(\"📈 Computing technical analysis indicators...\")\n",
    "ta_indicators = engineer.compute_ta_indicators(X, price_prefix=\"prices_\")\n",
    "\n",
    "if not ta_indicators.empty:\n",
    "    # Combine with base features\n",
    "    X_with_ta = X.join(ta_indicators, how='left').dropna()\n",
    "    \n",
    "    # Align indices\n",
    "    common_idx = X_with_ta.index.intersection(y.index)\n",
    "    X = X_with_ta.loc[common_idx]\n",
    "    y = y.loc[common_idx]\n",
    "    \n",
    "    print(f\"Added {ta_indicators.shape[1]} technical indicators\")\n",
    "    print(f\"Features with TA shape: {X.shape}\")\n",
    "else:\n",
    "    print(\"No technical indicators computed (TA-Lib may not be available)\")\n",
    "\n",
    "# Display some technical indicators\n",
    "ta_cols = [col for col in X.columns if any(indicator in col for indicator in ['rsi', 'macd', 'sma', 'ema'])]\n",
    "if ta_cols:\n",
    "    print(f\"Technical indicators sample: {ta_cols[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f13d49",
   "metadata": {},
   "source": [
    "## 5. TSFresh Feature Engineering with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TSFresh pipeline with Dask\n",
    "print(\"🧠 Starting TSFresh feature extraction with Dask...\")\n",
    "tsfresh_features = engineer.run_tsfresh_pipeline(X, y, client)\n",
    "\n",
    "if not tsfresh_features.empty:\n",
    "    print(f\"TSFresh features extracted: {tsfresh_features.shape}\")\n",
    "    print(f\"Sample TSFresh features: {list(tsfresh_features.columns[:5])}\")\n",
    "else:\n",
    "    print(\"No TSFresh features extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final feature set\n",
    "print(\"🎯 Creating final feature set...\")\n",
    "final_features = engineer.create_final_feature_set(\n",
    "    X_base=X,\n",
    "    y=y,\n",
    "    tsfresh_features=tsfresh_features,\n",
    "    include_ta_indicators=True\n",
    ")\n",
    "\n",
    "print(f\"Final feature set shape: {final_features.shape}\")\n",
    "print(f\"Features ready for ML pipeline\")\n",
    "\n",
    "# Show feature breakdown\n",
    "feature_cols = final_features.drop('target', axis=1).columns\n",
    "tsfresh_count = len([col for col in feature_cols if '__' in col])  # TSFresh features contain '__'\n",
    "base_count = len(feature_cols) - tsfresh_count\n",
    "\n",
    "print(f\"Feature breakdown:\")\n",
    "print(f\"   Base + TA features: {base_count}\")\n",
    "print(f\"   TSFresh features: {tsfresh_count}\")\n",
    "print(f\"   Total features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40fd29c",
   "metadata": {},
   "source": [
    "## 6. Machine Learning Pipeline with Optuna + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1cd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ML pipeline\n",
    "ml_pipeline = CryptoVolatilityMLPipeline(\n",
    "    n_trials=N_TRIALS,\n",
    "    n_rounds=N_ROUNDS,\n",
    "    eval_metric=EVAL_METRIC,\n",
    "    tree_method='hist',\n",
    "    early_stopping_rounds=25,\n",
    "    splits=5,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"ML Pipeline initialized:\")\n",
    "print(f\"   Trials: {N_TRIALS}\")\n",
    "print(f\"   Metric: {EVAL_METRIC}\")\n",
    "print(f\"   Training Rounds: {N_ROUNDS}\")\n",
    "print(f\"   Cross-validation Splits: 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete ML pipeline\n",
    "print(\"Starting complete ML pipeline...\")\n",
    "print(\"This will take several minutes for hyperparameter optimization...\")\n",
    "\n",
    "ml_results = ml_pipeline.run_complete_pipeline(\n",
    "    final_features=final_features,\n",
    "    client=client,\n",
    "    target_coin=TARGET_COIN,\n",
    "    optimize=True\n",
    ")\n",
    "\n",
    "print(\"ML Pipeline completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cb9b7",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "study = ml_results['study']\n",
    "final_model = ml_results['final_model']\n",
    "metrics = ml_results['metrics']\n",
    "y_test_pd = ml_results['y_test_pd']\n",
    "predictions_pd = ml_results['predictions_pd']\n",
    "\n",
    "# Display optimization results\n",
    "print(\"Hyperparameter Optimization Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best {config.ml.eval_metric}: {study.best_value:.6f}\")\n",
    "print(f\"Number of trials: {len(study.trials)}\")\n",
    "\n",
    "# Display model performance\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric.upper()}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis\n",
    "print(\"Additional Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Feature importance (if available)\n",
    "try:\n",
    "    importance = final_model['booster'].get_score(importance_type='weight')\n",
    "    top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"🏆 Top 10 Most Important Features:\")\n",
    "    for i, (feature, score) in enumerate(top_features, 1):\n",
    "        print(f\"  {i:2d}. {feature}: {score}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not extract feature importance: {e}\")\n",
    "\n",
    "# Prediction statistics\n",
    "print(f\"\\n📊 Prediction Statistics:\")\n",
    "print(f\"Prediction mean: {predictions_pd.mean():.6f}\")\n",
    "print(f\"Prediction std: {predictions_pd.std():.6f}\")\n",
    "print(f\"Actual mean: {y_test_pd.mean():.6f}\")\n",
    "print(f\"Actual std: {y_test_pd.std():.6f}\")\n",
    "\n",
    "# Correlation\n",
    "correlation = predictions_pd.corr(y_test_pd)\n",
    "print(f\"Correlation: {correlation:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf2f8c",
   "metadata": {},
   "source": [
    "## 8. Model Interpretation and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model insights\n",
    "print(\"Model Insights:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Best parameters interpretation\n",
    "best_params = study.best_params\n",
    "print(f\"Optimal learning rate: {best_params.get('learning_rate', 'N/A'):.4f}\")\n",
    "print(f\"Optimal max depth: {best_params.get('max_depth', 'N/A')}\")\n",
    "print(f\"Optimal subsample: {best_params.get('subsample', 'N/A'):.3f}\")\n",
    "print(f\"Optimal colsample_bytree: {best_params.get('colsample_bytree', 'N/A'):.3f}\")\n",
    "\n",
    "# Model complexity\n",
    "n_estimators = best_params.get('num_boost_rounds', config.ml.n_rounds)\n",
    "max_depth = best_params.get('max_depth', 6)\n",
    "complexity_score = n_estimators * max_depth / 1000\n",
    "print(f\"\\nModel complexity score: {complexity_score:.3f}\")\n",
    "\n",
    "# Performance vs baseline\n",
    "naive_mae = metrics.get('mase', float('inf'))\n",
    "if naive_mae < 1.0:\n",
    "    print(f\"Model beats naive forecast (MASE: {naive_mae:.3f})\")\n",
    "else:\n",
    "    print(f\"Model underperforms naive forecast (MASE: {naive_mae:.3f})\")\n",
    "\n",
    "# R² interpretation\n",
    "r2 = metrics['r2_score']\n",
    "if r2 > 0.5:\n",
    "    print(f\"Good explanatory power (R²: {r2:.3f})\")\n",
    "elif r2 > 0.2:\n",
    "    print(f\"Moderate explanatory power (R²: {r2:.3f})\")\n",
    "else:\n",
    "    print(f\"Low explanatory power (R²: {r2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a126cc56",
   "metadata": {},
   "source": [
    "## 9. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Target: {TARGET_COIN}\")\n",
    "print(f\"Frequency: {FREQUENCY}\")\n",
    "print(f\"Data points: {len(final_features)}\")\n",
    "print(f\"Features: {final_features.shape[1] - 1}\")\n",
    "print(f\"Optimization trials: {len(study.trials)}\")\n",
    "print(f\"Best {EVAL_METRIC.upper()}: {study.best_value:.6f}\")\n",
    "print(f\"Test R²: {metrics['r2_score']:.6f}\")\n",
    "if 'mase' in metrics:\n",
    "    print(f\"Test MASE: {metrics['mase']:.6f}\")\n",
    "\n",
    "# Feature breakdown\n",
    "feature_cols = final_features.drop('target', axis=1).columns\n",
    "tsfresh_features = [col for col in feature_cols if '__' in col]\n",
    "base_features = [col for col in feature_cols if '__' not in col]\n",
    "\n",
    "print(f\"\\nFeature Engineering:\")\n",
    "print(f\"Base + TA features: {len(base_features)}\")\n",
    "print(f\"TSFresh features: {len(tsfresh_features)}\")\n",
    "\n",
    "print(f\"\\nComputational Resources:\")\n",
    "print(f\"Dask workers: 4\")\n",
    "print(f\"Memory per worker: 4GB\")\n",
    "\n",
    "print(\"\\nPipeline execution completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39235cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Dask client\n",
    "cleanup_dask_client(client)\n",
    "print(\"Dask client cleaned up\")\n",
    "print(\"\\nAll done! Your cryptocurrency volatility forecasting model is ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
