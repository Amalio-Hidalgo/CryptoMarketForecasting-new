{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f31bbd",
   "metadata": {},
   "source": [
    "# Cryptocurrency Volatility Forecasting Toolkit\n",
    "## Advanced Machine Learning Pipeline with TSFresh Feature Engineering\n",
    "\n",
    "This notebook demonstrates a sophisticated cryptocurrency volatility forecasting pipeline that combines:\n",
    "\n",
    "- **Multi-Source Data Collection**: CoinGecko, Binance, Dune Analytics, FRED, Deribit\n",
    "- **Advanced Feature Engineering**: TSFresh time series feature extraction with Dask distributed computing  \n",
    "- **Professional ML Pipeline**: XGBoost with Optuna hyperparameter optimization\n",
    "- **Comprehensive Analysis**: Technical indicators, on-chain metrics, and macroeconomic data\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Data Collection & Integration**: Unified cryptocurrency, on-chain, and macro data\n",
    "2. **Feature Engineering**: TSFresh rolling time series feature extraction (600+ features)\n",
    "3. **Feature Selection**: Statistical significance testing with FDR control\n",
    "4. **Distributed Computing**: Dask cluster for scalable computation\n",
    "5. **Model Training**: XGBoost with Optuna hyperparameter optimization\n",
    "6. **Evaluation & Visualization**: Comprehensive performance metrics and plots\n",
    "\n",
    "### Target: Realized Volatility Forecasting\n",
    "Predicting next-period realized volatility for cryptocurrency returns using advanced time series features and multi-modal data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce9ce06",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration\n",
    "\n",
    "Setting up the environment with all required libraries, configurations, and Dask distributed computing cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4a684",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2487f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment setup completed\n",
      "üìÅ Working directory: c:\\CryptoMarketForecasting-new\\v2-volatility-forecasting\\notebooks\n",
      "üîó Repository root: c:\\CryptoMarketForecasting-new\\v2-volatility-forecasting\n",
      "‚úÖ All imports successful - ready for pipeline execution!\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries & Environment Setup\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Suppress common warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tsfresh')\n",
    "logging.getLogger('tsfresh').setLevel(logging.ERROR)\n",
    "\n",
    "# Distributed Computing\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster, progress\n",
    "from dask import delayed\n",
    "\n",
    "# TSFresh Feature Engineering\n",
    "from tsfresh import extract_features, select_features, extract_relevant_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, impute\n",
    "from tsfresh.convenience.bindings import dask_feature_extraction_on_chunk\n",
    "\n",
    "# Machine Learning & Optimization\n",
    "import xgboost as xgb\n",
    "from xgboost import dask as dxgb\n",
    "import optuna\n",
    "from optuna.integration.dask import DaskStorage\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Technical Analysis\n",
    "try:\n",
    "    import talib\n",
    "    TALIB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TALIB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  TA-Lib not available - technical indicators will be skipped\")\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "\n",
    "# Add src directory to path for imports\n",
    "notebook_dir = os.getcwd()\n",
    "repo_root = os.path.dirname(notebook_dir)\n",
    "src_path = os.path.join(repo_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "print(\"üîß Environment setup completed\")\n",
    "print(f\"üìÅ Working directory: {notebook_dir}\")\n",
    "print(f\"üîó Repository root: {repo_root}\")\n",
    "\n",
    "# Import project modules\n",
    "from data.collectors import CryptoDataCollector\n",
    "from config import Config\n",
    "\n",
    "print(\"‚úÖ All imports successful - ready for pipeline execution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417e531",
   "metadata": {},
   "source": [
    "## 2. Configuration & Constants\n",
    "\n",
    "Setting up key pipeline parameters and configurations for data collection, feature engineering, and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7b2f0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä PIPELINE CONFIGURATION\n",
      "==================================================\n",
      "üéØ Target Cryptocurrency: ETHEREUM\n",
      "üìÖ Data Range: 2024-10-02 to 2025-10-02 (365 days)\n",
      "üîÑ Frequency: 1D\n",
      "üèÜ Top Cryptocurrencies: 10\n",
      "ü™ü Rolling Window: 14 periods\n",
      "üß™ Optuna Trials: 100\n",
      "üå≥ XGBoost Rounds: 200\n",
      "üìà Evaluation Metric: MAE\n",
      "\n",
      "‚úÖ Configuration loaded successfully\n",
      "üîë API Keys configured: 3/3 (COINGECKO_API_KEY, DUNE_API_KEY, FRED_API_KEY)\n",
      "üé≤ Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE CONFIGURATION & CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "# Data Collection Parameters\n",
    "TARGET_COIN = \"ethereum\"           # Primary target for volatility forecasting\n",
    "BASE_FIAT = \"usd\"                 # Base currency for all prices\n",
    "TOP_N = 10                        # Number of top cryptocurrencies by market cap\n",
    "LOOKBACK_DAYS = 365               # Historical data window\n",
    "FREQUENCY = \"1D\"                  # Data frequency (1D = daily)\n",
    "TIMEZONE = \"Europe/Madrid\"        # Timezone for data alignment\n",
    "SLEEP_TIME = 6                    # API rate limiting delay (seconds)\n",
    "\n",
    "# Feature Engineering Parameters  \n",
    "TIME_WINDOW = 14                  # Rolling window for TSFresh feature extraction\n",
    "EXTRACTION_SETTINGS = EfficientFCParameters()  # TSFresh feature extraction parameters\n",
    "DEFAULT_FDR_LEVEL = 0.05          # False Discovery Rate for feature selection\n",
    "\n",
    "# Model Training Parameters\n",
    "RANDOM_SEED = 42                  # For reproducibility\n",
    "SPLITS = 10                       # Time series cross-validation splits\n",
    "DEFAULT_N_TRIALS = 100            # Optuna hyperparameter optimization trials\n",
    "DEFAULT_N_ROUNDS = 200            # XGBoost training rounds\n",
    "DEFAULT_XGB_METRIC = 'mae'        # XGBoost evaluation metric\n",
    "DEFAULT_TREE_METHOD = 'hist'      # XGBoost tree construction method\n",
    "DEFAULT_EARLY_STOPPING = 25       # Early stopping patience\n",
    "\n",
    "# Date Calculations\n",
    "START_DATE = (datetime.now() - timedelta(days=LOOKBACK_DAYS)).strftime(\"%Y-%m-%d\")\n",
    "TODAY = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(\"üìä PIPELINE CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üéØ Target Cryptocurrency: {TARGET_COIN.upper()}\")\n",
    "print(f\"üìÖ Data Range: {START_DATE} to {TODAY} ({LOOKBACK_DAYS} days)\")\n",
    "print(f\"üîÑ Frequency: {FREQUENCY}\")\n",
    "print(f\"üèÜ Top Cryptocurrencies: {TOP_N}\")\n",
    "print(f\"ü™ü Rolling Window: {TIME_WINDOW} periods\")\n",
    "print(f\"üß™ Optuna Trials: {DEFAULT_N_TRIALS}\")\n",
    "print(f\"üå≥ XGBoost Rounds: {DEFAULT_N_ROUNDS}\")\n",
    "print(f\"üìà Evaluation Metric: {DEFAULT_XGB_METRIC.upper()}\")\n",
    "\n",
    "# API Keys verification\n",
    "api_keys = ['COINGECKO_API_KEY', 'DUNE_API_KEY', 'FRED_API_KEY']\n",
    "available_keys = [k for k in api_keys if os.getenv(k)]\n",
    "print(f\"\\n‚úÖ Configuration loaded successfully\")\n",
    "print(f\"üîë API Keys configured: {len(available_keys)}/3 ({', '.join(available_keys)})\")\n",
    "\n",
    "# Set numpy random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"üé≤ Random seed set to {RANDOM_SEED} for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd66befb",
   "metadata": {},
   "source": [
    "## 3. Dask Distributed Computing Setup\n",
    "\n",
    "Initializing local Dask cluster for distributed computation. This enables parallel processing of TSFresh feature extraction and XGBoost training across multiple CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12173a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Closing existing Dask client...\n",
      "üßπ Closing existing Dask cluster...\n",
      "üöÄ Initializing Dask LocalCluster...\n",
      "üöÄ Initializing Dask LocalCluster...\n",
      "‚úÖ Dask cluster initialized successfully!\n",
      "üìä Dashboard: http://localhost:8787\n",
      "üë• Workers: 4\n",
      "üßµ Total threads: 16\n",
      "üíæ Total memory: 24.0 GB\n",
      "‚úÖ Dask cluster initialized successfully!\n",
      "üìä Dashboard: http://localhost:8787\n",
      "üë• Workers: 4\n",
      "üßµ Total threads: 16\n",
      "üíæ Total memory: 24.0 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-7d41aca6-9fad-11f0-b97c-f020ff5aa2b7</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> distributed.LocalCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:8787/status\" target=\"_blank\">http://127.0.0.1:8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">b4585ce5</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:8787/status\" target=\"_blank\">http://127.0.0.1:8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 4\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 16\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 22.35 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-e1d08b57-e109-4c8e-b29b-1fe5efb719a4</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:54983\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0 \n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:8787/status\" target=\"_blank\">http://127.0.0.1:8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:55003\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 4\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:55008/status\" target=\"_blank\">http://127.0.0.1:55008/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 5.59 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:54986\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> C:\\Users\\amali\\AppData\\Local\\Temp\\dask-scratch-space\\worker-r_9j_jvc\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 1</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:55002\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 4\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:55006/status\" target=\"_blank\">http://127.0.0.1:55006/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 5.59 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:54988\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> C:\\Users\\amali\\AppData\\Local\\Temp\\dask-scratch-space\\worker-sogtm105\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 2</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:55005\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 4\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:55010/status\" target=\"_blank\">http://127.0.0.1:55010/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 5.59 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:54990\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> C:\\Users\\amali\\AppData\\Local\\Temp\\dask-scratch-space\\worker-vvohzo_j\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 3</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:55004\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 4\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:55011/status\" target=\"_blank\">http://127.0.0.1:55011/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 5.59 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:54992\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> C:\\Users\\amali\\AppData\\Local\\Temp\\dask-scratch-space\\worker-zl0o98id\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:54983' processes=4 threads=16, memory=22.35 GiB>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DASK DISTRIBUTED COMPUTING CLUSTER SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress Dask logging to reduce duplicated output\n",
    "logging.getLogger('distributed').setLevel(logging.ERROR)\n",
    "logging.getLogger('distributed.scheduler').setLevel(logging.ERROR)\n",
    "logging.getLogger('distributed.nanny').setLevel(logging.ERROR)\n",
    "logging.getLogger('distributed.core').setLevel(logging.ERROR)\n",
    "logging.getLogger('distributed.diskutils').setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='distributed')\n",
    "\n",
    "# Clean up any existing clusters\n",
    "try:\n",
    "    if 'client' in globals() and client:\n",
    "        print(\"üßπ Closing existing Dask client...\")\n",
    "        client.close()\n",
    "    if 'cluster' in globals() and cluster:\n",
    "        print(\"üßπ Closing existing Dask cluster...\")\n",
    "        cluster.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Initialize optimized local cluster for cryptocurrency analysis\n",
    "print(\"üöÄ Initializing Dask LocalCluster...\")\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,                    # Number of worker processes\n",
    "    threads_per_worker=4,           # Threads per worker (adjust based on CPU cores)\n",
    "    processes=True,                 # Use processes for better parallelization\n",
    "    memory_limit='6GB',             # Memory limit per worker\n",
    "    dashboard_address=':8787',      # Dashboard port\n",
    "    silence_logs=True,             # Keep logs quiet\n",
    "    protocol='tcp'                  # Communication protocol\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"‚úÖ Dask cluster initialized successfully!\")\n",
    "print(f\"üìä Dashboard: http://localhost:8787\")\n",
    "print(f\"üë• Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "print(f\"üßµ Total threads: {sum(w['nthreads'] for w in client.scheduler_info()['workers'].values())}\")\n",
    "print(f\"üíæ Total memory: {sum(w['memory_limit'] for w in client.scheduler_info()['workers'].values()) / 1e9:.1f} GB\")\n",
    "\n",
    "# Display cluster information\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd70bbd",
   "metadata": {},
   "source": [
    "## 4. Multi-Source Data Collection\n",
    "\n",
    "Collecting comprehensive datasets from multiple sources:\n",
    "\n",
    "- **Cryptocurrency Data**: Price, volume, market cap (CoinGecko, Binance)\n",
    "- **On-Chain Analytics**: DeFi metrics, network activity (Dune Analytics) \n",
    "- **Derivatives Data**: Implied volatility indices (Deribit DVOL)\n",
    "- **Macroeconomic Data**: Interest rates, volatility indices (FRED)\n",
    "\n",
    "Each source provides unique insights into cryptocurrency market dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01a344b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê STARTING COMPREHENSIVE DATA COLLECTION\n",
      "============================================================\n",
      "üì° Data Collector initialized:\n",
      "   ‚Ä¢ Frequency: 1D\n",
      "   ‚Ä¢ Lookback: 365 days\n",
      "   ‚Ä¢ Top cryptocurrencies: 10\n",
      "   ‚Ä¢ Timezone: Europe/Madrid\n",
      "   ‚Ä¢ Credit Protection: ENABLED (cached_only)\n",
      "\n",
      "üí∞ 1. COLLECTING CRYPTOCURRENCY DATA\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Top 10 cryptocurrencies by market cap\n",
      "   ‚Ä¢ bitcoin, ethereum, ripple, tether, binancecoin...\n",
      "\n",
      "üìà Collecting price data...\n",
      "‚ö†Ô∏è  CoinGecko: 7 failures\n",
      "‚úÖ Price data collected: (366, 9)\n",
      "   ‚Ä¢ Date range: 2024-10-02 to 2025-10-02\n",
      "   ‚Ä¢ Features: 9 across 10 assets\n",
      "\n",
      "‚õìÔ∏è  2. COLLECTING ON-CHAIN ANALYTICS DATA\n",
      "--------------------------------------------------\n",
      "üîí Accessing cached Dune results (NO CREDITS)\n",
      "üîç Checking 25 queries for cached results...\n",
      "‚ö†Ô∏è  CoinGecko: 7 failures\n",
      "‚úÖ Price data collected: (366, 9)\n",
      "   ‚Ä¢ Date range: 2024-10-02 to 2025-10-02\n",
      "   ‚Ä¢ Features: 9 across 10 assets\n",
      "\n",
      "‚õìÔ∏è  2. COLLECTING ON-CHAIN ANALYTICS DATA\n",
      "--------------------------------------------------\n",
      "üîí Accessing cached Dune results (NO CREDITS)\n",
      "üîç Checking 25 queries for cached results...\n",
      "   ‚úÖ Query 5893929: 2102 rows (cached)\n",
      "   ‚úÖ Query 5893929: 2102 rows (cached)\n",
      "   ‚úÖ Query 5893461: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893952: 180 rows (cached)\n",
      "   ‚úÖ Query 5893461: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893952: 180 rows (cached)\n",
      "   ‚ö†Ô∏è  Query 5893947: No cached result\n",
      "   ‚úÖ Query 5894076: 181 rows (cached)\n",
      "   ‚ö†Ô∏è  Query 5893947: No cached result\n",
      "   ‚úÖ Query 5894076: 181 rows (cached)\n",
      "   ‚úÖ Query 5893557: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893307: 1586 rows (cached)\n",
      "   ‚úÖ Query 5893557: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893307: 1586 rows (cached)\n",
      "   ‚úÖ Query 5894092: 1695 rows (cached)\n",
      "   ‚úÖ Query 5894035: 1827 rows (cached)\n",
      "   ‚úÖ Query 5894092: 1695 rows (cached)\n",
      "   ‚úÖ Query 5894035: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893555: 1005 rows (cached)\n",
      "   ‚úÖ Query 5893552: 2830 rows (cached)\n",
      "   ‚úÖ Query 5893555: 1005 rows (cached)\n",
      "   ‚úÖ Query 5893552: 2830 rows (cached)\n",
      "   ‚úÖ Query 5893566: 1005 rows (cached)\n",
      "   ‚úÖ Query 5893566: 1005 rows (cached)\n",
      "   ‚úÖ Query 5893781: 1794 rows (cached)\n",
      "   ‚úÖ Query 5893821: 1816 rows (cached)\n",
      "   ‚úÖ Query 5893781: 1794 rows (cached)\n",
      "   ‚úÖ Query 5893821: 1816 rows (cached)\n",
      "   ‚úÖ Query 5892650: 1713 rows (cached)\n",
      "   ‚úÖ Query 5893009: 1096 rows (cached)\n",
      "   ‚úÖ Query 5892650: 1713 rows (cached)\n",
      "   ‚úÖ Query 5893009: 1096 rows (cached)\n",
      "   ‚úÖ Query 5892998: 1826 rows (cached)\n",
      "   ‚úÖ Query 5892998: 1826 rows (cached)\n",
      "   ‚úÖ Query 5893911: 3926 rows (cached)\n",
      "   ‚úÖ Query 5892742: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893911: 3926 rows (cached)\n",
      "   ‚úÖ Query 5892742: 1827 rows (cached)\n",
      "   ‚úÖ Query 5892720: 1826 rows (cached)\n",
      "   ‚úÖ Query 5892720: 1826 rows (cached)\n",
      "   ‚úÖ Query 5891651: 1794 rows (cached)\n",
      "   ‚úÖ Query 5891651: 1794 rows (cached)\n",
      "   ‚úÖ Query 5892696: 1827 rows (cached)\n",
      "   ‚úÖ Query 5892696: 1827 rows (cached)\n",
      "   ‚úÖ Query 5892424: 1826 rows (cached)\n",
      "   ‚úÖ Query 5892227: 1827 rows (cached)\n",
      "   ‚úÖ Query 5892424: 1826 rows (cached)\n",
      "   ‚úÖ Query 5892227: 1827 rows (cached)\n",
      "   ‚úÖ Query 5891691: 1826 rows (cached)\n",
      "üéä Successfully retrieved 24/25 cached datasets\n",
      "üí∞ Credits used: 0 (cached results)\n",
      "‚úÖ On-chain data: (3926, 79)\n",
      "   ‚Ä¢ Successfully retrieved: query\n",
      "   ‚Ä¢ Total datasets: 1 from cached queries\n",
      "üí∞ Credits used: 0 (cached results only)\n",
      "\n",
      "üìä 3. COLLECTING DERIVATIVES DATA\n",
      "----------------------------------------\n",
      "   ‚úÖ Query 5891691: 1826 rows (cached)\n",
      "üéä Successfully retrieved 24/25 cached datasets\n",
      "üí∞ Credits used: 0 (cached results)\n",
      "‚úÖ On-chain data: (3926, 79)\n",
      "   ‚Ä¢ Successfully retrieved: query\n",
      "   ‚Ä¢ Total datasets: 1 from cached queries\n",
      "üí∞ Credits used: 0 (cached results only)\n",
      "\n",
      "üìä 3. COLLECTING DERIVATIVES DATA\n",
      "----------------------------------------\n",
      "‚úÖ DVOL data: (366, 2)\n",
      "   ‚Ä¢ Implied volatility indices for BTC and ETH\n",
      "\n",
      "üèõÔ∏è  4. COLLECTING MACROECONOMIC DATA\n",
      "----------------------------------------\n",
      "‚úÖ DVOL data: (366, 2)\n",
      "   ‚Ä¢ Implied volatility indices for BTC and ETH\n",
      "\n",
      "üèõÔ∏è  4. COLLECTING MACROECONOMIC DATA\n",
      "----------------------------------------\n",
      "Error fetching move_bond_vol: 'observations'\n",
      "Error fetching move_bond_vol: 'observations'\n",
      "‚úÖ FRED data: (360, 6)\n",
      "   ‚Ä¢ Macroeconomic indicators from Federal Reserve\n",
      "\n",
      "üéâ DATA COLLECTION COMPLETED!\n",
      "========================================\n",
      "üìä Final Dataset Summary:\n",
      "   ‚Ä¢ Price data: (366, 9)\n",
      "   ‚Ä¢ On-chain data: (3926, 79)\n",
      "   ‚Ä¢ DVOL data: (366, 2)\n",
      "   ‚Ä¢ FRED data: (360, 6)\n",
      "   ‚Ä¢ Total features: 96 across 4 data sources\n",
      "üí∞ Total Dune credits used: 0 (cached results only)\n",
      "============================================================\n",
      "‚úÖ FRED data: (360, 6)\n",
      "   ‚Ä¢ Macroeconomic indicators from Federal Reserve\n",
      "\n",
      "üéâ DATA COLLECTION COMPLETED!\n",
      "========================================\n",
      "üìä Final Dataset Summary:\n",
      "   ‚Ä¢ Price data: (366, 9)\n",
      "   ‚Ä¢ On-chain data: (3926, 79)\n",
      "   ‚Ä¢ DVOL data: (366, 2)\n",
      "   ‚Ä¢ FRED data: (360, 6)\n",
      "   ‚Ä¢ Total features: 96 across 4 data sources\n",
      "üí∞ Total Dune credits used: 0 (cached results only)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MULTI-SOURCE DATA COLLECTION PIPELINE (CLEAN & ALIGNED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üåê STARTING COMPREHENSIVE DATA COLLECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize data collector with safe settings\n",
    "collector = CryptoDataCollector(\n",
    "    timezone=TIMEZONE,\n",
    "    top_n=TOP_N,\n",
    "    lookback_days=LOOKBACK_DAYS,\n",
    "    frequency=FREQUENCY,\n",
    "    dune_strategy=\"cached_only\",  # Safe: only use cached results\n",
    "    allow_dune_execution=False    # Protection: never consume credits\n",
    ")\n",
    "\n",
    "print(f\"üì° Data Collector initialized:\")\n",
    "print(f\"   ‚Ä¢ Frequency: {collector.FREQUENCY}\")\n",
    "print(f\"   ‚Ä¢ Lookback: {LOOKBACK_DAYS} days\")\n",
    "print(f\"   ‚Ä¢ Top cryptocurrencies: {TOP_N}\")\n",
    "print(f\"   ‚Ä¢ Timezone: {TIMEZONE}\")\n",
    "print(f\"   ‚Ä¢ Credit Protection: ENABLED (cached_only)\")\n",
    "\n",
    "# 1. CRYPTOCURRENCY UNIVERSE & PRICE DATA\n",
    "print(f\"\\nüí∞ 1. COLLECTING CRYPTOCURRENCY DATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get top cryptocurrency universe  \n",
    "universe_data = collector.coingecko_get_universe(n=TOP_N, output_format=\"both\")\n",
    "if isinstance(universe_data, dict):\n",
    "    top_coins = universe_data['ids']\n",
    "    print(f\"‚úÖ Top {TOP_N} cryptocurrencies by market cap\")\n",
    "    print(f\"   ‚Ä¢ {', '.join(top_coins[:5])}{'...' if TOP_N > 5 else ''}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to get universe data\")\n",
    "    top_coins = ['bitcoin', 'ethereum']  # Fallback\n",
    "\n",
    "# Collect comprehensive price data\n",
    "print(f\"\\nüìà Collecting price data...\")\n",
    "price_data = collector.coingecko_get_price_action(top_coins, sleep_time=SLEEP_TIME)\n",
    "\n",
    "if not price_data.empty:\n",
    "    print(f\"‚úÖ Price data collected: {price_data.shape}\")\n",
    "    print(f\"   ‚Ä¢ Date range: {price_data.index.min().date()} to {price_data.index.max().date()}\")\n",
    "    print(f\"   ‚Ä¢ Features: {price_data.shape[1]} across {len(top_coins)} assets\")\n",
    "else:\n",
    "    print(\"‚ùå No price data collected\")\n",
    "\n",
    "# 2. ON-CHAIN ANALYTICS DATA (SAFE APPROACH)\n",
    "print(f\"\\n‚õìÔ∏è  2. COLLECTING ON-CHAIN ANALYTICS DATA\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Use the direct approach with clean output\n",
    "try:\n",
    "    print(\"üîí Accessing cached Dune results (NO CREDITS)\")\n",
    "    \n",
    "    # Suppress verbose logging\n",
    "    import logging\n",
    "    dune_logger = logging.getLogger('dune_client')\n",
    "    original_level = dune_logger.level\n",
    "    dune_logger.setLevel(logging.ERROR)\n",
    "    \n",
    "    dune_data = collector.get_dune_data_direct()\n",
    "    \n",
    "    # Restore logging\n",
    "    dune_logger.setLevel(original_level)\n",
    "    \n",
    "    if not dune_data.empty:\n",
    "        print(f\"‚úÖ On-chain data: {dune_data.shape}\")\n",
    "        \n",
    "        # Show which datasets were successfully retrieved\n",
    "        dataset_names = []\n",
    "        for col in dune_data.columns:\n",
    "            if '_' in col:\n",
    "                dataset_name = col.split('_')[0]\n",
    "                if dataset_name not in dataset_names:\n",
    "                    dataset_names.append(dataset_name)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Successfully retrieved: {', '.join(dataset_names[:3])}{'...' if len(dataset_names) > 3 else ''}\")\n",
    "        print(f\"   ‚Ä¢ Total datasets: {len(dataset_names)} from cached queries\")\n",
    "        print(f\"üí∞ Credits used: 0 (cached results only)\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No cached on-chain data available\")\n",
    "        print(\"   ‚Ä¢ This is expected - queries may not have recent cached results\")\n",
    "        dune_data = pd.DataFrame()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  On-chain data collection completed with limitations\")\n",
    "    dune_data = pd.DataFrame()\n",
    "\n",
    "# 3. DERIVATIVES DATA\n",
    "print(f\"\\nüìä 3. COLLECTING DERIVATIVES DATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    dvol_data = collector.deribit_get_dvol(currencies=['BTC', 'ETH'])\n",
    "    if not dvol_data.empty:\n",
    "        print(f\"‚úÖ DVOL data: {dvol_data.shape}\")\n",
    "        print(f\"   ‚Ä¢ Implied volatility indices for BTC and ETH\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No DVOL data available\")\n",
    "        dvol_data = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  DVOL collection completed with limitations\")\n",
    "    dvol_data = pd.DataFrame()\n",
    "\n",
    "# 4. MACROECONOMIC DATA\n",
    "print(f\"\\nüèõÔ∏è  4. COLLECTING MACROECONOMIC DATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    fred_data = collector.fred_get_series()\n",
    "    if not fred_data.empty:\n",
    "        print(f\"‚úÖ FRED data: {fred_data.shape}\")\n",
    "        print(f\"   ‚Ä¢ Macroeconomic indicators from Federal Reserve\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No FRED data available\")\n",
    "        fred_data = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  FRED collection completed with limitations\")\n",
    "    fred_data = pd.DataFrame()\n",
    "\n",
    "# Final Summary\n",
    "print(f\"\\nüéâ DATA COLLECTION COMPLETED!\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate total features available\n",
    "total_features = 0\n",
    "datasets_available = 0\n",
    "if not price_data.empty:\n",
    "    total_features += price_data.shape[1]\n",
    "    datasets_available += 1\n",
    "if not dune_data.empty:\n",
    "    total_features += dune_data.shape[1]\n",
    "    datasets_available += 1\n",
    "if not dvol_data.empty:\n",
    "    total_features += dvol_data.shape[1]\n",
    "    datasets_available += 1\n",
    "if not fred_data.empty:\n",
    "    total_features += fred_data.shape[1]\n",
    "    datasets_available += 1\n",
    "\n",
    "print(f\"üìä Final Dataset Summary:\")\n",
    "print(f\"   ‚Ä¢ Price data: {price_data.shape if not price_data.empty else 'None'}\")  \n",
    "print(f\"   ‚Ä¢ On-chain data: {dune_data.shape if not dune_data.empty else 'None'}\")\n",
    "print(f\"   ‚Ä¢ DVOL data: {dvol_data.shape if not dvol_data.empty else 'None'}\")\n",
    "print(f\"   ‚Ä¢ FRED data: {fred_data.shape if not fred_data.empty else 'None'}\")\n",
    "print(f\"   ‚Ä¢ Total features: {total_features} across {datasets_available} data sources\")\n",
    "print(f\"üí∞ Total Dune credits used: 0 (cached results only)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be3087c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query_5893929_btc_block_time', 'query_5893929_date',\n",
       "       'query_5893461_close_price_usd', 'query_5893461_day',\n",
       "       'query_5893461_fees_usd', 'query_5893461_total_fees_btc',\n",
       "       'query_5893461_txn_count', 'query_5893461_txn_volume_btc',\n",
       "       'query_5893952_base_fee_usd', 'query_5893952_blob_sumbmission_fee_usd',\n",
       "       'query_5893952_date', 'query_5893952_mev_tips_usd',\n",
       "       'query_5893952_priority_fee_usd', 'query_5894076_daily_funding_fr',\n",
       "       'query_5894076_date', 'query_5894076_yearly_fr_pct',\n",
       "       'query_5894076_yearly_funding_fr', 'query_5893557_daily_burn',\n",
       "       'query_5893557_time', 'query_5893307_daily_active_addresses_L2s',\n",
       "       'query_5893307_date', 'query_5894092_date',\n",
       "       'query_5894092_total_margin_usd', 'query_5894092_total_revenue_usd',\n",
       "       'query_5894035_date', 'query_5894035_gas_fees_usd',\n",
       "       'query_5893555_date', 'query_5893555_sum_txns', 'query_5893552_day',\n",
       "       'query_5893552_users', 'query_5893566_daily_burn', 'query_5893566_time',\n",
       "       'query_5893781_amount', 'query_5893781_flow_type', 'query_5893781_time',\n",
       "       'query_5893821_date', 'query_5893821_eth_net_etf_flows_eth',\n",
       "       'query_5893821_eth_net_etf_flows_usd', 'query_5892650_date',\n",
       "       'query_5892650_fear', 'query_5892650_greed', 'query_5893009_date',\n",
       "       'query_5893009_total_btc_amount', 'query_5893009_total_usd_value',\n",
       "       'query_5892998_MAP1_Trend', 'query_5892998_MAP1_x',\n",
       "       'query_5892998_MAP2_Trend', 'query_5892998_MAP2_x',\n",
       "       'query_5892998_MAP3_Trend', 'query_5892998_MAP3_x',\n",
       "       'query_5892998_MAP4_Trend', 'query_5892998_MAP4_x',\n",
       "       'query_5892998_MAP_avg', 'query_5892998_date', 'query_5892998_eti',\n",
       "       'query_5892998_eti_num', 'query_5892998_last_eti',\n",
       "       'query_5893911_btc_hash_rate', 'query_5893911_btc_hashing_diff',\n",
       "       'query_5893911_date', 'query_5892742_btc_etf_flow(btc)',\n",
       "       'query_5892742_btc_etf_flow(usd)', 'query_5892742_date',\n",
       "       'query_5892720_date', 'query_5892720_fear_and_greed_index',\n",
       "       'query_5892424_date', 'query_5892424_median_eth_transfer_gas',\n",
       "       'query_5892424_median_eth_transfer_price', 'query_5892424_median_gas',\n",
       "       'query_5892227_agg_share', 'query_5892227_date',\n",
       "       'query_5891691_daily_total_volume', 'query_5891691_day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dune_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b7bef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Re-collecting Dune data with updated method...\n",
      "üîç Checking 25 queries for cached results...\n",
      "   ‚úÖ Query 5893929: 2102 rows (cached)\n",
      "   ‚úÖ Query 5893929: 2102 rows (cached)\n",
      "   ‚úÖ Query 5893461: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893952: 180 rows (cached)\n",
      "   ‚úÖ Query 5893461: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893952: 180 rows (cached)\n",
      "   ‚ö†Ô∏è  Query 5893947: No cached result\n",
      "   ‚úÖ Query 5894076: 181 rows (cached)\n",
      "   ‚ö†Ô∏è  Query 5893947: No cached result\n",
      "   ‚úÖ Query 5894076: 181 rows (cached)\n",
      "   ‚úÖ Query 5893557: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893307: 1586 rows (cached)\n",
      "   ‚úÖ Query 5893557: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893307: 1586 rows (cached)\n",
      "   ‚úÖ Query 5894092: 1695 rows (cached)\n",
      "   ‚úÖ Query 5894035: 1827 rows (cached)\n",
      "   ‚úÖ Query 5894092: 1695 rows (cached)\n",
      "   ‚úÖ Query 5894035: 1827 rows (cached)\n",
      "   ‚úÖ Query 5893555: 1005 rows (cached)\n",
      "   ‚úÖ Query 5893552: 2830 rows (cached)\n",
      "   ‚úÖ Query 5893555: 1005 rows (cached)\n",
      "   ‚úÖ Query 5893552: 2830 rows (cached)\n",
      "   ‚úÖ Query 5893566: 1005 rows (cached)\n",
      "   ‚ùå Query 5893781: 402 Client Error: Payment Requ...\n",
      "   ‚úÖ Query 5893566: 1005 rows (cached)\n",
      "   ‚ùå Query 5893781: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5893821: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892650: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5893821: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892650: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5893009: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892998: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5893009: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892998: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5893911: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892742: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5893911: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892742: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892720: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892720: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5891651: HTTPSConnectionPool(host='api....\n",
      "   ‚ùå Query 5891651: HTTPSConnectionPool(host='api....\n",
      "   ‚ùå Query 5892696: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892696: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892424: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892227: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892424: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5892227: 402 Client Error: Payment Requ...\n",
      "   ‚ùå Query 5891691: 402 Client Error: Payment Requ...\n",
      "üéä Successfully retrieved 11/25 cached datasets\n",
      "üí∞ Credits used: 0 (cached results)\n",
      "‚úÖ Fresh Dune data collected: (2830, 32)\n",
      "üìä Sample columns (no prefixes): ['query_5893929_btc_block_time', 'query_5893929_date', 'query_5893461_close_price_usd', 'query_5893461_day', 'query_5893461_fees_usd']\n",
      "\n",
      "üîç Updated column names:\n",
      "['query_5893929_btc_block_time', 'query_5893929_date', 'query_5893461_close_price_usd', 'query_5893461_day', 'query_5893461_fees_usd', 'query_5893461_total_fees_btc', 'query_5893461_txn_count', 'query_5893461_txn_volume_btc', 'query_5893952_base_fee_usd', 'query_5893952_blob_sumbmission_fee_usd']\n",
      "   ‚ùå Query 5891691: 402 Client Error: Payment Requ...\n",
      "üéä Successfully retrieved 11/25 cached datasets\n",
      "üí∞ Credits used: 0 (cached results)\n",
      "‚úÖ Fresh Dune data collected: (2830, 32)\n",
      "üìä Sample columns (no prefixes): ['query_5893929_btc_block_time', 'query_5893929_date', 'query_5893461_close_price_usd', 'query_5893461_day', 'query_5893461_fees_usd']\n",
      "\n",
      "üîç Updated column names:\n",
      "['query_5893929_btc_block_time', 'query_5893929_date', 'query_5893461_close_price_usd', 'query_5893461_day', 'query_5893461_fees_usd', 'query_5893461_total_fees_btc', 'query_5893461_txn_count', 'query_5893461_txn_volume_btc', 'query_5893952_base_fee_usd', 'query_5893952_blob_sumbmission_fee_usd']\n"
     ]
    }
   ],
   "source": [
    "# Re-collect Dune data with the updated method (no prefixes)\n",
    "print(\"üîÑ Re-collecting Dune data with updated method...\")\n",
    "\n",
    "# Create a new collector instance to ensure fresh collection\n",
    "fresh_collector = CryptoDataCollector(\n",
    "    timezone=TIMEZONE,\n",
    "    top_n=TOP_N,\n",
    "    lookback_days=LOOKBACK_DAYS,\n",
    "    frequency=FREQUENCY,\n",
    "    dune_strategy=\"cached_only\",\n",
    "    allow_dune_execution=False\n",
    ")\n",
    "\n",
    "# Suppress logging for clean output\n",
    "import logging\n",
    "dune_logger = logging.getLogger('dune_client')\n",
    "original_level = dune_logger.level\n",
    "dune_logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Get fresh data without prefixes\n",
    "dune_data_fresh = fresh_collector.get_dune_data_direct()\n",
    "\n",
    "# Restore logging\n",
    "dune_logger.setLevel(original_level)\n",
    "\n",
    "if not dune_data_fresh.empty:\n",
    "    print(f\"‚úÖ Fresh Dune data collected: {dune_data_fresh.shape}\")\n",
    "    print(f\"üìä Sample columns (no prefixes): {list(dune_data_fresh.columns[:5])}\")\n",
    "    # Replace the old data\n",
    "    dune_data = dune_data_fresh\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No fresh data available\")\n",
    "\n",
    "print(f\"\\nüîç Updated column names:\")\n",
    "print(dune_data.columns.tolist()[:10])  # Show first 10 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee89c5e",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Enhanced Credit Protection & Data Collection Status\n",
    "\n",
    "This section implements the lessons learned about safe Dune API usage:\n",
    "\n",
    "### üéØ **Key Improvements:**\n",
    "1. **Credit Protection**: Multiple safeguards prevent accidental Dune credit consumption\n",
    "2. **Direct API Access**: Clean, simple approach using `dune.get_latest_result()`\n",
    "3. **Cached Results**: FREE access to previously executed queries\n",
    "4. **Fallback Strategies**: Graceful degradation when data unavailable\n",
    "\n",
    "### üí∞ **Cost Structure:**\n",
    "- **Cached Results**: FREE (using `get_latest_result()` on previously executed queries)\n",
    "- **Fresh Execution**: ~6 credits per query (only when explicitly enabled)\n",
    "- **Other APIs**: CoinGecko, FRED, Deribit remain unchanged\n",
    "\n",
    "### ‚ö†Ô∏è **Expected API Behavior:**\n",
    "- **402 Payment Required**: Normal response when no cached results exist and execution is disabled\n",
    "- **Connection Errors**: Temporary network issues, system will continue with available data\n",
    "- **Partial Data**: System designed to work with whatever data sources are available\n",
    "\n",
    "### üîß **Configuration:**\n",
    "- `dune_strategy=\"cached_only\"`: Safe default, only accesses cached results\n",
    "- `allow_dune_execution=False`: Critical protection flag\n",
    "- Direct client approach: Simplest and most reliable method\n",
    "\n",
    "The system is working as designed - protecting your credits while utilizing any available cached data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6438c1",
   "metadata": {},
   "source": [
    "## 5. Data Integration & Unified Dataset Construction\n",
    "\n",
    "Combining all data sources into a unified dataset with proper temporal alignment and handling of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326938fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UNIFIED DATASET CONSTRUCTION & DATA INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîó INTEGRATING MULTI-SOURCE DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all available datasets\n",
    "datasets = {\n",
    "    'price_data': price_data,\n",
    "    'dune_data': dune_data, \n",
    "    'dvol_data': dvol_data,\n",
    "    'fred_data': fred_data\n",
    "}\n",
    "\n",
    "# Filter non-empty datasets\n",
    "available_datasets = {name: df for name, df in datasets.items() if len(df) > 0}\n",
    "print(f\"üìä Available datasets: {list(available_datasets.keys())}\")\n",
    "\n",
    "# Temporal alignment and integration\n",
    "unified_data = None\n",
    "integration_stats = {}\n",
    "\n",
    "for name, df in available_datasets.items():\n",
    "    try:\n",
    "        # Ensure proper timezone handling\n",
    "        if df.index.tz is not None:\n",
    "            df_aligned = df.copy()\n",
    "        else:\n",
    "            df_aligned = df.copy()\n",
    "            df_aligned.index = pd.DatetimeIndex(df_aligned.index).tz_localize(TIMEZONE)\n",
    "        \n",
    "        # Convert to date for daily alignment\n",
    "        df_aligned.index = df_aligned.index.tz_convert(TIMEZONE).date\n",
    "        \n",
    "        # Integrate with main dataset\n",
    "        if unified_data is None:\n",
    "            unified_data = df_aligned\n",
    "            integration_stats[name] = {'shape': df_aligned.shape, 'status': 'primary'}\n",
    "        else:\n",
    "            before_shape = unified_data.shape\n",
    "            unified_data = unified_data.join(df_aligned, how='outer')\n",
    "            after_shape = unified_data.shape\n",
    "            integration_stats[name] = {\n",
    "                'shape': df_aligned.shape, \n",
    "                'added_cols': after_shape[1] - before_shape[1],\n",
    "                'status': 'integrated'\n",
    "            }\n",
    "        \n",
    "        print(f\"‚úÖ {name}: {df_aligned.shape} -> Added {integration_stats[name].get('added_cols', df_aligned.shape[1])} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to integrate {name}: {e}\")\n",
    "        integration_stats[name] = {'status': 'failed', 'error': str(e)}\n",
    "\n",
    "# Dataset quality assessment\n",
    "print(f\"\\nüìà UNIFIED DATASET SUMMARY\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"üî¢ Final shape: {unified_data.shape}\")\n",
    "print(f\"üìÖ Date range: {unified_data.index.min()} to {unified_data.index.max()}\")\n",
    "print(f\"‚è±Ô∏è  Total days: {len(unified_data)} days\")\n",
    "\n",
    "# Missing data analysis\n",
    "missing_analysis = unified_data.isnull().sum().sort_values(ascending=False)\n",
    "high_missing = missing_analysis[missing_analysis > len(unified_data) * 0.5]\n",
    "\n",
    "print(f\"\\nüîç DATA QUALITY ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Complete columns: {len(missing_analysis[missing_analysis == 0])}\")\n",
    "print(f\"   ‚Ä¢ Partial missing: {len(missing_analysis[(missing_analysis > 0) & (missing_analysis <= len(unified_data) * 0.5)])}\")\n",
    "print(f\"   ‚Ä¢ High missing (>50%): {len(high_missing)}\")\n",
    "\n",
    "if len(high_missing) > 0:\n",
    "    print(f\"   ‚Ä¢ High missing columns: {list(high_missing.index[:5])}{'...' if len(high_missing) > 5 else ''}\")\n",
    "\n",
    "# Display sample of unified dataset\n",
    "print(f\"\\nüìä UNIFIED DATASET SAMPLE:\")\n",
    "print(unified_data.head())\n",
    "\n",
    "print(f\"\\n‚úÖ Data integration completed - ready for feature engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010818da",
   "metadata": {},
   "source": [
    "## 6. Target Variable Construction & Feature Container Preparation\n",
    "\n",
    "Creating the target variable (realized volatility) and preparing the feature matrix for advanced time series feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TARGET VARIABLE CONSTRUCTION & FEATURE PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üéØ CONSTRUCTING TARGET VARIABLE & FEATURE CONTAINER\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Focus on recent data with sufficient history for feature extraction\n",
    "recent_lookback = min(365, len(unified_data))  # Use up to 365 days or available data\n",
    "X = unified_data.iloc[-recent_lookback:].copy()\n",
    "\n",
    "print(f\"üìä Working dataset: {X.shape} ({recent_lookback} days)\")\n",
    "\n",
    "# Data cleaning and preprocessing\n",
    "print(f\"\\nüßπ DATA PREPROCESSING:\")\n",
    "\n",
    "# 1. Remove columns with excessive missing data (>90% missing)\n",
    "missing_threshold = 0.9\n",
    "cols_before = len(X.columns)\n",
    "sufficient_data_cols = X.isnull().sum() / len(X) < missing_threshold\n",
    "X = X.loc[:, sufficient_data_cols]\n",
    "cols_removed = cols_before - len(X.columns)\n",
    "print(f\"   ‚Ä¢ Removed {cols_removed} columns with >{missing_threshold*100}% missing data\")\n",
    "\n",
    "# 2. Forward fill missing values (max 3 periods)\n",
    "X = X.fillna(method='ffill', limit=3)\n",
    "print(f\"   ‚Ä¢ Applied forward fill (max 3 periods)\")\n",
    "\n",
    "# 3. Target variable construction - Ethereum realized volatility\n",
    "target_price_col = f'prices_{TARGET_COIN}'\n",
    "if target_price_col in X.columns:\n",
    "    print(f\"\\nüéØ TARGET VARIABLE CONSTRUCTION:\")\n",
    "    \n",
    "    # Calculate log returns\n",
    "    X[f'log_returns_{TARGET_COIN}'] = np.log(X[target_price_col]) - np.log(X[target_price_col].shift(1))\n",
    "    \n",
    "    # Realized volatility (absolute log returns)\n",
    "    X[f'realized_vol_{TARGET_COIN}'] = abs(X[f'log_returns_{TARGET_COIN}'])\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Log returns calculated for {TARGET_COIN}\")\n",
    "    print(f\"   ‚Ä¢ Realized volatility computed\")\n",
    "    \n",
    "    # Log returns statistics\n",
    "    log_rets = X[f'log_returns_{TARGET_COIN}'].dropna()\n",
    "    print(f\"   ‚Ä¢ Mean log return: {log_rets.mean():.6f}\")\n",
    "    print(f\"   ‚Ä¢ Volatility (std): {log_rets.std():.6f}\")\n",
    "    print(f\"   ‚Ä¢ Skewness: {log_rets.skew():.3f}\")\n",
    "    print(f\"   ‚Ä¢ Kurtosis: {log_rets.kurtosis():.3f}\")\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Target price column '{target_price_col}' not found!\")\n",
    "\n",
    "# 4. Differencing for stationarity\n",
    "print(f\"\\nüìà STATIONARITY TRANSFORMATION:\")\n",
    "X_stationary = X.diff().dropna()\n",
    "print(f\"   ‚Ä¢ Applied first differencing\")\n",
    "print(f\"   ‚Ä¢ Shape after differencing: {X_stationary.shape}\")\n",
    "\n",
    "# 5. Target variable alignment\n",
    "y = X_stationary[f'realized_vol_{TARGET_COIN}'].shift(-1).dropna().rename(\"target\")\n",
    "print(f\"   ‚Ä¢ Target variable (next-period volatility): {len(y)} observations\")\n",
    "\n",
    "# 6. Feature-target alignment\n",
    "common_idx = X_stationary.index.intersection(y.index)\n",
    "X_final = X_stationary.loc[common_idx]\n",
    "y_final = y.loc[common_idx]\n",
    "\n",
    "print(f\"\\n‚úÖ FINAL FEATURE CONTAINER:\")\n",
    "print(f\"   ‚Ä¢ Features shape: {X_final.shape}\")\n",
    "print(f\"   ‚Ä¢ Target shape: {len(y_final)}\")\n",
    "print(f\"   ‚Ä¢ Date range: {X_final.index.min()} to {X_final.index.max()}\")\n",
    "\n",
    "# Target variable statistics\n",
    "print(f\"\\nüìä TARGET VARIABLE STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Mean: {y_final.mean():.6f}\")\n",
    "print(f\"   ‚Ä¢ Std: {y_final.std():.6f}\")\n",
    "print(f\"   ‚Ä¢ Min: {y_final.min():.6f}\")\n",
    "print(f\"   ‚Ä¢ Max: {y_final.max():.6f}\")\n",
    "print(f\"   ‚Ä¢ 95th percentile: {y_final.quantile(0.95):.6f}\")\n",
    "\n",
    "# Visualization of target variable\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Time series plot\n",
    "plt.subplot(2, 2, 1)\n",
    "y_final.plot(alpha=0.7, color='darkblue')\n",
    "plt.title(f'{TARGET_COIN.title()} Realized Volatility (Target Variable)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Realized Volatility')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution plot\n",
    "plt.subplot(2, 2, 2)\n",
    "y_final.hist(bins=50, alpha=0.7, color='darkgreen', edgecolor='black')\n",
    "plt.title('Target Variable Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Realized Volatility')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Log returns plot\n",
    "plt.subplot(2, 2, 3)\n",
    "X_final[f'log_returns_{TARGET_COIN}'].plot(alpha=0.7, color='darkred')\n",
    "plt.title(f'{TARGET_COIN.title()} Log Returns', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Log Returns')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Autocorrelation of target\n",
    "plt.subplot(2, 2, 4)\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(y_final.dropna())\n",
    "plt.title('Target Variable Autocorrelation', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüöÄ Ready for TSFresh feature engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f96292",
   "metadata": {},
   "source": [
    "## 7. Advanced Feature Engineering with TSFresh\n",
    "\n",
    "This section implements the core of our advanced feature engineering pipeline:\n",
    "\n",
    "1. **Time Series Rolling**: Converting wide-format data to rolled time series format\n",
    "2. **TSFresh Feature Extraction**: Generating 600+ statistical time series features  \n",
    "3. **Distributed Computing**: Using Dask for parallel feature extraction\n",
    "4. **Feature Selection**: Statistical significance testing to select relevant features\n",
    "\n",
    "This approach mirrors the methodology from  development-workspace\\LatestNotebook.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3472915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED FEATURE ENGINEERING WITH TSFRESH & DASK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üß† ADVANCED FEATURE ENGINEERING PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define Dask processing functions (mirroring LatestNotebook approach)\n",
    "def roll_dask(df):\n",
    "    \"\"\"Roll time series for TSFresh feature extraction\"\"\"\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"üîÑ Processing partition with {len(df)} rows, columns: {df.columns.tolist()[:5]}...\")\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Roll time series with specified window\n",
    "    rolled = roll_time_series(\n",
    "        df,\n",
    "        column_id='variable',\n",
    "        column_sort='date',\n",
    "        max_timeshift=TIME_WINDOW,\n",
    "        min_timeshift=1,\n",
    "        rolling_direction=1,\n",
    "        n_jobs=1  # Single job per partition for Dask\n",
    "    )\n",
    "    return rolled\n",
    "\n",
    "def extract_dask(df):\n",
    "    \"\"\"Extract TSFresh features from rolled time series\"\"\"\n",
    "    df = df.copy().dropna()\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"üîç Extracting features for partition with {len(df)} rows...\")\n",
    "    \n",
    "    # Use efficient feature parameters to balance speed vs. feature richness\n",
    "    features = extract_features(\n",
    "        df,\n",
    "        column_id='id',\n",
    "        column_sort='date', \n",
    "        column_kind='variable',\n",
    "        column_value='value',\n",
    "        default_fc_parameters=EXTRACTION_SETTINGS,\n",
    "        n_jobs=1,  # Single job per partition\n",
    "        disable_progressbar=True\n",
    "    )\n",
    "    return features\n",
    "\n",
    "def select_dask(df, y):\n",
    "    \"\"\"Select statistically significant features\"\"\"\n",
    "    df = df.reset_index(level=0, drop=True).join(y, how='inner').dropna()\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"üéØ Selecting features for partition with {len(df)} rows...\")\n",
    "    \n",
    "    # Feature selection with FDR control\n",
    "    features = select_features(\n",
    "        df.drop('target', axis=1),\n",
    "        df['target'],\n",
    "        ml_task='regression',\n",
    "        fdr_level=DEFAULT_FDR_LEVEL,\n",
    "        hypotheses_independent=False,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    return features\n",
    "\n",
    "# STEP 1: Convert to long format for TSFresh processing\n",
    "print(f\"\\nüìã STEP 1: DATA FORMAT CONVERSION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Convert wide DataFrame to long format (variable-value pairs)\n",
    "FC = X_final.reset_index().melt(id_vars=['date']).sort_values(by='variable')\n",
    "print(f\"‚úÖ Converted to long format: {FC.shape}\")\n",
    "\n",
    "# Create Dask DataFrame with one partition per variable for parallel processing\n",
    "n_partitions = FC['variable'].nunique()\n",
    "FC_dask = dd.from_pandas(FC, npartitions=n_partitions)\n",
    "\n",
    "# Verify partitioning (each partition should have one unique variable)\n",
    "unique_vars_per_partition = FC_dask.map_partitions(lambda df: df['variable'].nunique()).compute()\n",
    "print(f\"üìä Created {n_partitions} partitions (variables per partition: {unique_vars_per_partition.unique()})\")\n",
    "\n",
    "# Display sample of long format data\n",
    "print(f\"\\nüìã LONG FORMAT SAMPLE:\")\n",
    "print(FC.head(10))\n",
    "\n",
    "# STEP 2: Time series rolling with Dask\n",
    "print(f\"\\nüîÑ STEP 2: TIME SERIES ROLLING\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Test rolling on one partition to get metadata\n",
    "print(\"üß™ Testing rolling on sample partition...\")\n",
    "df_test = FC_dask.partitions[0].compute()\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])\n",
    "\n",
    "rolled_test = roll_time_series(\n",
    "    df_test,\n",
    "    column_id='variable',\n",
    "    column_sort='date',\n",
    "    max_timeshift=TIME_WINDOW,\n",
    "    min_timeshift=1,\n",
    "    rolling_direction=1\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Rolling test successful: {rolled_test.shape}\")\n",
    "\n",
    "# Apply rolling to all partitions with Dask\n",
    "print(\"üöÄ Applying rolling transformation to all partitions...\")\n",
    "rolled_dask = FC_dask.map_partitions(roll_dask, meta=rolled_test).persist()\n",
    "\n",
    "print(f\"‚úÖ Time series rolling completed and persisted in memory\")\n",
    "\n",
    "# STEP 3: Feature extraction with TSFresh\n",
    "print(f\"\\nüîç STEP 3: TSFRESH FEATURE EXTRACTION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"‚öôÔ∏è  Using {EXTRACTION_SETTINGS.__class__.__name__} feature parameters\")\n",
    "print(\"üöÄ Starting distributed feature extraction...\")\n",
    "\n",
    "# Extract features using Dask (this is the computationally intensive step)\n",
    "features_dask = rolled_dask.map_partitions(extract_dask, enforce_metadata=False).persist()\n",
    "\n",
    "print(f\"‚úÖ Feature extraction completed and persisted\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Computing feature extraction results...\")\n",
    "progress_bar = progress.ProgressBar()\n",
    "progress_bar.register()\n",
    "\n",
    "# Compute results from all partitions\n",
    "extracted_futures = client.compute(features_dask.to_delayed())\n",
    "extracted_results = []\n",
    "\n",
    "for i, future in enumerate(extracted_futures):\n",
    "    try:\n",
    "        result = future.result()\n",
    "        if len(result) > 0:\n",
    "            extracted_results.append(result)\n",
    "            print(f\"‚úÖ Partition {i+1}: {result.shape} features extracted\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Partition {i+1}: No features extracted\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Partition {i+1} failed: {e}\")\n",
    "\n",
    "# Combine all extracted features\n",
    "if extracted_results:\n",
    "    tsfresh_features = pd.concat(extracted_results, axis=0, sort=False)\n",
    "    print(f\"\\nüéä FEATURE EXTRACTION COMPLETED!\")\n",
    "    print(f\"   ‚Ä¢ Total extracted features: {tsfresh_features.shape}\")\n",
    "    print(f\"   ‚Ä¢ Feature types: {len(tsfresh_features.columns)} unique features\")\n",
    "    \n",
    "    # Display sample features\n",
    "    print(f\"\\nüìä SAMPLE EXTRACTED FEATURES:\")\n",
    "    print(tsfresh_features.head())\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No features were successfully extracted!\")\n",
    "    tsfresh_features = pd.DataFrame()\n",
    "\n",
    "print(f\"\\nüéØ Ready for feature selection phase!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30d8cb",
   "metadata": {},
   "source": [
    "## 8. Feature Selection & Final Dataset Construction\n",
    "\n",
    "Statistical feature selection using False Discovery Rate (FDR) control to identify the most predictive features while controlling for multiple testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a545319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SELECTION & FINAL DATASET CONSTRUCTION  \n",
    "# =============================================================================\n",
    "\n",
    "print(\"üéØ FEATURE SELECTION & FINAL DATASET CONSTRUCTION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# STEP 4: Statistical Feature Selection\n",
    "print(f\"\\nüî¨ STEP 4: STATISTICAL FEATURE SELECTION\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "if len(tsfresh_features) > 0:\n",
    "    print(f\"üìä Starting feature selection from {tsfresh_features.shape[1]} TSFresh features...\")\n",
    "    \n",
    "    # Apply statistical feature selection with Dask\n",
    "    selected_dask = tsfresh_features.apply(lambda df: select_dask(df, y_final), axis=1)\n",
    "    \n",
    "    print(\"üöÄ Computing feature selection results...\")\n",
    "    selected_futures = client.compute(selected_dask.to_delayed()) if hasattr(selected_dask, 'to_delayed') else []\n",
    "    \n",
    "    # Alternative approach: Direct feature selection on TSFresh features\n",
    "    print(\"üî¨ Applying direct feature selection...\")\n",
    "    \n",
    "    # Align TSFresh features with target\n",
    "    aligned_idx = tsfresh_features.index.intersection(y_final.index)\n",
    "    tsfresh_aligned = tsfresh_features.loc[aligned_idx]\n",
    "    y_aligned = y_final.loc[aligned_idx]\n",
    "    \n",
    "    if len(tsfresh_aligned) > 0 and len(y_aligned) > 0:\n",
    "        print(f\"‚úÖ Aligned data: {tsfresh_aligned.shape} features, {len(y_aligned)} targets\")\n",
    "        \n",
    "        # Select significant TSFresh features\n",
    "        selected_tsfresh = select_features(\n",
    "            tsfresh_aligned,\n",
    "            y_aligned,\n",
    "            ml_task='regression', \n",
    "            fdr_level=DEFAULT_FDR_LEVEL,\n",
    "            hypotheses_independent=False,\n",
    "            n_jobs=-1  # Use all available cores\n",
    "        )\n",
    "        \n",
    "        print(f\"üéä TSFresh feature selection completed!\")\n",
    "        print(f\"   ‚Ä¢ Selected features: {selected_tsfresh.shape}\")\n",
    "        print(f\"   ‚Ä¢ Selection rate: {selected_tsfresh.shape[1]/tsfresh_aligned.shape[1]*100:.1f}%\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No aligned TSFresh features available\")\n",
    "        selected_tsfresh = pd.DataFrame()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No TSFresh features available for selection\")\n",
    "    selected_tsfresh = pd.DataFrame()\n",
    "\n",
    "# STEP 5: Base Feature Selection (Original Variables)\n",
    "print(f\"\\nüìä STEP 5: BASE FEATURE SELECTION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(\"üî¨ Selecting significant base features...\")\n",
    "\n",
    "# Select significant features from original variables\n",
    "base_selected = select_features(\n",
    "    X_final, \n",
    "    y_final, \n",
    "    fdr_level=DEFAULT_FDR_LEVEL, \n",
    "    ml_task='regression',\n",
    "    hypotheses_independent=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Base feature selection completed!\")\n",
    "print(f\"   ‚Ä¢ Selected features: {base_selected.shape}\")\n",
    "print(f\"   ‚Ä¢ Selection rate: {base_selected.shape[1]/X_final.shape[1]*100:.1f}%\")\n",
    "\n",
    "# STEP 6: Final Feature Set Construction\n",
    "print(f\"\\nüèóÔ∏è  STEP 6: FINAL FEATURE SET CONSTRUCTION\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Combine TSFresh and base features\n",
    "if len(selected_tsfresh) > 0:\n",
    "    print(\"üîó Combining TSFresh and base features...\")\n",
    "    final_features = selected_tsfresh.join(base_selected, how='outer')\n",
    "    feature_sources = {\n",
    "        'tsfresh': selected_tsfresh.shape[1],\n",
    "        'base': base_selected.shape[1],\n",
    "        'total': final_features.shape[1]\n",
    "    }\n",
    "else:\n",
    "    print(\"üìä Using base features only...\")\n",
    "    final_features = base_selected\n",
    "    feature_sources = {\n",
    "        'tsfresh': 0,\n",
    "        'base': base_selected.shape[1], \n",
    "        'total': final_features.shape[1]\n",
    "    }\n",
    "\n",
    "# Final alignment with target\n",
    "common_idx = final_features.index.intersection(y_final.index)\n",
    "final_features = final_features.loc[common_idx]\n",
    "y_final_aligned = y_final.loc[common_idx]\n",
    "\n",
    "print(f\"üéä FINAL DATASET CONSTRUCTED!\")\n",
    "print(f\"   ‚Ä¢ TSFresh features: {feature_sources['tsfresh']}\")\n",
    "print(f\"   ‚Ä¢ Base features: {feature_sources['base']}\")\n",
    "print(f\"   ‚Ä¢ Total features: {feature_sources['total']}\")\n",
    "print(f\"   ‚Ä¢ Samples: {len(final_features)}\")\n",
    "print(f\"   ‚Ä¢ Date range: {final_features.index.min()} to {final_features.index.max()}\")\n",
    "\n",
    "# Feature importance preview (correlation with target)\n",
    "if len(final_features) > 0:\n",
    "    feature_correlations = final_features.corrwith(y_final_aligned).abs().sort_values(ascending=False)\n",
    "    top_features = feature_correlations.head(10)\n",
    "    \n",
    "    print(f\"\\nüîù TOP 10 FEATURES BY CORRELATION:\")\n",
    "    for i, (feature, corr) in enumerate(top_features.items(), 1):\n",
    "        print(f\"   {i:2d}. {feature[:50]:<50} | {corr:.4f}\")\n",
    "\n",
    "# Create final dataset for ML pipeline\n",
    "final_dataset = final_features.join(y_final_aligned.rename('target'), how='inner')\n",
    "\n",
    "print(f\"\\nüìà FEATURE STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Mean features per sample: {final_features.notna().sum(axis=1).mean():.1f}\")\n",
    "print(f\"   ‚Ä¢ Feature completeness: {(1 - final_features.isnull().sum().sum() / (final_features.shape[0] * final_features.shape[1]))*100:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Target correlation range: [{feature_correlations.min():.4f}, {feature_correlations.max():.4f}]\")\n",
    "\n",
    "# Display final dataset sample\n",
    "print(f\"\\nüìä FINAL DATASET SAMPLE:\")\n",
    "print(final_dataset.head())\n",
    "\n",
    "print(f\"\\nüöÄ Ready for XGBoost + Optuna ML pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520d9d1",
   "metadata": {},
   "source": [
    "## 9. XGBoost + Optuna ML Pipeline\n",
    "\n",
    "Final machine learning pipeline with:\n",
    "\n",
    "1. **Dask DMatrix Construction**: Distributed data matrices for scalable training\n",
    "2. **Optuna Hyperparameter Optimization**: Bayesian optimization for best parameters  \n",
    "3. **XGBoost Training**: Gradient boosting with early stopping\n",
    "4. **Model Evaluation**: Comprehensive metrics and visualization\n",
    "\n",
    "This mirrors the sophisticated approach from LatestNotebook.ipynb with professional presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6eb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# XGBOOST + OPTUNA MACHINE LEARNING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ü§ñ XGBOOST + OPTUNA ML PIPELINE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Optuna objective function for hyperparameter optimization\n",
    "def optuna_objective(trial):\n",
    "    \"\"\"Optuna objective function for XGBoost hyperparameter optimization\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": DEFAULT_XGB_METRIC,\n",
    "        \"tree_method\": DEFAULT_TREE_METHOD,\n",
    "        \"random_state\": RANDOM_SEED,\n",
    "        \n",
    "        # Hyperparameters to optimize\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.1, 10, log=True),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 0.01, 10.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.01, 10.0, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "    }\n",
    "    \n",
    "    # Train model with current parameters\n",
    "    model = dxgb.train(\n",
    "        client,\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=params[\"n_estimators\"],\n",
    "        early_stopping_rounds=DEFAULT_EARLY_STOPPING,\n",
    "        evals=[(dtrain, \"train\")],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    return model[\"history\"][\"train\"][DEFAULT_XGB_METRIC][-1]\n",
    "\n",
    "# STEP 1: Prepare Dask DMatrix\n",
    "print(f\"\\nüèóÔ∏è  STEP 1: DASK DMATRIX PREPARATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if len(final_dataset) > 50:  # Ensure sufficient data\n",
    "    \n",
    "    # Split into features and target\n",
    "    X_ml = final_dataset.drop('target', axis=1)\n",
    "    y_ml = final_dataset['target']\n",
    "    \n",
    "    print(f\"üìä ML Dataset prepared:\")\n",
    "    print(f\"   ‚Ä¢ Features: {X_ml.shape}\")\n",
    "    print(f\"   ‚Ä¢ Target: {len(y_ml)} samples\")\n",
    "    \n",
    "    # Create Dask DataFrames for distributed processing\n",
    "    X_dask = dd.from_pandas(X_ml, npartitions=SPLITS)\n",
    "    y_dask = dd.from_pandas(y_ml, npartitions=SPLITS)\n",
    "    \n",
    "    # Time series split for validation\n",
    "    n_samples = len(X_ml)\n",
    "    n_train = int(n_samples * 0.8)  # 80% training, 20% test\n",
    "    \n",
    "    X_train_dask = X_dask.iloc[:n_train]\n",
    "    X_test_dask = X_dask.iloc[n_train:]\n",
    "    y_train_dask = y_dask.iloc[:n_train]\n",
    "    y_test_dask = y_dask.iloc[n_train:]\n",
    "    \n",
    "    # Persist in memory for faster access\n",
    "    X_train_dask, X_test_dask, y_train_dask, y_test_dask = client.persist([\n",
    "        X_train_dask, X_test_dask, y_train_dask, y_test_dask\n",
    "    ])\n",
    "    \n",
    "    # Create DMatrix for XGBoost\n",
    "    dtrain = dxgb.DaskDMatrix(client, X_train_dask, y_train_dask)\n",
    "    \n",
    "    print(f\"‚úÖ Dask DMatrix created successfully!\")\n",
    "    print(f\"   ‚Ä¢ Training samples: {n_train}\")\n",
    "    print(f\"   ‚Ä¢ Test samples: {n_samples - n_train}\")\n",
    "    \n",
    "    # STEP 2: Optuna Hyperparameter Optimization\n",
    "    print(f\"\\nüß™ STEP 2: OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"üöÄ Starting Bayesian optimization with {DEFAULT_N_TRIALS} trials...\")\n",
    "    \n",
    "    # Create Optuna study with Dask storage\n",
    "    storage = DaskStorage()\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        storage=storage,\n",
    "        study_name=f\"crypto_volatility_{TARGET_COIN}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        optuna_objective,\n",
    "        n_trials=DEFAULT_N_TRIALS,\n",
    "        n_jobs=4,  # Parallel trials\n",
    "        gc_after_trial=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    print(f\"üéä HYPERPARAMETER OPTIMIZATION COMPLETED!\")\n",
    "    print(f\"   ‚Ä¢ Best {DEFAULT_XGB_METRIC.upper()}: {study.best_value:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Total trials: {len(study.trials)}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST HYPERPARAMETERS:\")\n",
    "    for param, value in study.best_params.items():\n",
    "        print(f\"   ‚Ä¢ {param}: {value}\")\n",
    "    \n",
    "    # STEP 3: Final Model Training\n",
    "    print(f\"\\nüå≥ STEP 3: FINAL MODEL TRAINING\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    print(\"üöÄ Training final model with optimized hyperparameters...\")\n",
    "    \n",
    "    final_model = dxgb.train(\n",
    "        client,\n",
    "        study.best_params,\n",
    "        dtrain,\n",
    "        num_boost_round=study.best_params.get(\"n_estimators\", DEFAULT_N_ROUNDS),\n",
    "        early_stopping_rounds=DEFAULT_EARLY_STOPPING,\n",
    "        evals=[(dtrain, \"train\")],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Final model training completed!\")\n",
    "    \n",
    "    # STEP 4: Model Predictions\n",
    "    print(f\"\\nüîÆ STEP 4: MODEL PREDICTIONS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Get feature names from model\n",
    "    model_features = final_model['booster'].feature_names\n",
    "    print(f\"üìä Model uses {len(model_features)} features\")\n",
    "    \n",
    "    # Create test DMatrix\n",
    "    dtest = dxgb.DaskDMatrix(client, X_test_dask[model_features])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = dxgb.predict(client, final_model, dtest)\n",
    "    \n",
    "    print(f\"‚úÖ Predictions completed!\")\n",
    "    \n",
    "    # STEP 5: Model Evaluation\n",
    "    print(f\"\\nüìà STEP 5: MODEL EVALUATION\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Convert to pandas for evaluation\n",
    "    y_test_pd = y_test_dask.compute()\n",
    "    predictions_pd = pd.Series(predictions.compute(), index=y_test_pd.index)\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    r2 = r2_score(y_test_pd, predictions_pd)\n",
    "    mae = mean_absolute_error(y_test_pd, predictions_pd)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_pd, predictions_pd))\n",
    "    \n",
    "    # Advanced metrics\n",
    "    std_target = y_test_pd.std()\n",
    "    mae_std_ratio = mae / std_target\n",
    "    \n",
    "    # MASE (Mean Absolute Scaled Error)\n",
    "    naive_forecast = y_test_pd.shift(1)  \n",
    "    mae_naive = mean_absolute_error(y_test_pd[1:], naive_forecast[1:])\n",
    "    mase = mae / mae_naive if mae_naive != 0 else np.nan\n",
    "    \n",
    "    print(f\"üéä MODEL PERFORMANCE METRICS:\")\n",
    "    print(f\"   ‚Ä¢ R¬≤ Score: {r2:.6f}\")\n",
    "    print(f\"   ‚Ä¢ MAE: {mae:.6f}\")\n",
    "    print(f\"   ‚Ä¢ RMSE: {rmse:.6f}\")  \n",
    "    print(f\"   ‚Ä¢ MAE/StdDev: {mae_std_ratio:.6f}\")\n",
    "    print(f\"   ‚Ä¢ MASE: {mase:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Target Std: {std_target:.6f}\")\n",
    "    \n",
    "    model_performance = {\n",
    "        'r2_score': r2,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse, \n",
    "        'mae_std_ratio': mae_std_ratio,\n",
    "        'mase': mase,\n",
    "        'target_std': std_target,\n",
    "        'best_params': study.best_params,\n",
    "        'best_optuna_score': study.best_value\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüéâ ADVANCED ML PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Insufficient data for ML pipeline: {len(final_dataset)} samples (minimum: 50)\")\n",
    "    model_performance = None\n",
    "    predictions_pd = None\n",
    "    y_test_pd = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf83ea7",
   "metadata": {},
   "source": [
    "## 10. Advanced Visualization & Results Analysis\n",
    "\n",
    "Professional visualization and comprehensive analysis of model performance, feature importance, and prediction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADVANCED VISUALIZATION & RESULTS ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä ADVANCED VISUALIZATION & RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if model_performance is not None and predictions_pd is not None:\n",
    "    \n",
    "    # Create comprehensive visualization dashboard\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # 1. Time Series Prediction Plot\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    viz_data = pd.DataFrame({\n",
    "        'Actual': y_test_pd,\n",
    "        'Predicted': predictions_pd\n",
    "    })\n",
    "    \n",
    "    viz_data.plot(ax=ax1, alpha=0.8, linewidth=2)\n",
    "    ax1.set_title(f'{TARGET_COIN.title()} Realized Volatility Forecasting\\nTime Series Predictions', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Realized Volatility')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Prediction Scatter Plot\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    ax2.scatter(y_test_pd, predictions_pd, alpha=0.6, s=30, color='darkblue')\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test_pd.min(), predictions_pd.min())\n",
    "    max_val = max(y_test_pd.max(), predictions_pd.max())\n",
    "    ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax2.set_xlabel('Actual Volatility')\n",
    "    ax2.set_ylabel('Predicted Volatility')\n",
    "    ax2.set_title(f'Prediction Accuracy\\nR¬≤ = {model_performance[\"r2_score\"]:.4f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Residuals Plot\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    residuals = y_test_pd - predictions_pd\n",
    "    ax3.scatter(predictions_pd, residuals, alpha=0.6, s=30, color='darkgreen')\n",
    "    ax3.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    ax3.set_xlabel('Predicted Volatility')\n",
    "    ax3.set_ylabel('Residuals')\n",
    "    ax3.set_title(f'Residuals Analysis\\nMAE = {model_performance[\"mae\"]:.6f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Prediction Distribution\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    y_test_pd.hist(bins=30, alpha=0.7, label='Actual', color='blue', density=True)\n",
    "    predictions_pd.hist(bins=30, alpha=0.7, label='Predicted', color='orange', density=True)\n",
    "    ax4.set_xlabel('Volatility')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.set_title('Distribution Comparison', fontsize=12, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Error Distribution\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    residuals.hist(bins=30, alpha=0.7, color='darkred', edgecolor='black')\n",
    "    ax5.set_xlabel('Prediction Error')\n",
    "    ax5.set_ylabel('Frequency')\n",
    "    ax5.set_title(f'Error Distribution\\nRMSE = {model_performance[\"rmse\"]:.6f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Feature Importance (if available)\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    try:\n",
    "        if hasattr(final_model['booster'], 'get_score'):\n",
    "            importance = final_model['booster'].get_score(importance_type='gain')\n",
    "            if importance:\n",
    "                top_features = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True)[:15])\n",
    "                feature_names = [name[:20] + '...' if len(name) > 20 else name for name in top_features.keys()]\n",
    "                values = list(top_features.values())\n",
    "                \n",
    "                y_pos = np.arange(len(feature_names))\n",
    "                ax6.barh(y_pos, values, alpha=0.8, color='skyblue', edgecolor='black')\n",
    "                ax6.set_yticks(y_pos)\n",
    "                ax6.set_yticklabels(feature_names, fontsize=8)\n",
    "                ax6.set_xlabel('Feature Importance (Gain)')\n",
    "                ax6.set_title('Top 15 Feature Importance', fontsize=12, fontweight='bold')\n",
    "                ax6.grid(True, alpha=0.3, axis='x')\n",
    "            else:\n",
    "                ax6.text(0.5, 0.5, 'Feature importance\\nnot available', \n",
    "                        ha='center', va='center', transform=ax6.transAxes, fontsize=12)\n",
    "                ax6.set_title('Feature Importance', fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            ax6.text(0.5, 0.5, 'Feature importance\\nnot available', \n",
    "                    ha='center', va='center', transform=ax6.transAxes, fontsize=12)\n",
    "            ax6.set_title('Feature Importance', fontsize=12, fontweight='bold')\n",
    "    except Exception as e:\n",
    "        ax6.text(0.5, 0.5, f'Feature importance\\nerror: {str(e)[:30]}...', \n",
    "                ha='center', va='center', transform=ax6.transAxes, fontsize=10)\n",
    "        ax6.set_title('Feature Importance', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 7. Optuna Optimization History\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    if 'study' in locals() and len(study.trials) > 1:\n",
    "        trial_values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "        if trial_values:\n",
    "            ax7.plot(trial_values, alpha=0.7, linewidth=2, color='purple')\n",
    "            ax7.axhline(y=study.best_value, color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Best: {study.best_value:.6f}')\n",
    "            ax7.set_xlabel('Trial Number')\n",
    "            ax7.set_ylabel(f'{DEFAULT_XGB_METRIC.upper()}')\n",
    "            ax7.set_title('Optuna Optimization History', fontsize=12, fontweight='bold')\n",
    "            ax7.legend()\n",
    "            ax7.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax7.text(0.5, 0.5, 'No optimization\\ndata available', \n",
    "                    ha='center', va='center', transform=ax7.transAxes, fontsize=12)\n",
    "            ax7.set_title('Optimization History', fontsize=12, fontweight='bold')\n",
    "    else:\n",
    "        ax7.text(0.5, 0.5, 'No optimization\\ndata available', \n",
    "                ha='center', va='center', transform=ax7.transAxes, fontsize=12)\n",
    "        ax7.set_title('Optimization History', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 8. Model Performance Summary\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    ax8.axis('off')\n",
    "    \n",
    "    performance_text = f\"\"\"\n",
    "    üìä MODEL PERFORMANCE SUMMARY\n",
    "    \n",
    "    üéØ Target: {TARGET_COIN.title()} Realized Volatility\n",
    "    üìÖ Test Period: {len(y_test_pd)} days\n",
    "    \n",
    "    üìà REGRESSION METRICS:\n",
    "    ‚Ä¢ R¬≤ Score: {model_performance['r2_score']:.4f}\n",
    "    ‚Ä¢ MAE: {model_performance['mae']:.6f}\n",
    "    ‚Ä¢ RMSE: {model_performance['rmse']:.6f}\n",
    "    ‚Ä¢ MASE: {model_performance['mase']:.4f}\n",
    "    ‚Ä¢ MAE/StdDev: {model_performance['mae_std_ratio']:.4f}\n",
    "    \n",
    "    üß™ OPTIMIZATION:\n",
    "    ‚Ä¢ Best Optuna Score: {model_performance['best_optuna_score']:.6f}\n",
    "    ‚Ä¢ Trials: {len(study.trials) if 'study' in locals() else 'N/A'}\n",
    "    \n",
    "    üèóÔ∏è DATASET:\n",
    "    ‚Ä¢ Features: {len(model_features) if 'model_features' in locals() else 'N/A'}\n",
    "    ‚Ä¢ Training Samples: {n_train if 'n_train' in locals() else 'N/A'}\n",
    "    ‚Ä¢ Test Samples: {len(y_test_pd)}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax8.text(0.05, 0.95, performance_text, transform=ax8.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    # 9. Prediction Confidence Intervals (simplified)\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    \n",
    "    # Calculate rolling prediction accuracy\n",
    "    window_size = min(20, len(y_test_pd) // 5)\n",
    "    if window_size > 2:\n",
    "        rolling_mae = pd.Series(np.abs(residuals)).rolling(window=window_size).mean()\n",
    "        rolling_mae.plot(ax=ax9, alpha=0.8, linewidth=2, color='darkred')\n",
    "        ax9.fill_between(rolling_mae.index, 0, rolling_mae.values, alpha=0.3, color='red')\n",
    "        ax9.set_xlabel('Time')\n",
    "        ax9.set_ylabel('Rolling MAE')\n",
    "        ax9.set_title(f'Prediction Accuracy Over Time\\n(Window: {window_size} days)', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "        ax9.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax9.text(0.5, 0.5, 'Insufficient data\\nfor rolling analysis', \n",
    "                ha='center', va='center', transform=ax9.transAxes, fontsize=12)\n",
    "        ax9.set_title('Rolling Accuracy', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary Statistics Table\n",
    "    print(f\"\\nüìä COMPREHENSIVE RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary_data = {\n",
    "        'Metric': ['R¬≤ Score', 'MAE', 'RMSE', 'MASE', 'MAE/StdDev', 'Best Optuna Score'],\n",
    "        'Value': [\n",
    "            f\"{model_performance['r2_score']:.6f}\",\n",
    "            f\"{model_performance['mae']:.6f}\",\n",
    "            f\"{model_performance['rmse']:.6f}\",\n",
    "            f\"{model_performance['mase']:.6f}\",\n",
    "            f\"{model_performance['mae_std_ratio']:.6f}\",\n",
    "            f\"{model_performance['best_optuna_score']:.6f}\"\n",
    "        ],\n",
    "        'Interpretation': [\n",
    "            'Explained variance (higher = better)',\n",
    "            'Average absolute error (lower = better)',\n",
    "            'Root mean squared error (lower = better)',\n",
    "            'Mean absolute scaled error (lower = better)',\n",
    "            'Error relative to volatility (lower = better)',\n",
    "            'Optimization objective value'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nüéä ADVANCED CRYPTOCURRENCY VOLATILITY FORECASTING COMPLETED!\")\n",
    "    print(f\"üéØ Model successfully predicts {TARGET_COIN.title()} realized volatility\")\n",
    "    print(f\"üìà Achieved R¬≤ = {model_performance['r2_score']:.4f} with MAE = {model_performance['mae']:.6f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No model performance data available for visualization\")\n",
    "    print(\"Please ensure the ML pipeline completed successfully\")\n",
    "\n",
    "# Clean up resources\n",
    "print(f\"\\nüßπ CLEANING UP RESOURCES...\")\n",
    "try:\n",
    "    if 'client' in globals() and client:\n",
    "        print(\"Closing Dask client...\")\n",
    "        client.close()\n",
    "    if 'cluster' in globals() and cluster:\n",
    "        print(\"Closing Dask cluster...\")\n",
    "        cluster.close()\n",
    "    print(\"‚úÖ Cleanup completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Cleanup warning: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ ADVANCED PIPELINE EXECUTION COMPLETED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd409642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick ML Pipeline Test (without hyperparameter optimization)\n",
    "print(\"=== QUICK ML PIPELINE TEST ===\")\n",
    "\n",
    "# Setup the ML pipeline with basic parameters\n",
    "try:\n",
    "    # Import the modules fresh\n",
    "    import importlib\n",
    "    import sys\n",
    "    \n",
    "    # Reload the pipeline module to get fresh imports\n",
    "    if 'models.pipeline' in sys.modules:\n",
    "        importlib.reload(sys.modules['models.pipeline'])\n",
    "    \n",
    "    from models.pipeline import CryptoVolatilityMLPipeline\n",
    "    \n",
    "    # Create pipeline with conservative settings\n",
    "    ml_pipeline = CryptoVolatilityMLPipeline(\n",
    "        n_trials=5,  # Very small for quick test\n",
    "        n_rounds=50,  # Reduced rounds\n",
    "        eval_metric='mae',\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ ML Pipeline created successfully\")\n",
    "    print(f\"Training on {len(feature_set)} samples with {len(feature_set.columns)} features\")\n",
    "    \n",
    "    # Set basic parameters manually to avoid optimization\n",
    "    basic_params = {\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 100,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    print(\"Running ML pipeline with basic parameters...\")\n",
    "    \n",
    "    results = ml_pipeline.run_complete_pipeline(\n",
    "        final_features=feature_set,\n",
    "        client=client,\n",
    "        target_coin=\"ethereum\", \n",
    "        optimize=False  # Skip optimization, use default parameters\n",
    "    )\n",
    "    \n",
    "    # Display basic results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä PIPELINE RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if 'model_performance' in results:\n",
    "        perf = results['model_performance']\n",
    "        print(f\"‚úÖ Model trained successfully!\")\n",
    "        print(f\"üìà R¬≤ Score: {perf.get('r2_score', 'N/A'):.4f}\")\n",
    "        print(f\"üìâ MAE: {perf.get('mae', 'N/A'):.6f}\")\n",
    "        print(f\"üìâ RMSE: {perf.get('rmse', 'N/A'):.6f}\")\n",
    "    \n",
    "    print(f\"‚ö° Feature importance available: {'feature_importance' in results}\")\n",
    "    print(f\"üìä Predictions available: {'predictions' in results}\")\n",
    "    \n",
    "    print(\"\\nüéâ Basic ML pipeline completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in ML pipeline: {str(e)}\")\n",
    "    print(\"This might be due to cached imports. Try restarting the kernel if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Different Frequency Settings\n",
    "print(\"=== FREQUENCY COMPARISON DEMONSTRATION ===\")\n",
    "\n",
    "# Test different frequencies\n",
    "frequencies_to_test = [\"1D\", \"1H\"]\n",
    "frequency_results = {}\n",
    "\n",
    "for test_freq in frequencies_to_test:\n",
    "    try:\n",
    "        print(f\"\\n--- Testing {test_freq} frequency ---\")\n",
    "        \n",
    "        # Create a new collector with different frequency\n",
    "        test_collector = CryptoDataCollector(\n",
    "            timezone=\"Europe/Madrid\",\n",
    "            top_n=3,  # Reduced for faster testing\n",
    "            lookback_days=7,  # Very short for demonstration\n",
    "            frequency=test_freq\n",
    "        )\n",
    "        \n",
    "        print(f\"Collector settings:\")\n",
    "        print(f\"  - Frequency: {test_collector.FREQUENCY}\")\n",
    "        print(f\"  - Pandas freq: {test_collector.get_pandas_freq()}\")\n",
    "        print(f\"  - Binance interval: {test_collector.get_binance_interval()}\")\n",
    "        print(f\"  - Deribit resolution: {test_collector.get_deribit_resolution()}\")\n",
    "        print(f\"  - Batch size: {test_collector.get_batch_size_for_frequency()}\")\n",
    "        \n",
    "        # Test a quick data collection\n",
    "        small_universe = ['bitcoin', 'ethereum']\n",
    "        test_data = test_collector.coingecko_get_price_action(small_universe, sleep_time=1)\n",
    "        \n",
    "        frequency_results[test_freq] = {\n",
    "            'shape': test_data.shape,\n",
    "            'date_range': (test_data.index.min(), test_data.index.max()) if len(test_data) > 0 else None,\n",
    "            'pandas_freq': test_collector.get_pandas_freq(),\n",
    "            'batch_size': test_collector.get_batch_size_for_frequency()\n",
    "        }\n",
    "        \n",
    "        print(f\"  - Data shape: {test_data.shape}\")\n",
    "        if len(test_data) > 0:\n",
    "            print(f\"  - Date range: {test_data.index.min()} to {test_data.index.max()}\")\n",
    "            print(f\"  - Sample frequency: {test_data.index.freq if hasattr(test_data.index, 'freq') else 'Inferred'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing {test_freq}: {e}\")\n",
    "        frequency_results[test_freq] = {'error': str(e)}\n",
    "\n",
    "# Summary comparison\n",
    "print(f\"\\n=== FREQUENCY COMPARISON SUMMARY ===\")\n",
    "for freq, results in frequency_results.items():\n",
    "    print(f\"\\n{freq} Frequency:\")\n",
    "    if 'error' in results:\n",
    "        print(f\"  - Error: {results['error']}\")\n",
    "    else:\n",
    "        print(f\"  - Data points: {results['shape'][0]} rows, {results['shape'][1]} columns\")\n",
    "        print(f\"  - Pandas frequency: {results['pandas_freq']}\")\n",
    "        print(f\"  - Dune batch size: {results['batch_size']}\")\n",
    "        if results['date_range']:\n",
    "            print(f\"  - Date range: {results['date_range'][0]} to {results['date_range'][1]}\")\n",
    "\n",
    "print(f\"\\n=== FREQUENCY IMPLEMENTATION COMPLETE ===\")\n",
    "print(\"All data collection methods now support frequency parameter!\")\n",
    "print(\"Available frequencies: 1D (daily), 1H (hourly)\")\n",
    "print(\"Frequency is automatically applied to:\")\n",
    "print(\"  - CoinGecko data resampling\")\n",
    "print(\"  - Binance API intervals\")\n",
    "print(\"  - Deribit DVOL resolution\")\n",
    "print(\"  - FRED data resampling\")\n",
    "print(\"  - Dune Analytics batch sizing\")\n",
    "print(\"  - Feature engineering alignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5307297",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This quick example demonstrates:\n",
    "\n",
    "1. **Data Collection**: Basic crypto price data collection\n",
    "2. **Feature Engineering**: Simple technical indicators\n",
    "3. **ML Pipeline**: Quick XGBoost model with hyperparameter optimization\n",
    "\n",
    "For a full-featured pipeline with TSFresh, multiple data sources, and extensive optimization, see `main_pipeline.ipynb`.\n",
    "\n",
    "### Next Steps:\n",
    "- Configure more API keys in `.env` for additional data sources\n",
    "- Increase `lookback_days` and `n_trials` for better model performance\n",
    "- Experiment with different `extraction_settings` for TSFresh\n",
    "- Try different target coins and evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
